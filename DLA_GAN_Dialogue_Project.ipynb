{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# \ud83c\udfad DLA GAN Dialogue Project\n\n[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/jawaharganesh24189/DLA/blob/main/DLA_GAN_Dialogue_Project.ipynb)\n\nAn adversarial dialogue generation system using GANs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## \ud83d\udce6 Section 1: Setup & Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages (uncomment if needed)\n",
    "# !pip install tensorflow numpy matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import glob\n",
    "import json\n",
    "import csv\n",
    "import pickle\n",
    "from dataclasses import dataclass\n",
    "from typing import List, Dict, Optional, Tuple\n",
    "from collections import defaultdict, Counter\n",
    "from io import StringIO\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "print(f'\u2705 TensorFlow version: {tf.__version__}')\n",
    "print(f'\u2705 GPU available: {tf.config.list_physical_devices(\"GPU\")}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## \ud83d\udcbe Section 2: Google Drive & Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATASET_PATH = '/content/drive/MyDrive/DLA_Notebooks_Data_PGPM/Dataset/'\n",
    "OUTPUT_FILE = 'cleandata.txt'\n",
    "\n",
    "print(f'\ud83d\udcc1 Dataset path: {DATASET_PATH}')\n",
    "print(f'\ud83d\udcc4 Output file: {OUTPUT_FILE}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## \ud83c\udfac Section 3: Dialogue Parsing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class DialogueTurn:\n",
    "    context: str\n",
    "    response: str\n",
    "    metadata: Optional[Dict] = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DialogueParser:\n",
    "    \"\"\"Multi-format dialogue parser\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.context_response_pattern = re.compile(\n",
    "            r'context:\\s*(.+?)\\s*response:\\s*(.+?)(?=\\ncontext:|$)',\n",
    "            re.DOTALL | re.IGNORECASE\n",
    "        )\n",
    "        self.dialogue_pattern = re.compile(r'^(.+?):\\s*(.+?)$', re.MULTILINE)\n",
    "    \n",
    "    def parse_file(self, filepath: str) -> List[DialogueTurn]:\n",
    "        with open(filepath, 'r', encoding='utf-8', errors='ignore') as f:\n",
    "            content = f.read()\n",
    "        turns = self._parse_context_response(content)\n",
    "        if not turns:\n",
    "            turns = self._parse_dialogue_format(content)\n",
    "        return turns\n",
    "    \n",
    "    def _parse_context_response(self, content: str) -> List[DialogueTurn]:\n",
    "        matches = self.context_response_pattern.findall(content)\n",
    "        turns = []\n",
    "        for context, response in matches:\n",
    "            context = self._clean_text(context)\n",
    "            response = self._clean_text(response)\n",
    "            if context and response:\n",
    "                turns.append(DialogueTurn(\n",
    "                    context=context,\n",
    "                    response=response,\n",
    "                    metadata={'format': 'context_response'}\n",
    "                ))\n",
    "        return turns\n",
    "    \n",
    "    def _parse_dialogue_format(self, content: str) -> List[DialogueTurn]:\n",
    "        lines = content.strip().split('\\n')\n",
    "        turns = []\n",
    "        context_buffer = []\n",
    "        for line in lines:\n",
    "            line = line.strip()\n",
    "            if not line:\n",
    "                continue\n",
    "            match = self.dialogue_pattern.match(line)\n",
    "            if match:\n",
    "                speaker, dialogue = match.groups()\n",
    "                dialogue = self._clean_text(dialogue)\n",
    "                if dialogue:\n",
    "                    context = ' '.join(context_buffer[-3:]) if context_buffer else ''\n",
    "                    turns.append(DialogueTurn(\n",
    "                        context=context,\n",
    "                        response=dialogue,\n",
    "                        metadata={'format': 'dialogue', 'speaker': speaker.strip()}\n",
    "                    ))\n",
    "                    context_buffer.append(f'{speaker}: {dialogue}')\n",
    "        return turns\n",
    "    \n",
    "    def _clean_text(self, text: str) -> str:\n",
    "        text = re.sub(r'\\s+', ' ', text)\n",
    "        text = text.replace('\\\\', ' ')\n",
    "        return text.strip()\n",
    "    \n",
    "    def parse_directory(self, directory: str, pattern: str = '*.txt',\n",
    "                       auto_save_txt: bool = True,\n",
    "                       output_file: str = 'cleandata.txt') -> List[DialogueTurn]:\n",
    "        all_turns = []\n",
    "        file_pattern = os.path.join(directory, pattern)\n",
    "        files = glob.glob(file_pattern)\n",
    "        print(f'\ud83d\udcc1 Found {len(files)} files')\n",
    "        for i, filepath in enumerate(sorted(files), 1):\n",
    "            try:\n",
    "                turns = self.parse_file(filepath)\n",
    "                all_turns.extend(turns)\n",
    "                if i % 100 == 0:\n",
    "                    print(f'   Processed {i}/{len(files)} files...')\n",
    "            except Exception as e:\n",
    "                print(f'\u26a0\ufe0f  Error parsing {os.path.basename(filepath)}: {e}')\n",
    "                continue\n",
    "        print(f'\\n\u2705 Total dialogue turns: {len(all_turns)}')\n",
    "        if auto_save_txt and all_turns:\n",
    "            self.save_to_cleandata(all_turns, output_file)\n",
    "        return all_turns\n",
    "    \n",
    "    def save_to_cleandata(self, turns: List[DialogueTurn], output_file: str = 'cleandata.txt'):\n",
    "        print(f'\\n{\"-\"*70}')\n",
    "        print(f'\ud83d\udcbe Creating {output_file}')\n",
    "        dialogue_text = self.to_dialogue_format(turns)\n",
    "        with open(output_file, 'w', encoding='utf-8') as f:\n",
    "            f.write(dialogue_text)\n",
    "        lines = dialogue_text.split('\\n')\n",
    "        char_counts = {}\n",
    "        for line in lines:\n",
    "            if ':' in line:\n",
    "                char = line.split(':', 1)[0].strip()\n",
    "                char_counts[char] = char_counts.get(char, 0) + 1\n",
    "        print(f'\u2705 Saved: {output_file} ({len(lines)} lines, {len(char_counts)} characters)')\n",
    "        print(\"-\"*70)\n",
    "    \n",
    "    def to_dialogue_format(self, turns: List[DialogueTurn]) -> str:\n",
    "        lines = []\n",
    "        for turn in turns:\n",
    "            speaker = 'Unknown'\n",
    "            if turn.metadata and 'speaker' in turn.metadata:\n",
    "                speaker = turn.metadata['speaker']\n",
    "            if speaker and turn.response:\n",
    "                lines.append(f'{speaker}: {turn.response}')\n",
    "        return '\\n'.join(lines)\n",
    "\n",
    "print('\u2705 DialogueParser class defined!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parse dialogue files\n",
    "dialogue_parser = DialogueParser()\n",
    "turns = dialogue_parser.parse_directory(\n",
    "    directory=DATASET_PATH,\n",
    "    auto_save_txt=True,\n",
    "    output_file=OUTPUT_FILE\n",
    ")\n",
    "print(f'\\n\u2705 Parsing complete! {OUTPUT_FILE} ready.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## \ud83d\udd24 Section 4: Data Processing & Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FlexibleDialogueDataProcessor:\n",
    "    \"\"\"Processes cleandata.txt for GAN training\"\"\"\n",
    "    \n",
    "    def __init__(self, file_path: str, seq_length: int = 50):\n",
    "        self.file_path = file_path\n",
    "        self.seq_length = seq_length\n",
    "        self.dialogues = []\n",
    "        self.characters = set()\n",
    "        self.tokenizer = None\n",
    "        self.vocab_size = 0\n",
    "    \n",
    "    def load_and_parse(self) -> List[str]:\n",
    "        print(f'\ud83d\udcd6 Loading: {self.file_path}')\n",
    "        with open(self.file_path, 'r', encoding='utf-8') as f:\n",
    "            lines = f.readlines()\n",
    "        for line in lines:\n",
    "            line = line.strip()\n",
    "            if ':' in line:\n",
    "                parts = line.split(':', 1)\n",
    "                if len(parts) == 2:\n",
    "                    character = parts[0].strip()\n",
    "                    dialogue = parts[1].strip()\n",
    "                    if character and dialogue:\n",
    "                        self.characters.add(character)\n",
    "                        self.dialogues.append(dialogue)\n",
    "        print(f'\u2705 Loaded {len(self.dialogues)} dialogues')\n",
    "        print(f'\u2705 Found {len(self.characters)} characters')\n",
    "        return self.dialogues\n",
    "    \n",
    "    def build_vocabulary(self, max_vocab_size: int = 5000):\n",
    "        print(f'\\n\ud83d\udd24 Building vocabulary (max: {max_vocab_size})...')\n",
    "        self.tokenizer = Tokenizer(num_words=max_vocab_size, oov_token='<UNK>')\n",
    "        self.tokenizer.fit_on_texts(self.dialogues)\n",
    "        self.vocab_size = min(len(self.tokenizer.word_index) + 1, max_vocab_size)\n",
    "        print(f'\u2705 Vocabulary size: {self.vocab_size}')\n",
    "        return self.tokenizer\n",
    "    \n",
    "    def create_sequences(self) -> np.ndarray:\n",
    "        print(f'\\n\ud83d\udcca Creating sequences (length: {self.seq_length})...')\n",
    "        sequences = self.tokenizer.texts_to_sequences(self.dialogues)\n",
    "        padded = pad_sequences(sequences, maxlen=self.seq_length, padding='pre')\n",
    "        print(f'\u2705 Created {len(padded)} sequences')\n",
    "        return padded\n",
    "\n",
    "print('\u2705 FlexibleDialogueDataProcessor class defined!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize and load data\n",
    "data_processor = FlexibleDialogueDataProcessor(file_path=OUTPUT_FILE, seq_length=50)\n",
    "dialogues = data_processor.load_and_parse()\n",
    "tokenizer = data_processor.build_vocabulary(max_vocab_size=5000)\n",
    "vocab_size = data_processor.vocab_size\n",
    "sequences = data_processor.create_sequences()\n",
    "print(f'\\n\u2705 Data ready! Vocab: {vocab_size}, Sequences: {len(sequences)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## \ud83e\udde0 Section 5: Model Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Generator(keras.Model):\n",
    "    def __init__(self, vocab_size, embedding_dim=128, lstm_units=256):\n",
    "        super(Generator, self).__init__()\n",
    "        self.embedding = layers.Embedding(vocab_size, embedding_dim)\n",
    "        self.lstm1 = layers.LSTM(lstm_units, return_sequences=True)\n",
    "        self.lstm2 = layers.LSTM(lstm_units, return_sequences=True)\n",
    "        self.dropout = layers.Dropout(0.3)\n",
    "        self.dense = layers.Dense(vocab_size, activation='softmax')\n",
    "    \n",
    "    def call(self, inputs, training=False):\n",
    "        x = self.embedding(inputs)\n",
    "        x = self.lstm1(x)\n",
    "        x = self.lstm2(x)\n",
    "        if training:\n",
    "            x = self.dropout(x, training=training)\n",
    "        return self.dense(x)\n",
    "\n",
    "print('\u2705 Generator defined!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Discriminator(keras.Model):\n",
    "    def __init__(self, vocab_size, embedding_dim=128, lstm_units=256):\n",
    "        super(Discriminator, self).__init__()\n",
    "        self.embedding = layers.Embedding(vocab_size, embedding_dim)\n",
    "        self.lstm = layers.LSTM(lstm_units)\n",
    "        self.dropout = layers.Dropout(0.3)\n",
    "        self.dense1 = layers.Dense(128, activation='relu')\n",
    "        self.dense2 = layers.Dense(1, activation='sigmoid')\n",
    "    \n",
    "    def call(self, inputs, training=False):\n",
    "        x = self.embedding(inputs)\n",
    "        x = self.lstm(x)\n",
    "        if training:\n",
    "            x = self.dropout(x, training=training)\n",
    "        x = self.dense1(x)\n",
    "        return self.dense2(x)\n",
    "\n",
    "print('\u2705 Discriminator defined!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generator = Generator(vocab_size=vocab_size, embedding_dim=128, lstm_units=256)\n",
    "discriminator = Discriminator(vocab_size=vocab_size, embedding_dim=128, lstm_units=256)\n",
    "generator_optimizer = keras.optimizers.Adam(learning_rate=0.0001)\n",
    "discriminator_optimizer = keras.optimizers.Adam(learning_rate=0.0001)\n",
    "loss_fn = keras.losses.BinaryCrossentropy()\n",
    "print('\u2705 Models initialized!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## \ud83c\udfcb\ufe0f Section 6: Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 64\n",
    "EPOCHS = 10\n",
    "print(f'\u2699\ufe0f  Batch size: {BATCH_SIZE}, Epochs: {EPOCHS}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_gan(generator, discriminator, sequences, epochs, batch_size):\n",
    "    \"\"\"Train the GAN\"\"\"\n",
    "    history = {'d_loss': [], 'g_loss': [], 'd_accuracy': []}\n",
    "    num_batches = len(sequences) // batch_size\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        print(f'\\nEpoch {epoch + 1}/{epochs}')\n",
    "        np.random.shuffle(sequences)\n",
    "        epoch_d_loss, epoch_g_loss, epoch_d_acc = [], [], []\n",
    "        \n",
    "        for batch in range(num_batches):\n",
    "            start_idx = batch * batch_size\n",
    "            end_idx = start_idx + batch_size\n",
    "            real_sequences = sequences[start_idx:end_idx]\n",
    "            \n",
    "            # Train Discriminator\n",
    "            with tf.GradientTape() as tape:\n",
    "                noise = tf.random.uniform(\n",
    "                    shape=(batch_size, data_processor.seq_length),\n",
    "                    minval=0, maxval=vocab_size, dtype=tf.int32\n",
    "                )\n",
    "                fake_sequences = generator(noise, training=True)\n",
    "                fake_sequences = tf.argmax(fake_sequences, axis=-1)\n",
    "                real_output = discriminator(real_sequences, training=True)\n",
    "                fake_output = discriminator(fake_sequences, training=True)\n",
    "                real_loss = loss_fn(tf.ones_like(real_output), real_output)\n",
    "                fake_loss = loss_fn(tf.zeros_like(fake_output), fake_output)\n",
    "                d_loss = real_loss + fake_loss\n",
    "            d_gradients = tape.gradient(d_loss, discriminator.trainable_variables)\n",
    "            discriminator_optimizer.apply_gradients(zip(d_gradients, discriminator.trainable_variables))\n",
    "            \n",
    "            # Train Generator\n",
    "            with tf.GradientTape() as tape:\n",
    "                noise = tf.random.uniform(\n",
    "                    shape=(batch_size, data_processor.seq_length),\n",
    "                    minval=0, maxval=vocab_size, dtype=tf.int32\n",
    "                )\n",
    "                fake_sequences = generator(noise, training=True)\n",
    "                fake_sequences = tf.argmax(fake_sequences, axis=-1)\n",
    "                fake_output = discriminator(fake_sequences, training=False)\n",
    "                g_loss = loss_fn(tf.ones_like(fake_output), fake_output)\n",
    "            g_gradients = tape.gradient(g_loss, generator.trainable_variables)\n",
    "            generator_optimizer.apply_gradients(zip(g_gradients, generator.trainable_variables))\n",
    "            \n",
    "            d_accuracy = ((real_output > 0.5).numpy().mean() + (fake_output <= 0.5).numpy().mean()) / 2\n",
    "            epoch_d_loss.append(d_loss.numpy())\n",
    "            epoch_g_loss.append(g_loss.numpy())\n",
    "            epoch_d_acc.append(d_accuracy)\n",
    "            \n",
    "            if (batch + 1) % 10 == 0:\n",
    "                print(f'  Batch {batch + 1}/{num_batches} - D_loss: {d_loss.numpy():.4f}, G_loss: {g_loss.numpy():.4f}')\n",
    "        \n",
    "        history['d_loss'].append(np.mean(epoch_d_loss))\n",
    "        history['g_loss'].append(np.mean(epoch_g_loss))\n",
    "        history['d_accuracy'].append(np.mean(epoch_d_acc))\n",
    "        print(f'Epoch {epoch + 1} - D_loss: {history[\"d_loss\"][-1]:.4f}, G_loss: {history[\"g_loss\"][-1]:.4f}')\n",
    "    \n",
    "    return history\n",
    "\n",
    "print('\u2705 Training function defined!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('\ud83d\ude80 Starting training...\\n')\n",
    "training_history = train_gan(generator, discriminator, sequences, EPOCHS, BATCH_SIZE)\n",
    "print('\\n\u2705 Training complete!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## \ud83c\udfad Section 7: Dialogue Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_dialogue(generator, tokenizer, seed_text='', max_length=50):\n",
    "    \"\"\"Generate dialogue\"\"\"\n",
    "    if seed_text:\n",
    "        sequence = tokenizer.texts_to_sequences([seed_text])[0]\n",
    "        sequence = pad_sequences([sequence], maxlen=max_length, padding='pre')\n",
    "    else:\n",
    "        sequence = np.random.randint(1, vocab_size, size=(1, max_length))\n",
    "    predictions = generator(sequence, training=False)\n",
    "    predicted_ids = tf.argmax(predictions, axis=-1).numpy()[0]\n",
    "    words = []\n",
    "    for idx in predicted_ids:\n",
    "        for word, word_idx in tokenizer.word_index.items():\n",
    "            if word_idx == idx:\n",
    "                words.append(word)\n",
    "                break\n",
    "    return ' '.join(words)\n",
    "\n",
    "print('\u2705 Generation function defined!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('\ud83c\udfad Generating dialogues...\\n')\n",
    "for i in range(5):\n",
    "    dialogue = generate_dialogue(generator, tokenizer, max_length=30)\n",
    "    print(f'{i+1}. {dialogue}')\n",
    "print('\\n\u2705 Generation complete!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## \ud83d\udcca Section 8: Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1, 2, figsize=(12, 4))\n",
    "axes[0].plot(training_history['d_loss'], label='D Loss')\n",
    "axes[0].plot(training_history['g_loss'], label='G Loss')\n",
    "axes[0].set_title('Training Loss')\n",
    "axes[0].set_xlabel('Epoch')\n",
    "axes[0].set_ylabel('Loss')\n",
    "axes[0].legend()\n",
    "axes[0].grid(True)\n",
    "axes[1].plot(training_history['d_accuracy'], label='D Accuracy', color='green')\n",
    "axes[1].set_title('Discriminator Accuracy')\n",
    "axes[1].set_xlabel('Epoch')\n",
    "axes[1].set_ylabel('Accuracy')\n",
    "axes[1].legend()\n",
    "axes[1].grid(True)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## \ud83d\udcbe Section 9: Model Persistence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save models\n",
    "os.makedirs('models', exist_ok=True)\n",
    "generator.save('models/generator.h5')\n",
    "discriminator.save('models/discriminator.h5')\n",
    "with open('models/tokenizer.pickle', 'wb') as f:\n",
    "    pickle.dump(tokenizer, f)\n",
    "print('\u2705 Models saved!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## \ud83c\udfaf Section 10: Summary\n\n### \u2705 Complete!\n\n**Built:**\n1. DialogueParser - Multi-format parsing\n2. FlexibleDialogueDataProcessor - Tokenization\n3. GAN Architecture - Generator & Discriminator\n4. Training Pipeline - Adversarial training\n5. Generation System - Create new dialogues\n6. Visualization - Training metrics\n7. Persistence - Save/load models\n\n**Next Steps:**\n- Train longer for better quality\n- Experiment with hyperparameters\n- Add character-specific generation\n- Implement evaluation metrics\n\n\ud83c\udf89 **Congratulations!**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}