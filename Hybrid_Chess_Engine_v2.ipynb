{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<a href=\"https://colab.research.google.com/github/jawaharganesh24189/DLA/blob/main/Hybrid_Chess_Engine_v2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# \ud83e\udde0 Hybrid Neural Chess Engine v2.0\n\n## \ud83d\udcda Overview\n\nThis is an **improved version** of the Hybrid Neural Chess Engine that combines:\n- **Imitation Learning**: Learning from master games (e.g., Hikaru Nakamura)\n- **Self-Play Reinforcement**: Improving through self-play\n\n### \u2728 Key Improvements in v2.0:\n- \u2705 **Early stopping** with validation split and patience mechanism\n- \u2705 **Google Drive integration** for persistent model storage\n- \u2705 **Improved model architecture** with batch normalization and dropout\n- \u2705 **Comprehensive metrics tracking** (Top-1/Top-5 accuracy, loss curves)\n- \u2705 **Checkpoint management** (best model + periodic saves)\n- \u2705 **Self-play game saving** as PGN files\n- \u2705 **Training visualization** (loss plots, accuracy curves)\n- \u2705 **Progress bars** using tqdm\n- \u2705 **Resume from checkpoint** capability\n- \u2705 **Organized directory structure** on Google Drive\n\n### \ud83c\udfd7\ufe0f Architecture:\n- **Policy Network**: CNN + Transformer \u2192 Move Probabilities (4544 moves)\n- **Value Network**: CNN \u2192 Position Evaluation (-1 to +1)\n\n### \ud83d\udcd6 Notebook Sections:\n1. **Setup & Configuration**\n2. **Google Drive Setup**\n3. **Data Preparation**\n4. **Model Architecture** (with improvements)\n5. **Training Utilities** (early stopping, checkpointing, metrics)\n6. **Imitation Learning**\n7. **Self-Play Reinforcement**\n8. **Evaluation & Metrics**\n9. **Model Saving & Loading**\n10. **Play Against Engine**\n11. **Visualization**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## \ud83d\udccb SECTION 1 \u2014 Configuration\n\nConfigure all hyperparameters and paths here for easy adjustment."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ===== CONFIGURATION =====\n# All hyperparameters and settings in one place for easy tuning\n\nCONFIG = {\n    # === Paths ===\n    'pgn_path': '/content/drive/MyDrive/ChessEngine/data/master_games.pgn',\n    'save_dir': '/content/drive/MyDrive/ChessEngine/',\n    \n    # === Data Parameters ===\n    'max_games': 200,  # Number of games to load from PGN\n    'train_val_split': 0.8,  # 80% train, 20% validation\n    \n    # === Training Parameters ===\n    'batch_size': 32,\n    'epochs': 10,\n    'learning_rate': 1e-3,\n    'weight_decay': 1e-5,\n    \n    # === Early Stopping ===\n    'patience': 5,  # Stop if no improvement for N epochs\n    'min_delta': 0.001,  # Minimum change to qualify as improvement\n    \n    # === Checkpointing ===\n    'save_every': 2,  # Save checkpoint every N epochs\n    \n    # === Self-Play Parameters ===\n    'selfplay_games': 20,\n    'max_plies': 200,\n    \n    # === Mode ===\n    # Options: 'imitation', 'selfplay', or 'hybrid'\n    'mode': 'hybrid',\n    \n    # === Random Seed ===\n    'seed': 42\n}\n\nprint(\"\u2705 Configuration loaded successfully!\")\nprint(\"\\nKey settings:\")\nfor key, value in CONFIG.items():\n    print(f\"  \u2022 {key}: {value}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## \ud83d\udd27 SECTION 2 \u2014 Setup & Installation\n\nInstall required packages and import libraries."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Install required libraries\n!pip -q install python-chess torch torchvision tqdm matplotlib\n\n# Standard library imports\nimport os\nimport json\nimport random\nimport copy\nfrom datetime import datetime\nfrom pathlib import Path\nfrom collections import defaultdict\n\n# Third-party imports\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom tqdm.auto import tqdm\n\n# Chess library\nimport chess\nimport chess.pgn\n\n# PyTorch imports\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torch.optim as optim\nfrom torch.utils.data import Dataset, DataLoader, random_split\n\nprint(\"\u2705 All libraries imported successfully!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Set seeds for reproducibility\ndef set_seed(seed):\n    \"\"\"Set random seeds for reproducibility.\"\"\"\n    random.seed(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    if torch.cuda.is_available():\n        torch.cuda.manual_seed(seed)\n        torch.cuda.manual_seed_all(seed)\n    torch.backends.cudnn.deterministic = True\n    torch.backends.cudnn.benchmark = False\n\nset_seed(CONFIG['seed'])\n\n# Device setup\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\nprint(f\"\\n\ud83d\udda5\ufe0f  Using device: {device}\")\n\nif device.type == 'cuda':\n    print(f\"   GPU: {torch.cuda.get_device_name(0)}\")\n    print(f\"   Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.2f} GB\")\n\nCONFIG['device'] = device"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## \ud83d\udcbe SECTION 3 \u2014 Google Drive Setup\n\nMount Google Drive and create organized directory structure."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Mount Google Drive\nfrom google.colab import drive\ndrive.mount('/content/drive')\nprint(\"\u2705 Google Drive mounted successfully!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def setup_directories(base_dir):\n    \"\"\"\n    Create organized directory structure for the chess engine.\n    \n    Directory structure:\n    ChessEngine/\n    \u251c\u2500\u2500 models/       (saved model checkpoints)\n    \u251c\u2500\u2500 games/        (self-play games in PGN format)\n    \u251c\u2500\u2500 logs/         (training logs and metrics)\n    \u251c\u2500\u2500 data/         (PGN data files)\n    \u2514\u2500\u2500 plots/        (training visualizations)\n    \"\"\"\n    dirs = {\n        'base': base_dir,\n        'models': os.path.join(base_dir, 'models'),\n        'games': os.path.join(base_dir, 'games'),\n        'logs': os.path.join(base_dir, 'logs'),\n        'data': os.path.join(base_dir, 'data'),\n        'plots': os.path.join(base_dir, 'plots')\n    }\n    \n    print(\"Creating directory structure...\")\n    for name, path in dirs.items():\n        os.makedirs(path, exist_ok=True)\n        print(f\"  \u2713 {name:10s}: {path}\")\n    \n    return dirs\n\n# Setup directories\nDIRS = setup_directories(CONFIG['save_dir'])\nprint(\"\\n\u2705 Directory structure created successfully!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## \ud83d\udcca SECTION 4 \u2014 Data Preparation\n\nBoard encoding and move vocabulary generation."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def board_to_tensor(board):\n    \"\"\"\n    Convert chess board to 12\u00d78\u00d78 tensor.\n    \n    Channels represent: [P,N,B,R,Q,K, p,n,b,r,q,k]\n    Where uppercase = white pieces, lowercase = black pieces\n    \n    Args:\n        board: chess.Board object\n    \n    Returns:\n        torch.Tensor of shape (12, 8, 8)\n    \"\"\"\n    tensor = torch.zeros(12, 8, 8, dtype=torch.float32)\n    \n    piece_to_idx = {\n        'P': 0, 'N': 1, 'B': 2, 'R': 3, 'Q': 4, 'K': 5,\n        'p': 6, 'n': 7, 'b': 8, 'r': 9, 'q': 10, 'k': 11\n    }\n    \n    for square in chess.SQUARES:\n        piece = board.piece_at(square)\n        if piece:\n            rank, file = divmod(square, 8)\n            channel = piece_to_idx[piece.symbol()]\n            tensor[channel, rank, file] = 1.0\n    \n    return tensor\n\n# Test board encoding\ntest_board = chess.Board()\ntest_tensor = board_to_tensor(test_board)\nprint(f\"Board tensor shape: {test_tensor.shape}\")\nprint(f\"\u2705 Board encoding working correctly!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def generate_move_vocab():\n    \"\"\"\n    Generate move vocabulary covering all possible chess moves.\n    \n    Includes:\n    - Standard moves (source square \u2192 destination square)\n    - Promotion moves (pawn reaching last rank with piece selection)\n    \n    Returns:\n        move_to_idx: dict mapping (from_sq, to_sq, promotion) -> index\n        idx_to_move: dict mapping index -> (from_sq, to_sq, promotion)\n        vocab_size: total number of unique moves\n    \"\"\"\n    moves = []\n    \n    # Standard moves: 64 \u00d7 64 combinations (excluding same square)\n    for from_sq in range(64):\n        for to_sq in range(64):\n            if from_sq != to_sq:\n                moves.append((from_sq, to_sq, None))\n    \n    # Promotions for white pawns (rank 6 \u2192 rank 7)\n    for file in range(8):\n        from_sq = chess.square(file, 6)\n        for to_file in [file-1, file, file+1]:  # Can promote to 3 squares\n            if 0 <= to_file < 8:\n                to_sq = chess.square(to_file, 7)\n                for promo in [chess.QUEEN, chess.ROOK, chess.BISHOP, chess.KNIGHT]:\n                    moves.append((from_sq, to_sq, promo))\n    \n    # Promotions for black pawns (rank 1 \u2192 rank 0)\n    for file in range(8):\n        from_sq = chess.square(file, 1)\n        for to_file in [file-1, file, file+1]:\n            if 0 <= to_file < 8:\n                to_sq = chess.square(to_file, 0)\n                for promo in [chess.QUEEN, chess.ROOK, chess.BISHOP, chess.KNIGHT]:\n                    moves.append((from_sq, to_sq, promo))\n    \n    move_to_idx = {move: idx for idx, move in enumerate(moves)}\n    idx_to_move = {idx: move for idx, move in enumerate(moves)}\n    \n    return move_to_idx, idx_to_move, len(moves)\n\n# Generate move vocabulary\nMOVE_TO_IDX, IDX_TO_MOVE, VOCAB_SIZE = generate_move_vocab()\nprint(f\"Move vocabulary size: {VOCAB_SIZE}\")\nprint(f\"\u2705 Move vocabulary generated successfully!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def move_to_index(move, move_to_idx):\n    \"\"\"Convert chess.Move to vocabulary index.\"\"\"\n    key = (move.from_square, move.to_square, move.promotion)\n    return move_to_idx.get(key, None)\n\ndef index_to_move(idx, idx_to_move):\n    \"\"\"Convert vocabulary index to chess.Move.\"\"\"\n    if idx not in idx_to_move:\n        return None\n    from_sq, to_sq, promo = idx_to_move[idx]\n    return chess.Move(from_sq, to_sq, promotion=promo)\n\n# Test conversions\ntest_move = chess.Move.from_uci(\"e2e4\")\ntest_idx = move_to_index(test_move, MOVE_TO_IDX)\nrecovered_move = index_to_move(test_idx, IDX_TO_MOVE)\nprint(f\"Test: {test_move} \u2192 {test_idx} \u2192 {recovered_move}\")\nprint(f\"\u2705 Move conversion working correctly!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## \ud83d\udcda SECTION 5 \u2014 Dataset Class\n\nLoad and process PGN games with train/validation split."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class ChessDataset(Dataset):\n    \"\"\"\n    Chess dataset from PGN file.\n    Extracts board positions and corresponding moves from master games.\n    \"\"\"\n    \n    def __init__(self, pgn_path, max_games=None):\n        self.positions = []\n        self.moves = []\n        \n        print(f\"Loading games from: {pgn_path}\")\n        \n        if not os.path.exists(pgn_path):\n            print(f\"\u26a0\ufe0f  Warning: File not found: {pgn_path}\")\n            print(\"   Using empty dataset. Make sure to upload your PGN file to Google Drive!\")\n            return\n        \n        with open(pgn_path) as pgn_file:\n            game_count = 0\n            pbar = tqdm(desc=\"Loading games\", total=max_games)\n            \n            while True:\n                game = chess.pgn.read_game(pgn_file)\n                if game is None or (max_games and game_count >= max_games):\n                    break\n                \n                board = game.board()\n                for move in game.mainline_moves():\n                    # Save position before move\n                    self.positions.append(board_to_tensor(board))\n                    \n                    # Save move as index\n                    move_idx = move_to_index(move, MOVE_TO_IDX)\n                    if move_idx is not None:\n                        self.moves.append(move_idx)\n                    else:\n                        # Remove position if move couldn't be encoded\n                        self.positions.pop()\n                    \n                    board.push(move)\n                \n                game_count += 1\n                pbar.update(1)\n            \n            pbar.close()\n        \n        print(f\"\u2705 Loaded {game_count} games with {len(self.positions)} positions\")\n    \n    def __len__(self):\n        return len(self.positions)\n    \n    def __getitem__(self, idx):\n        return self.positions[idx], self.moves[idx]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def create_dataloaders(dataset, batch_size, train_split=0.8):\n    \"\"\"\n    Split dataset into train/validation sets and create DataLoaders.\n    \n    Args:\n        dataset: ChessDataset instance\n        batch_size: Batch size for training\n        train_split: Fraction of data to use for training (rest for validation)\n    \n    Returns:\n        train_loader, val_loader: PyTorch DataLoader objects\n    \"\"\"\n    if len(dataset) == 0:\n        print(\"\u26a0\ufe0f  Empty dataset! Cannot create dataloaders.\")\n        return None, None\n    \n    # Calculate split sizes\n    train_size = int(train_split * len(dataset))\n    val_size = len(dataset) - train_size\n    \n    # Split dataset\n    train_dataset, val_dataset = random_split(\n        dataset, \n        [train_size, val_size],\n        generator=torch.Generator().manual_seed(CONFIG['seed'])\n    )\n    \n    # Create dataloaders\n    train_loader = DataLoader(\n        train_dataset, \n        batch_size=batch_size, \n        shuffle=True,\n        num_workers=0,  # Use 0 for Colab compatibility\n        pin_memory=True if device.type == 'cuda' else False\n    )\n    \n    val_loader = DataLoader(\n        val_dataset, \n        batch_size=batch_size, \n        shuffle=False,\n        num_workers=0,\n        pin_memory=True if device.type == 'cuda' else False\n    )\n    \n    print(f\"\ud83d\udcca Dataset split:\")\n    print(f\"   Train: {len(train_dataset):,} positions\")\n    print(f\"   Val:   {len(val_dataset):,} positions\")\n    \n    return train_loader, val_loader"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## \ud83c\udfd7\ufe0f SECTION 6 \u2014 Model Architecture\n\nImproved neural network architectures with batch normalization and dropout."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class PolicyNetwork(nn.Module):\n    \"\"\"\n    Policy Network: Predicts move probabilities.\n    \n    Architecture: CNN + Transformer + Policy Head\n    \n    Improvements in v2:\n    - Batch normalization for training stability\n    - Dropout for regularization\n    - Xavier weight initialization\n    - Deeper CNN layers\n    \"\"\"\n    \n    def __init__(self, vocab_size=4544, dropout=0.2):\n        super(PolicyNetwork, self).__init__()\n        \n        # CNN layers for spatial feature extraction\n        self.conv1 = nn.Conv2d(12, 64, kernel_size=3, padding=1)\n        self.bn1 = nn.BatchNorm2d(64)\n        \n        self.conv2 = nn.Conv2d(64, 128, kernel_size=3, padding=1)\n        self.bn2 = nn.BatchNorm2d(128)\n        \n        self.conv3 = nn.Conv2d(128, 256, kernel_size=3, padding=1)\n        self.bn3 = nn.BatchNorm2d(256)\n        \n        # Flatten: 256 * 8 * 8 = 16384\n        self.fc1 = nn.Linear(16384, 512)\n        self.bn_fc1 = nn.BatchNorm1d(512)\n        self.dropout1 = nn.Dropout(dropout)\n        \n        # Transformer encoder layers\n        encoder_layer = nn.TransformerEncoderLayer(\n            d_model=512,\n            nhead=8,\n            dim_feedforward=2048,\n            dropout=dropout,\n            batch_first=True\n        )\n        self.transformer = nn.TransformerEncoder(encoder_layer, num_layers=2)\n        \n        # Policy head\n        self.fc_policy = nn.Linear(512, vocab_size)\n        self.dropout2 = nn.Dropout(dropout)\n        \n        # Initialize weights\n        self._init_weights()\n    \n    def _init_weights(self):\n        \"\"\"Initialize weights using Xavier initialization.\"\"\"\n        for m in self.modules():\n            if isinstance(m, nn.Conv2d) or isinstance(m, nn.Linear):\n                nn.init.xavier_uniform_(m.weight)\n                if m.bias is not None:\n                    nn.init.zeros_(m.bias)\n    \n    def forward(self, x):\n        # CNN feature extraction\n        x = F.relu(self.bn1(self.conv1(x)))\n        x = F.relu(self.bn2(self.conv2(x)))\n        x = F.relu(self.bn3(self.conv3(x)))\n        \n        # Flatten\n        x = x.view(x.size(0), -1)\n        \n        # Dense layer\n        x = F.relu(self.bn_fc1(self.fc1(x)))\n        x = self.dropout1(x)\n        \n        # Transformer (expects batch, seq_len, features)\n        x = x.unsqueeze(1)  # Add sequence dimension\n        x = self.transformer(x)\n        x = x.squeeze(1)  # Remove sequence dimension\n        \n        # Policy head\n        x = self.dropout2(x)\n        logits = self.fc_policy(x)\n        \n        return logits\n\nprint(\"\u2705 PolicyNetwork defined!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class ValueNetwork(nn.Module):\n    \"\"\"\n    Value Network: Evaluates board positions.\n    \n    Architecture: CNN + Dense Layers\n    Output: Single value in range [-1, 1]\n    \n    Improvements in v2:\n    - Batch normalization\n    - Dropout\n    - Deeper architecture\n    - Better initialization\n    \"\"\"\n    \n    def __init__(self, dropout=0.2):\n        super(ValueNetwork, self).__init__()\n        \n        # CNN layers\n        self.conv1 = nn.Conv2d(12, 64, kernel_size=3, padding=1)\n        self.bn1 = nn.BatchNorm2d(64)\n        \n        self.conv2 = nn.Conv2d(64, 128, kernel_size=3, padding=1)\n        self.bn2 = nn.BatchNorm2d(128)\n        \n        self.conv3 = nn.Conv2d(128, 256, kernel_size=3, padding=1)\n        self.bn3 = nn.BatchNorm2d(256)\n        \n        # Flatten: 256 * 8 * 8 = 16384\n        self.fc1 = nn.Linear(16384, 512)\n        self.bn_fc1 = nn.BatchNorm1d(512)\n        self.dropout1 = nn.Dropout(dropout)\n        \n        self.fc2 = nn.Linear(512, 256)\n        self.bn_fc2 = nn.BatchNorm1d(256)\n        self.dropout2 = nn.Dropout(dropout)\n        \n        self.fc_value = nn.Linear(256, 1)\n        \n        # Initialize weights\n        self._init_weights()\n    \n    def _init_weights(self):\n        \"\"\"Initialize weights using Xavier initialization.\"\"\"\n        for m in self.modules():\n            if isinstance(m, nn.Conv2d) or isinstance(m, nn.Linear):\n                nn.init.xavier_uniform_(m.weight)\n                if m.bias is not None:\n                    nn.init.zeros_(m.bias)\n    \n    def forward(self, x):\n        # CNN feature extraction\n        x = F.relu(self.bn1(self.conv1(x)))\n        x = F.relu(self.bn2(self.conv2(x)))\n        x = F.relu(self.bn3(self.conv3(x)))\n        \n        # Flatten\n        x = x.view(x.size(0), -1)\n        \n        # Dense layers\n        x = F.relu(self.bn_fc1(self.fc1(x)))\n        x = self.dropout1(x)\n        \n        x = F.relu(self.bn_fc2(self.fc2(x)))\n        x = self.dropout2(x)\n        \n        # Value head (Tanh to bound output in [-1, 1])\n        value = torch.tanh(self.fc_value(x))\n        \n        return value\n\nprint(\"\u2705 ValueNetwork defined!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Initialize models\npolicy_net = PolicyNetwork(vocab_size=VOCAB_SIZE).to(device)\nvalue_net = ValueNetwork().to(device)\n\nprint(f\"\\n\ud83d\udcca Model Statistics:\")\nprint(f\"   Policy Network parameters: {sum(p.numel() for p in policy_net.parameters()):,}\")\nprint(f\"   Value Network parameters:  {sum(p.numel() for p in value_net.parameters()):,}\")\nprint(f\"\\n\u2705 Models initialized and moved to {device}!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## \ud83c\udfaf SECTION 7 \u2014 Training Utilities\n\nEarly stopping, checkpointing, and metrics tracking."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class EarlyStopping:\n    \"\"\"\n    Early stopping to stop training when validation loss stops improving.\n    \"\"\"\n    \n    def __init__(self, patience=5, min_delta=0.001, verbose=True):\n        \"\"\"\n        Args:\n            patience: How many epochs to wait after last improvement\n            min_delta: Minimum change to qualify as an improvement\n            verbose: Whether to print messages\n        \"\"\"\n        self.patience = patience\n        self.min_delta = min_delta\n        self.verbose = verbose\n        self.counter = 0\n        self.best_loss = None\n        self.early_stop = False\n        self.best_epoch = 0\n    \n    def __call__(self, val_loss, epoch):\n        if self.best_loss is None:\n            self.best_loss = val_loss\n            self.best_epoch = epoch\n        elif val_loss > self.best_loss - self.min_delta:\n            self.counter += 1\n            if self.verbose:\n                print(f\"   Early stopping counter: {self.counter}/{self.patience}\")\n            if self.counter >= self.patience:\n                self.early_stop = True\n        else:\n            self.best_loss = val_loss\n            self.best_epoch = epoch\n            self.counter = 0\n    \n    def should_stop(self):\n        return self.early_stop\n\nprint(\"\u2705 EarlyStopping class defined!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def compute_accuracy(logits, targets, k_values=[1, 5]):\n    \"\"\"\n    Compute Top-K accuracy.\n    \n    Args:\n        logits: Model output logits (batch_size, vocab_size)\n        targets: Ground truth indices (batch_size,)\n        k_values: List of K values to compute (e.g., [1, 5])\n    \n    Returns:\n        Dictionary with Top-K accuracies\n    \"\"\"\n    batch_size = targets.size(0)\n    _, pred = logits.topk(max(k_values), dim=1, largest=True, sorted=True)\n    pred = pred.t()\n    correct = pred.eq(targets.view(1, -1).expand_as(pred))\n    \n    accuracies = {}\n    for k in k_values:\n        correct_k = correct[:k].reshape(-1).float().sum(0, keepdim=True)\n        accuracies[f'top{k}'] = (correct_k / batch_size * 100).item()\n    \n    return accuracies\n\nprint(\"\u2705 Accuracy computation function defined!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def save_checkpoint(model, optimizer, epoch, metrics, filepath, is_best=False):\n    \"\"\"\n    Save model checkpoint to Google Drive.\n    \n    Args:\n        model: PyTorch model\n        optimizer: Optimizer state\n        epoch: Current epoch number\n        metrics: Dictionary of training metrics\n        filepath: Path to save checkpoint\n        is_best: Whether this is the best model so far\n    \"\"\"\n    checkpoint = {\n        'epoch': epoch,\n        'model_state_dict': model.state_dict(),\n        'optimizer_state_dict': optimizer.state_dict(),\n        'metrics': metrics,\n        'timestamp': datetime.now().isoformat()\n    }\n    \n    torch.save(checkpoint, filepath)\n    \n    if is_best:\n        best_path = filepath.replace('.pth', '_best.pth')\n        torch.save(checkpoint, best_path)\n        print(f\"   \ud83d\udcbe Saved best model to: {best_path}\")\n    else:\n        print(f\"   \ud83d\udcbe Saved checkpoint to: {filepath}\")\n\ndef load_checkpoint(filepath, model, optimizer=None):\n    \"\"\"\n    Load model checkpoint from file.\n    \n    Args:\n        filepath: Path to checkpoint file\n        model: PyTorch model to load weights into\n        optimizer: Optional optimizer to restore state\n    \n    Returns:\n        epoch, metrics: Epoch number and training metrics from checkpoint\n    \"\"\"\n    checkpoint = torch.load(filepath, map_location=device)\n    model.load_state_dict(checkpoint['model_state_dict'])\n    \n    if optimizer is not None:\n        optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n    \n    print(f\"\u2705 Loaded checkpoint from epoch {checkpoint['epoch']}\")\n    return checkpoint['epoch'], checkpoint['metrics']\n\nprint(\"\u2705 Checkpoint functions defined!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## \ud83d\udcd6 SECTION 8 \u2014 Imitation Learning\n\nTrain policy network on master games with early stopping and validation."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def train_policy_network(model, train_loader, val_loader, epochs, learning_rate, \n                          save_dir, patience=5, save_every=2):\n    \"\"\"\n    Train policy network with early stopping and checkpointing.\n    \n    Args:\n        model: PolicyNetwork instance\n        train_loader: Training data loader\n        val_loader: Validation data loader\n        epochs: Maximum number of epochs\n        learning_rate: Learning rate for optimizer\n        save_dir: Directory to save checkpoints and logs\n        patience: Early stopping patience\n        save_every: Save checkpoint every N epochs\n    \n    Returns:\n        history: Dictionary with training metrics\n    \"\"\"\n    # Setup\n    optimizer = optim.Adam(model.parameters(), lr=learning_rate, \n                           weight_decay=CONFIG['weight_decay'])\n    criterion = nn.CrossEntropyLoss()\n    early_stopping = EarlyStopping(patience=patience, min_delta=CONFIG['min_delta'])\n    \n    # Tracking\n    history = {\n        'train_loss': [],\n        'val_loss': [],\n        'train_top1': [],\n        'train_top5': [],\n        'val_top1': [],\n        'val_top5': [],\n        'epochs': []\n    }\n    \n    best_val_loss = float('inf')\n    \n    print(\"\\n\" + \"=\"*60)\n    print(\"\ud83d\ude80 Starting Policy Network Training\")\n    print(\"=\"*60)\n    \n    for epoch in range(epochs):\n        # ===== TRAINING PHASE =====\n        model.train()\n        train_loss = 0.0\n        train_top1 = 0.0\n        train_top5 = 0.0\n        \n        pbar = tqdm(train_loader, desc=f\"Epoch {epoch+1}/{epochs} [Train]\")\n        for batch_idx, (boards, moves) in enumerate(pbar):\n            boards = boards.to(device)\n            moves = moves.to(device)\n            \n            # Forward pass\n            optimizer.zero_grad()\n            logits = model(boards)\n            loss = criterion(logits, moves)\n            \n            # Backward pass\n            loss.backward()\n            optimizer.step()\n            \n            # Compute metrics\n            train_loss += loss.item()\n            acc = compute_accuracy(logits, moves, k_values=[1, 5])\n            train_top1 += acc['top1']\n            train_top5 += acc['top5']\n            \n            # Update progress bar\n            pbar.set_postfix({\n                'loss': f\"{loss.item():.4f}\",\n                'top1': f\"{acc['top1']:.2f}%\"\n            })\n        \n        train_loss /= len(train_loader)\n        train_top1 /= len(train_loader)\n        train_top5 /= len(train_loader)\n        \n        # ===== VALIDATION PHASE =====\n        model.eval()\n        val_loss = 0.0\n        val_top1 = 0.0\n        val_top5 = 0.0\n        \n        with torch.no_grad():\n            pbar = tqdm(val_loader, desc=f\"Epoch {epoch+1}/{epochs} [Val]\")\n            for boards, moves in pbar:\n                boards = boards.to(device)\n                moves = moves.to(device)\n                \n                logits = model(boards)\n                loss = criterion(logits, moves)\n                \n                val_loss += loss.item()\n                acc = compute_accuracy(logits, moves, k_values=[1, 5])\n                val_top1 += acc['top1']\n                val_top5 += acc['top5']\n                \n                pbar.set_postfix({\n                    'loss': f\"{loss.item():.4f}\",\n                    'top1': f\"{acc['top1']:.2f}%\"\n                })\n        \n        val_loss /= len(val_loader)\n        val_top1 /= len(val_loader)\n        val_top5 /= len(val_loader)\n        \n        # Update history\n        history['train_loss'].append(train_loss)\n        history['val_loss'].append(val_loss)\n        history['train_top1'].append(train_top1)\n        history['train_top5'].append(train_top5)\n        history['val_top1'].append(val_top1)\n        history['val_top5'].append(val_top5)\n        history['epochs'].append(epoch + 1)\n        \n        # Print epoch summary\n        print(f\"\\n\ud83d\udcca Epoch {epoch+1} Summary:\")\n        print(f\"   Train Loss: {train_loss:.4f} | Val Loss: {val_loss:.4f}\")\n        print(f\"   Train Top-1: {train_top1:.2f}% | Val Top-1: {val_top1:.2f}%\")\n        print(f\"   Train Top-5: {train_top5:.2f}% | Val Top-5: {val_top5:.2f}%\")\n        \n        # Save checkpoint\n        is_best = val_loss < best_val_loss\n        if is_best:\n            best_val_loss = val_loss\n        \n        if (epoch + 1) % save_every == 0 or is_best:\n            checkpoint_path = os.path.join(save_dir, 'models', \n                                          f'policy_net_epoch_{epoch+1}.pth')\n            save_checkpoint(model, optimizer, epoch + 1, history, \n                          checkpoint_path, is_best=is_best)\n        \n        # Early stopping check\n        early_stopping(val_loss, epoch + 1)\n        if early_stopping.should_stop():\n            print(f\"\\n\u23f9\ufe0f  Early stopping triggered!\")\n            print(f\"   Best epoch: {early_stopping.best_epoch}\")\n            print(f\"   Best val loss: {early_stopping.best_loss:.4f}\")\n            break\n        \n        print()\n    \n    # Save final history\n    history_path = os.path.join(save_dir, 'logs', 'policy_training_history.json')\n    with open(history_path, 'w') as f:\n        json.dump(history, f, indent=2)\n    print(f\"\\n\ud83d\udcbe Training history saved to: {history_path}\")\n    \n    print(\"\\n\" + \"=\"*60)\n    print(\"\u2705 Training Complete!\")\n    print(\"=\"*60)\n    \n    return history\n\nprint(\"\u2705 Training function defined!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Training Example\n\nLoad dataset and train the policy network."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load dataset\ndataset = ChessDataset(CONFIG['pgn_path'], max_games=CONFIG['max_games'])\n\n# Create dataloaders\nif len(dataset) > 0:\n    train_loader, val_loader = create_dataloaders(\n        dataset, \n        batch_size=CONFIG['batch_size'],\n        train_split=CONFIG['train_val_split']\n    )\n    \n    # Train policy network\n    if CONFIG['mode'] in ['imitation', 'hybrid']:\n        print(\"\\n\ud83c\udfaf Training Policy Network...\")\n        history = train_policy_network(\n            policy_net,\n            train_loader,\n            val_loader,\n            epochs=CONFIG['epochs'],\n            learning_rate=CONFIG['learning_rate'],\n            save_dir=CONFIG['save_dir'],\n            patience=CONFIG['patience'],\n            save_every=CONFIG['save_every']\n        )\nelse:\n    print(\"\u26a0\ufe0f  No dataset loaded. Skipping training.\")\n    print(\"   Upload a PGN file to Google Drive and update CONFIG['pgn_path']\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## \ud83c\udfae SECTION 9 \u2014 Self-Play Reinforcement\n\nGenerate games through self-play and save them as PGN files."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def sample_legal_move_from_policy(board, policy_net):\n    \"\"\"\n    Sample a legal move from the policy network.\n    \n    Args:\n        board: chess.Board object\n        policy_net: PolicyNetwork model\n    \n    Returns:\n        chess.Move: Selected move\n    \"\"\"\n    # Get policy logits\n    board_tensor = board_to_tensor(board).unsqueeze(0).to(device)\n    with torch.no_grad():\n        logits = policy_net(board_tensor).squeeze(0)\n    \n    # Mask illegal moves\n    legal_moves = list(board.legal_moves)\n    legal_indices = []\n    for move in legal_moves:\n        idx = move_to_index(move, MOVE_TO_IDX)\n        if idx is not None:\n            legal_indices.append(idx)\n    \n    if not legal_indices:\n        # Fallback to random legal move\n        return random.choice(legal_moves)\n    \n    # Get probabilities for legal moves only\n    legal_logits = logits[legal_indices]\n    probs = F.softmax(legal_logits, dim=0)\n    \n    # Sample move\n    sampled_idx = torch.multinomial(probs, 1).item()\n    move_idx = legal_indices[sampled_idx]\n    \n    return index_to_move(move_idx, IDX_TO_MOVE)\n\nprint(\"\u2705 Move sampling function defined!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "PIECE_VALUES = {\n    chess.PAWN: 1,\n    chess.KNIGHT: 3,\n    chess.BISHOP: 3,\n    chess.ROOK: 5,\n    chess.QUEEN: 9,\n    chess.KING: 0\n}\n\ndef calculate_material_score(board):\n    \"\"\"Calculate material balance on the board.\"\"\"\n    white_score = 0\n    black_score = 0\n    \n    for square in chess.SQUARES:\n        piece = board.piece_at(square)\n        if piece:\n            value = PIECE_VALUES.get(piece.piece_type, 0)\n            if piece.color == chess.WHITE:\n                white_score += value\n            else:\n                black_score += value\n    \n    return white_score, black_score\n\nprint(\"\u2705 Material scoring function defined!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def self_play_game(policy_net, max_plies=200):\n    \"\"\"\n    Play a complete game using the policy network for both sides.\n    \n    Args:\n        policy_net: PolicyNetwork model\n        max_plies: Maximum number of moves (half-moves)\n    \n    Returns:\n        game: chess.pgn.Game object\n        result: Game result string ('1-0', '0-1', '1/2-1/2')\n    \"\"\"\n    board = chess.Board()\n    game = chess.pgn.Game()\n    game.headers[\"Event\"] = \"Self-Play\"\n    game.headers[\"Date\"] = datetime.now().strftime(\"%Y.%m.%d\")\n    game.headers[\"White\"] = \"PolicyNet\"\n    game.headers[\"Black\"] = \"PolicyNet\"\n    \n    node = game\n    ply_count = 0\n    \n    while not board.is_game_over() and ply_count < max_plies:\n        move = sample_legal_move_from_policy(board, policy_net)\n        board.push(move)\n        node = node.add_variation(move)\n        ply_count += 1\n    \n    # Determine result\n    if board.is_checkmate():\n        result = \"1-0\" if board.turn == chess.BLACK else \"0-1\"\n    elif board.is_stalemate() or board.is_insufficient_material():\n        result = \"1/2-1/2\"\n    elif ply_count >= max_plies:\n        # Game reached max plies - decide by material\n        white_mat, black_mat = calculate_material_score(board)\n        if white_mat > black_mat:\n            result = \"1-0\"\n        elif black_mat > white_mat:\n            result = \"0-1\"\n        else:\n            result = \"1/2-1/2\"\n    else:\n        result = \"1/2-1/2\"\n    \n    game.headers[\"Result\"] = result\n    return game, result\n\nprint(\"\u2705 Self-play function defined!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def generate_selfplay_games(policy_net, num_games, save_dir):\n    \"\"\"\n    Generate multiple self-play games and save them to Google Drive.\n    \n    Args:\n        policy_net: PolicyNetwork model\n        num_games: Number of games to generate\n        save_dir: Directory to save PGN files\n    \n    Returns:\n        statistics: Dictionary with game statistics\n    \"\"\"\n    stats = {\n        'white_wins': 0,\n        'black_wins': 0,\n        'draws': 0,\n        'total_plies': 0,\n        'games': []\n    }\n    \n    games_dir = os.path.join(save_dir, 'games')\n    os.makedirs(games_dir, exist_ok=True)\n    \n    print(f\"\\n\ud83c\udfae Generating {num_games} self-play games...\")\n    print(\"=\"*60)\n    \n    for i in tqdm(range(num_games), desc=\"Self-play\"):\n        game, result = self_play_game(policy_net, max_plies=CONFIG['max_plies'])\n        \n        # Update statistics\n        if result == \"1-0\":\n            stats['white_wins'] += 1\n        elif result == \"0-1\":\n            stats['black_wins'] += 1\n        else:\n            stats['draws'] += 1\n        \n        ply_count = len(list(game.mainline_moves()))\n        stats['total_plies'] += ply_count\n        stats['games'].append({\n            'game_number': i + 1,\n            'result': result,\n            'plies': ply_count\n        })\n        \n        # Save game to PGN file\n        pgn_path = os.path.join(games_dir, f'selfplay_game_{i+1}.pgn')\n        with open(pgn_path, 'w') as f:\n            print(game, file=f)\n    \n    # Calculate averages\n    stats['avg_plies'] = stats['total_plies'] / num_games if num_games > 0 else 0\n    \n    # Print summary\n    print(\"\\n\ud83d\udcca Self-Play Statistics:\")\n    print(f\"   White wins: {stats['white_wins']} ({stats['white_wins']/num_games*100:.1f}%)\")\n    print(f\"   Black wins: {stats['black_wins']} ({stats['black_wins']/num_games*100:.1f}%)\")\n    print(f\"   Draws:      {stats['draws']} ({stats['draws']/num_games*100:.1f}%)\")\n    print(f\"   Avg plies:  {stats['avg_plies']:.1f}\")\n    print(f\"\\n\ud83d\udcbe Games saved to: {games_dir}\")\n    \n    # Save statistics\n    stats_path = os.path.join(save_dir, 'logs', 'selfplay_stats.json')\n    with open(stats_path, 'w') as f:\n        json.dump(stats, f, indent=2)\n    \n    return stats\n\nprint(\"\u2705 Self-play generation function defined!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Generate self-play games (if in selfplay or hybrid mode)\nif CONFIG['mode'] in ['selfplay', 'hybrid']:\n    print(\"\\n\ud83c\udfae Starting self-play...\")\n    selfplay_stats = generate_selfplay_games(\n        policy_net,\n        num_games=CONFIG['selfplay_games'],\n        save_dir=CONFIG['save_dir']\n    )\nelse:\n    print(\"\u23ed\ufe0f  Skipping self-play (mode='imitation')\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## \ud83d\udcc8 SECTION 10 \u2014 Training Visualization\n\nVisualize training metrics and save plots to Google Drive."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def plot_training_history(history, save_dir):\n    \"\"\"\n    Plot training history and save figures to Google Drive.\n    \n    Args:\n        history: Dictionary with training metrics\n        save_dir: Directory to save plots\n    \"\"\"\n    if not history or 'epochs' not in history:\n        print(\"\u26a0\ufe0f  No training history available\")\n        return\n    \n    plots_dir = os.path.join(save_dir, 'plots')\n    os.makedirs(plots_dir, exist_ok=True)\n    \n    epochs = history['epochs']\n    \n    # Create figure with subplots\n    fig, axes = plt.subplots(1, 2, figsize=(15, 5))\n    \n    # Plot 1: Loss curves\n    ax = axes[0]\n    ax.plot(epochs, history['train_loss'], 'b-o', label='Train Loss', linewidth=2)\n    ax.plot(epochs, history['val_loss'], 'r-s', label='Val Loss', linewidth=2)\n    ax.set_xlabel('Epoch', fontsize=12)\n    ax.set_ylabel('Loss', fontsize=12)\n    ax.set_title('Training and Validation Loss', fontsize=14, fontweight='bold')\n    ax.legend(fontsize=11)\n    ax.grid(True, alpha=0.3)\n    \n    # Plot 2: Accuracy curves\n    ax = axes[1]\n    ax.plot(epochs, history['train_top1'], 'b-o', label='Train Top-1', linewidth=2)\n    ax.plot(epochs, history['val_top1'], 'r-s', label='Val Top-1', linewidth=2)\n    ax.plot(epochs, history['train_top5'], 'b--^', label='Train Top-5', linewidth=2, alpha=0.7)\n    ax.plot(epochs, history['val_top5'], 'r--v', label='Val Top-5', linewidth=2, alpha=0.7)\n    ax.set_xlabel('Epoch', fontsize=12)\n    ax.set_ylabel('Accuracy (%)', fontsize=12)\n    ax.set_title('Training and Validation Accuracy', fontsize=14, fontweight='bold')\n    ax.legend(fontsize=10)\n    ax.grid(True, alpha=0.3)\n    \n    plt.tight_layout()\n    \n    # Save figure\n    plot_path = os.path.join(plots_dir, 'training_history.png')\n    plt.savefig(plot_path, dpi=300, bbox_inches='tight')\n    print(f\"\ud83d\udcbe Training plots saved to: {plot_path}\")\n    \n    plt.show()\n\nprint(\"\u2705 Visualization function defined!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Visualize training history (if available)\nif 'history' in locals() and history:\n    plot_training_history(history, CONFIG['save_dir'])\nelse:\n    print(\"\u26a0\ufe0f  No training history to visualize\")\n    print(\"   Run training first to generate metrics\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## \u265f\ufe0f SECTION 11 \u2014 Play Against the Engine\n\nInteractive gameplay and move prediction."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def predict_best_move(board, policy_net):\n    \"\"\"\n    Predict the best move for the current position (greedy selection).\n    \n    Args:\n        board: chess.Board object\n        policy_net: PolicyNetwork model\n    \n    Returns:\n        chess.Move: Best predicted move\n    \"\"\"\n    # Get policy logits\n    board_tensor = board_to_tensor(board).unsqueeze(0).to(device)\n    with torch.no_grad():\n        logits = policy_net(board_tensor).squeeze(0)\n    \n    # Get legal moves\n    legal_moves = list(board.legal_moves)\n    legal_indices = []\n    legal_moves_list = []\n    \n    for move in legal_moves:\n        idx = move_to_index(move, MOVE_TO_IDX)\n        if idx is not None:\n            legal_indices.append(idx)\n            legal_moves_list.append(move)\n    \n    if not legal_indices:\n        return random.choice(legal_moves)\n    \n    # Get scores for legal moves\n    legal_logits = logits[legal_indices]\n    best_idx = torch.argmax(legal_logits).item()\n    \n    return legal_moves_list[best_idx]\n\nprint(\"\u2705 Move prediction function defined!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def play_against_engine(policy_net, player_color=chess.WHITE):\n    \"\"\"\n    Play a game against the trained engine.\n    \n    Args:\n        policy_net: PolicyNetwork model\n        player_color: chess.WHITE or chess.BLACK\n    \"\"\"\n    board = chess.Board()\n    print(\"\\n\" + \"=\"*60)\n    print(\"\u265f\ufe0f  Playing Against Chess Engine\")\n    print(\"=\"*60)\n    print(f\"You are playing as: {'White' if player_color == chess.WHITE else 'Black'}\")\n    print(\"Enter moves in UCI format (e.g., 'e2e4')\")\n    print(\"Type 'quit' to exit\\n\")\n    \n    while not board.is_game_over():\n        print(board)\n        print()\n        \n        if board.turn == player_color:\n            # Player's turn\n            while True:\n                move_str = input(\"Your move: \").strip().lower()\n                if move_str == 'quit':\n                    print(\"Game ended by player.\")\n                    return\n                \n                try:\n                    move = chess.Move.from_uci(move_str)\n                    if move in board.legal_moves:\n                        board.push(move)\n                        break\n                    else:\n                        print(\"\u274c Illegal move! Try again.\")\n                except:\n                    print(\"\u274c Invalid format! Use UCI notation (e.g., 'e2e4')\")\n        else:\n            # Engine's turn\n            print(\"\ud83e\udd16 Engine is thinking...\")\n            move = predict_best_move(board, policy_net)\n            print(f\"\ud83e\udd16 Engine plays: {move.uci()}\")\n            board.push(move)\n        \n        print()\n    \n    # Game over\n    print(board)\n    print(\"\\n\" + \"=\"*60)\n    print(\"\ud83c\udfc1 Game Over!\")\n    print(f\"Result: {board.result()}\")\n    print(\"=\"*60)\n\nprint(\"\u2705 Interactive play function defined!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Example: Display current board state\nexample_board = chess.Board()\nprint(\"Starting position:\")\nprint(example_board)\nprint(f\"\\n\u2705 Ready to play! Use play_against_engine(policy_net) to start a game.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## \ud83d\udcbe SECTION 12 \u2014 Model Management\n\nLoad and save models, resume training."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def load_best_model(save_dir, model, model_name='policy_net'):\n    \"\"\"\n    Load the best saved model checkpoint.\n    \n    Args:\n        save_dir: Base directory for saved models\n        model: Model instance to load weights into\n        model_name: Name of the model ('policy_net' or 'value_net')\n    \n    Returns:\n        epoch, metrics: Information from the checkpoint\n    \"\"\"\n    best_path = os.path.join(save_dir, 'models', f'{model_name}_best.pth')\n    \n    if not os.path.exists(best_path):\n        print(f\"\u26a0\ufe0f  No best model found at: {best_path}\")\n        return None, None\n    \n    checkpoint = torch.load(best_path, map_location=device)\n    model.load_state_dict(checkpoint['model_state_dict'])\n    \n    print(f\"\u2705 Loaded best {model_name} from epoch {checkpoint['epoch']}\")\n    print(f\"   Validation loss: {checkpoint['metrics']['val_loss'][-1]:.4f}\")\n    \n    return checkpoint['epoch'], checkpoint['metrics']\n\ndef list_saved_models(save_dir):\n    \"\"\"List all saved model checkpoints.\"\"\"\n    models_dir = os.path.join(save_dir, 'models')\n    \n    if not os.path.exists(models_dir):\n        print(f\"\u26a0\ufe0f  Models directory not found: {models_dir}\")\n        return\n    \n    model_files = sorted([f for f in os.listdir(models_dir) if f.endswith('.pth')])\n    \n    if not model_files:\n        print(\"No saved models found.\")\n        return\n    \n    print(\"\\n\ud83d\udcc1 Saved Models:\")\n    print(\"=\"*60)\n    for f in model_files:\n        path = os.path.join(models_dir, f)\n        size = os.path.getsize(path) / (1024 * 1024)  # MB\n        print(f\"  \u2022 {f:40s} ({size:.2f} MB)\")\n    print(\"=\"*60)\n\nprint(\"\u2705 Model management functions defined!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# List available models\nlist_saved_models(CONFIG['save_dir'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## \ud83c\udf89 Summary\n\n### \u2705 What's Included in v2.0:\n\n1. **Configuration Management**: All hyperparameters in one place\n2. **Google Drive Integration**: Persistent storage for models, games, and logs\n3. **Improved Architecture**: Batch normalization and dropout for better training\n4. **Training Utilities**: Early stopping, checkpointing, metrics tracking\n5. **Validation Split**: Proper train/val split with monitoring\n6. **Top-K Accuracy**: Top-1 and Top-5 accuracy metrics\n7. **Self-Play**: Generate and save games in PGN format\n8. **Visualization**: Training curves and accuracy plots\n9. **Model Management**: Save/load checkpoints, resume training\n10. **Interactive Play**: Play against the trained engine\n\n### \ud83d\ude80 Next Steps:\n\n1. **Upload your PGN data** to `/content/drive/MyDrive/ChessEngine/data/`\n2. **Run training** by executing the cells in order\n3. **Generate self-play games** to create additional training data\n4. **Visualize results** with the plotting functions\n5. **Play against your engine** to test its strength\n\n### \ud83d\udcda Tips:\n\n- Start with a small dataset (50-100 games) to test functionality\n- Monitor validation loss to avoid overfitting\n- Adjust `patience` and `min_delta` for early stopping\n- Save checkpoints frequently in case of interruption\n- Use self-play to generate unlimited training data\n\n### \ufffd\ufffd Troubleshooting:\n\n- **Out of memory**: Reduce `batch_size` in CONFIG\n- **Training too slow**: Use GPU runtime (Runtime \u2192 Change runtime type)\n- **No improvement**: Try different learning rates or architectures\n- **Overfitting**: Increase dropout or reduce model complexity\n\nHappy training! \ud83e\udde0\u265f\ufe0f"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}