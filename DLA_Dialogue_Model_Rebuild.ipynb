{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Building a Dialogue Language Model using DLA Sample Code\n",
        "\n",
        "**Author**: Deep Learning Academy  \n",
        "**Date**: {}\n",
        "\n",
        "This notebook demonstrates how to build a sequence-to-sequence dialogue model using the DLA (Deep Learning Academy) sample code for data processing. We'll use the `DialogueParser` class from `src/data_processor.py` to prepare our dialogue data and then build a transformer-based model for dialogue generation."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1.0 Background\n",
        "\n",
        "In this notebook, we will:\n",
        "1. Use the DLA DialogueParser to process dialogue data from various formats\n",
        "2. Build a sequence-to-sequence model for dialogue generation\n",
        "3. Train the model on context-response pairs\n",
        "4. Generate responses given dialogue context\n",
        "\n",
        "The DLA sample code (`src/data_processor.py`) provides a flexible parser that can handle:\n",
        "- Context-response pair format\n",
        "- Speaker-labeled dialogue format\n",
        "- Mixed formats with scene descriptions"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2.0 Setup and Import Dependencies"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Install required packages if needed\n",
        "# !pip install tensorflow keras numpy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "import sys\n",
        "import numpy as np\n",
        "import random\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "\n",
        "print(f\"TensorFlow version: {tf.__version__}\")\n",
        "print(f\"Keras version: {keras.__version__}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3.0 Import DLA Sample Code\n",
        "\n",
        "We'll import the DialogueParser from the DLA sample code to process our dialogue data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Add src directory to path to import the DLA sample code\n",
        "import sys\n",
        "sys.path.append('./src')\n",
        "\n",
        "# Import the DLA DialogueParser\n",
        "from data_processor import DialogueParser, DialogueTurn, DatasetStatistics\n",
        "\n",
        "print(\"DLA DialogueParser imported successfully!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4.0 Prepare Sample Dialogue Data\n",
        "\n",
        "For demonstration purposes, we'll create sample dialogue data. In production, you would use the DialogueParser to load data from files."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create sample dialogue data in context-response format\n",
        "sample_dialogues = \"\"\"\n",
        "context: Hello, how are you today?\n",
        "response: I'm doing well, thank you for asking! How about you?\n",
        "\n",
        "context: I'm doing well, thank you for asking! How about you?\n",
        "response: I'm great! What brings you here today?\n",
        "\n",
        "context: What brings you here today?\n",
        "response: I wanted to learn about building dialogue models.\n",
        "\n",
        "context: I wanted to learn about building dialogue models.\n",
        "response: That's wonderful! Dialogue models are fascinating.\n",
        "\n",
        "context: Can you help me with my project?\n",
        "response: Of course! I'd be happy to help. What do you need?\n",
        "\n",
        "context: I'd be happy to help. What do you need?\n",
        "response: I need to understand sequence-to-sequence models.\n",
        "\n",
        "context: What is machine learning?\n",
        "response: Machine learning is a subset of artificial intelligence.\n",
        "\n",
        "context: How does deep learning work?\n",
        "response: Deep learning uses neural networks with multiple layers.\n",
        "\n",
        "context: What are transformers in NLP?\n",
        "response: Transformers are neural network architectures for sequence processing.\n",
        "\n",
        "context: Tell me about attention mechanisms.\n",
        "response: Attention helps models focus on relevant parts of the input.\n",
        "\"\"\"\n",
        "\n",
        "# Save sample data to a file\n",
        "with open('sample_dialogues.txt', 'w') as f:\n",
        "    f.write(sample_dialogues)\n",
        "\n",
        "print(\"Sample dialogue data created!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5.0 Use DLA DialogueParser to Process Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Initialize the DLA DialogueParser\n",
        "parser = DialogueParser()\n",
        "\n",
        "# Parse the sample dialogue file\n",
        "dialogue_turns = parser.parse_file('sample_dialogues.txt')\n",
        "\n",
        "print(f\"Parsed {len(dialogue_turns)} dialogue turns\")\n",
        "print(f\"\\nFirst dialogue turn:\")\n",
        "print(f\"Context: {dialogue_turns[0].context}\")\n",
        "print(f\"Response: {dialogue_turns[0].response}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6.0 Calculate Dataset Statistics"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Use DLA DatasetStatistics to analyze the data\n",
        "stats = DatasetStatistics.calculate_stats(dialogue_turns)\n",
        "\n",
        "print(\"Dataset Statistics:\")\n",
        "for key, value in stats.items():\n",
        "    if isinstance(value, float):\n",
        "        print(f\"  {key}: {value:.2f}\")\n",
        "    else:\n",
        "        print(f\"  {key}: {value}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 7.0 Prepare Data for Model Training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Extract contexts and responses\n",
        "contexts = [turn.context for turn in dialogue_turns]\n",
        "responses = [\"[start] \" + turn.response + \" [end]\" for turn in dialogue_turns]\n",
        "\n",
        "# Create text pairs\n",
        "text_pairs = list(zip(contexts, responses))\n",
        "\n",
        "# Shuffle and split data\n",
        "random.seed(42)\n",
        "random.shuffle(text_pairs)\n",
        "\n",
        "val_samples = int(0.2 * len(text_pairs))\n",
        "train_samples = len(text_pairs) - val_samples\n",
        "\n",
        "train_pairs = text_pairs[:train_samples]\n",
        "val_pairs = text_pairs[train_samples:]\n",
        "\n",
        "print(f\"Training samples: {len(train_pairs)}\")\n",
        "print(f\"Validation samples: {len(val_pairs)}\")\n",
        "print(f\"\\nExample training pair:\")\n",
        "print(f\"Context: {train_pairs[0][0]}\")\n",
        "print(f\"Response: {train_pairs[0][1]}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 8.0 Vectorize Text Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from tensorflow.keras.layers import TextVectorization\n",
        "\n",
        "# Configuration\n",
        "vocab_size = 5000\n",
        "sequence_length = 20\n",
        "\n",
        "# Create text vectorization layers\n",
        "context_vectorization = TextVectorization(\n",
        "    max_tokens=vocab_size,\n",
        "    output_mode='int',\n",
        "    output_sequence_length=sequence_length,\n",
        ")\n",
        "\n",
        "response_vectorization = TextVectorization(\n",
        "    max_tokens=vocab_size,\n",
        "    output_mode='int',\n",
        "    output_sequence_length=sequence_length + 1,  # +1 for start/end tokens\n",
        ")\n",
        "\n",
        "# Adapt to the training data\n",
        "train_contexts = [pair[0] for pair in train_pairs]\n",
        "train_responses = [pair[1] for pair in train_pairs]\n",
        "\n",
        "context_vectorization.adapt(train_contexts)\n",
        "response_vectorization.adapt(train_responses)\n",
        "\n",
        "print(f\"Context vocabulary size: {context_vectorization.vocabulary_size()}\")\n",
        "print(f\"Response vocabulary size: {response_vectorization.vocabulary_size()}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 9.0 Create Training Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def format_dataset(contexts, responses):\n",
        "    contexts = context_vectorization(contexts)\n",
        "    responses = response_vectorization(responses)\n",
        "    return ({\n",
        "        \"context\": contexts,\n",
        "        \"response\": responses[:, :-1],\n",
        "    }, responses[:, 1:])\n",
        "\n",
        "def make_dataset(pairs, batch_size=64):\n",
        "    contexts_list = [pair[0] for pair in pairs]\n",
        "    responses_list = [pair[1] for pair in pairs]\n",
        "    dataset = tf.data.Dataset.from_tensor_slices((contexts_list, responses_list))\n",
        "    dataset = dataset.batch(batch_size)\n",
        "    dataset = dataset.map(format_dataset, num_parallel_calls=tf.data.AUTOTUNE)\n",
        "    return dataset.shuffle(2048).prefetch(tf.data.AUTOTUNE).cache()\n",
        "\n",
        "train_ds = make_dataset(train_pairs)\n",
        "val_ds = make_dataset(val_pairs)\n",
        "\n",
        "print(\"Training dataset created successfully!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 10.0 Build Sequence-to-Sequence Model with Transformer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from tensorflow.keras import layers\n",
        "\n",
        "class TransformerEncoder(layers.Layer):\n",
        "    def __init__(self, embed_dim, dense_dim, num_heads, **kwargs):\n",
        "        super().__init__(**kwargs)\n",
        "        self.embed_dim = embed_dim\n",
        "        self.dense_dim = dense_dim\n",
        "        self.num_heads = num_heads\n",
        "        self.attention = layers.MultiHeadAttention(\n",
        "            num_heads=num_heads, key_dim=embed_dim\n",
        "        )\n",
        "        self.dense_proj = keras.Sequential([\n",
        "            layers.Dense(dense_dim, activation='relu'),\n",
        "            layers.Dense(embed_dim),\n",
        "        ])\n",
        "        self.layernorm_1 = layers.LayerNormalization()\n",
        "        self.layernorm_2 = layers.LayerNormalization()\n",
        "\n",
        "    def call(self, inputs, mask=None):\n",
        "        if mask is not None:\n",
        "            mask = mask[:, tf.newaxis, :]\n",
        "        attention_output = self.attention(inputs, inputs, attention_mask=mask)\n",
        "        proj_input = self.layernorm_1(inputs + attention_output)\n",
        "        proj_output = self.dense_proj(proj_input)\n",
        "        return self.layernorm_2(proj_input + proj_output)\n",
        "\n",
        "    def get_config(self):\n",
        "        config = super().get_config()\n",
        "        config.update({\n",
        "            \"embed_dim\": self.embed_dim,\n",
        "            \"dense_dim\": self.dense_dim,\n",
        "            \"num_heads\": self.num_heads,\n",
        "        })\n",
        "        return config\n",
        "\n",
        "print(\"TransformerEncoder defined\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class PositionalEmbedding(layers.Layer):\n",
        "    def __init__(self, sequence_length, input_dim, output_dim, **kwargs):\n",
        "        super().__init__(**kwargs)\n",
        "        self.token_embeddings = layers.Embedding(\n",
        "            input_dim=input_dim, output_dim=output_dim\n",
        "        )\n",
        "        self.position_embeddings = layers.Embedding(\n",
        "            input_dim=sequence_length, output_dim=output_dim\n",
        "        )\n",
        "        self.sequence_length = sequence_length\n",
        "        self.input_dim = input_dim\n",
        "        self.output_dim = output_dim\n",
        "\n",
        "    def call(self, inputs):\n",
        "        length = tf.shape(inputs)[-1]\n",
        "        positions = tf.range(start=0, limit=length, delta=1)\n",
        "        embedded_tokens = self.token_embeddings(inputs)\n",
        "        embedded_positions = self.position_embeddings(positions)\n",
        "        return embedded_tokens + embedded_positions\n",
        "\n",
        "    def compute_mask(self, inputs, mask=None):\n",
        "        return tf.math.not_equal(inputs, 0)\n",
        "\n",
        "    def get_config(self):\n",
        "        config = super().get_config()\n",
        "        config.update({\n",
        "            \"sequence_length\": self.sequence_length,\n",
        "            \"input_dim\": self.input_dim,\n",
        "            \"output_dim\": self.output_dim,\n",
        "        })\n",
        "        return config\n",
        "\n",
        "print(\"PositionalEmbedding defined\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class TransformerDecoder(layers.Layer):\n",
        "    def __init__(self, embed_dim, dense_dim, num_heads, **kwargs):\n",
        "        super().__init__(**kwargs)\n",
        "        self.embed_dim = embed_dim\n",
        "        self.dense_dim = dense_dim\n",
        "        self.num_heads = num_heads\n",
        "        self.attention_1 = layers.MultiHeadAttention(\n",
        "            num_heads=num_heads, key_dim=embed_dim\n",
        "        )\n",
        "        self.attention_2 = layers.MultiHeadAttention(\n",
        "            num_heads=num_heads, key_dim=embed_dim\n",
        "        )\n",
        "        self.dense_proj = keras.Sequential([\n",
        "            layers.Dense(dense_dim, activation='relu'),\n",
        "            layers.Dense(embed_dim),\n",
        "        ])\n",
        "        self.layernorm_1 = layers.LayerNormalization()\n",
        "        self.layernorm_2 = layers.LayerNormalization()\n",
        "        self.layernorm_3 = layers.LayerNormalization()\n",
        "        self.supports_masking = True\n",
        "\n",
        "    def get_causal_attention_mask(self, inputs):\n",
        "        input_shape = tf.shape(inputs)\n",
        "        batch_size, sequence_length = input_shape[0], input_shape[1]\n",
        "        i = tf.range(sequence_length)[:, tf.newaxis]\n",
        "        j = tf.range(sequence_length)\n",
        "        mask = tf.cast(i >= j, dtype='int32')\n",
        "        mask = tf.reshape(mask, (1, input_shape[1], input_shape[1]))\n",
        "        mult = tf.concat(\n",
        "            [tf.expand_dims(batch_size, -1), tf.constant([1, 1], dtype=tf.int32)],\n",
        "            axis=0,\n",
        "        )\n",
        "        return tf.tile(mask, mult)\n",
        "\n",
        "    def call(self, inputs, encoder_outputs, mask=None):\n",
        "        causal_mask = self.get_causal_attention_mask(inputs)\n",
        "        if mask is not None:\n",
        "            padding_mask = tf.cast(mask[:, tf.newaxis, :], dtype='int32')\n",
        "            padding_mask = tf.minimum(padding_mask, causal_mask)\n",
        "        else:\n",
        "            padding_mask = causal_mask\n",
        "        attention_output_1 = self.attention_1(\n",
        "            query=inputs,\n",
        "            value=inputs,\n",
        "            key=inputs,\n",
        "            attention_mask=causal_mask,\n",
        "        )\n",
        "        attention_output_1 = self.layernorm_1(inputs + attention_output_1)\n",
        "        attention_output_2 = self.attention_2(\n",
        "            query=attention_output_1,\n",
        "            value=encoder_outputs,\n",
        "            key=encoder_outputs,\n",
        "            attention_mask=padding_mask,\n",
        "        )\n",
        "        attention_output_2 = self.layernorm_2(attention_output_1 + attention_output_2)\n",
        "        proj_output = self.dense_proj(attention_output_2)\n",
        "        return self.layernorm_3(attention_output_2 + proj_output)\n",
        "\n",
        "    def get_config(self):\n",
        "        config = super().get_config()\n",
        "        config.update({\n",
        "            \"embed_dim\": self.embed_dim,\n",
        "            \"dense_dim\": self.dense_dim,\n",
        "            \"num_heads\": self.num_heads,\n",
        "        })\n",
        "        return config\n",
        "\n",
        "print(\"TransformerDecoder defined\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 11.0 Build Complete Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Model configuration\n",
        "embed_dim = 256\n",
        "dense_dim = 2048\n",
        "num_heads = 8\n",
        "\n",
        "# Encoder\n",
        "encoder_inputs = keras.Input(shape=(None,), dtype='int64', name='context')\n",
        "x = PositionalEmbedding(sequence_length, vocab_size, embed_dim)(encoder_inputs)\n",
        "encoder_outputs = TransformerEncoder(embed_dim, dense_dim, num_heads)(x)\n",
        "\n",
        "# Decoder\n",
        "decoder_inputs = keras.Input(shape=(None,), dtype='int64', name='response')\n",
        "x = PositionalEmbedding(sequence_length, vocab_size, embed_dim)(decoder_inputs)\n",
        "x = TransformerDecoder(embed_dim, dense_dim, num_heads)(x, encoder_outputs)\n",
        "x = layers.Dropout(0.5)(x)\n",
        "decoder_outputs = layers.Dense(vocab_size, activation='softmax')(x)\n",
        "\n",
        "# Create model\n",
        "dialogue_model = keras.Model([encoder_inputs, decoder_inputs], decoder_outputs)\n",
        "\n",
        "print(\"Model built successfully!\")\n",
        "dialogue_model.summary()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 12.0 Compile and Train the Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Compile the model\n",
        "dialogue_model.compile(\n",
        "    optimizer='adam',\n",
        "    loss='sparse_categorical_crossentropy',\n",
        "    metrics=['accuracy']\n",
        ")\n",
        "\n",
        "print(\"Model compiled!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Train the model\n",
        "# Note: For better results, train for more epochs with more data\n",
        "history = dialogue_model.fit(\n",
        "    train_ds,\n",
        "    epochs=10,\n",
        "    validation_data=val_ds,\n",
        ")\n",
        "\n",
        "print(\"Training complete!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 13.0 Generate Responses\n",
        "\n",
        "Now we'll create a function to generate dialogue responses given a context."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def decode_sequence(input_context):\n",
        "    \"\"\"\n",
        "    Generate a response given an input context.\n",
        "    \"\"\"\n",
        "    # Vectorize the input context\n",
        "    tokenized_input = context_vectorization([input_context])\n",
        "    \n",
        "    # Initialize the response with the start token\n",
        "    decoded_response = \"[start]\"\n",
        "    \n",
        "    for i in range(sequence_length):\n",
        "        # Vectorize the current decoded response\n",
        "        tokenized_target = response_vectorization([decoded_response])\n",
        "        \n",
        "        # Predict the next token\n",
        "        predictions = dialogue_model.predict(\n",
        "            [tokenized_input, tokenized_target],\n",
        "            verbose=0\n",
        "        )\n",
        "        \n",
        "        # Get the most likely next token\n",
        "        sampled_token_index = np.argmax(predictions[0, i, :])\n",
        "        \n",
        "        # Get the word from vocabulary\n",
        "        sampled_token = response_vectorization.get_vocabulary()[sampled_token_index]\n",
        "        \n",
        "        # Append to the decoded response\n",
        "        decoded_response += \" \" + sampled_token\n",
        "        \n",
        "        # Stop if we hit the end token\n",
        "        if sampled_token == \"[end]\":\n",
        "            break\n",
        "    \n",
        "    # Remove start and end tokens\n",
        "    decoded_response = decoded_response.replace(\"[start] \", \"\")\n",
        "    decoded_response = decoded_response.replace(\" [end]\", \"\")\n",
        "    \n",
        "    return decoded_response\n",
        "\n",
        "print(\"Response generation function defined!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 14.0 Test the Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Test the model with various contexts\n",
        "test_contexts = [\n",
        "    \"Hello, how are you today?\",\n",
        "    \"What brings you here today?\",\n",
        "    \"Can you help me with my project?\",\n",
        "    \"What is machine learning?\",\n",
        "    \"Tell me about deep learning.\"\n",
        "]\n",
        "\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"GENERATING DIALOGUE RESPONSES\")\n",
        "print(\"=\"*80 + \"\\n\")\n",
        "\n",
        "for context in test_contexts:\n",
        "    response = decode_sequence(context)\n",
        "    print(f\"Context:  {context}\")\n",
        "    print(f\"Response: {response}\")\n",
        "    print(\"-\" * 80)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 15.0 Using DLA Sample Code for Production Data\n",
        "\n",
        "The above example uses sample data. For production use, you can use the DLA DialogueParser to load and process real dialogue datasets:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Example: Load dialogue data from a directory using DLA sample code\n",
        "\"\"\"\n",
        "# Initialize parser\n",
        "parser = DialogueParser()\n",
        "\n",
        "# Parse all dialogue files in a directory\n",
        "dialogue_turns = parser.parse_directory(\n",
        "    directory='/path/to/dialogue/dataset',\n",
        "    pattern='*.txt'\n",
        ")\n",
        "\n",
        "# Calculate statistics\n",
        "stats = DatasetStatistics.calculate_stats(dialogue_turns)\n",
        "print('Dataset Statistics:', stats)\n",
        "\n",
        "# Convert to training format\n",
        "jsonl_output = parser.to_training_format(dialogue_turns, 'jsonl')\n",
        "\n",
        "# Extract context-response pairs for model training\n",
        "contexts = [turn.context for turn in dialogue_turns]\n",
        "responses = ['[start] ' + turn.response + ' [end]' for turn in dialogue_turns]\n",
        "\n",
        "# Continue with model training as shown above...\n",
        "\"\"\"\n",
        "\n",
        "print(\"See the code above for production usage with real dialogue datasets.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 16.0 Summary\n",
        "\n",
        "In this notebook, we:\n",
        "1. \u2705 Imported and used the DLA DialogueParser sample code\n",
        "2. \u2705 Processed dialogue data using the context-response parser\n",
        "3. \u2705 Built a transformer-based sequence-to-sequence model\n",
        "4. \u2705 Trained the model on dialogue pairs\n",
        "5. \u2705 Generated responses given dialogue contexts\n",
        "\n",
        "### Key Takeaways:\n",
        "- The DLA sample code (`src/data_processor.py`) provides flexible dialogue parsing\n",
        "- Transformer architecture works well for dialogue generation\n",
        "- The model learns to generate contextually relevant responses\n",
        "- For better results, use larger datasets and train for more epochs\n",
        "\n",
        "### Next Steps:\n",
        "- Use the DialogueParser to load your own dialogue datasets\n",
        "- Experiment with different model architectures\n",
        "- Fine-tune hyperparameters for better performance\n",
        "- Implement beam search for better response generation"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}