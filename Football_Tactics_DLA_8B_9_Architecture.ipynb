{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# âš½ Football Tactics Transformer - Using DLA 8B & 9 Architectures\n",
    "\n",
    "**Author**: Deep Learning Academy\n",
    "**Date**: 2026-02-12\n",
    "\n",
    "## Architecture Source\n",
    "This notebook uses the EXACT transformer architectures from:\n",
    "- **Notebook 8B**: `TransformerEncoder` and `TransformerDecoder` with cross-attention\n",
    "- **Notebook 9**: `WarmupSchedule`, temperature sampling, and top-K sampling\n",
    "\n",
    "## What We're Building\n",
    "A sequence-to-sequence model that:\n",
    "1. **Encodes** game state (formation, ball position, score)\n",
    "2. **Decodes** tactical sequences (player actions)\n",
    "3. Uses **real match data** from Football Data APIs\n",
    "4. Generates **diverse tactics** with temperature and top-K sampling\n",
    "\n",
    "## Data Sources\n",
    "- **StatsBomb Open Data**: https://github.com/statsbomb/open-data\n",
    "- **Football-Data.org API**: https://www.football-data.org/\n",
    "- Automatic fallback to high-quality synthetic data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup and Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages\n",
    "!pip install -q tensorflow keras numpy pandas matplotlib seaborn requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "import json\n",
    "import requests\n",
    "import string\n",
    "import re\n",
    "from datetime import datetime\n",
    "from functools import partial\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# TensorFlow and Keras\n",
    "import tensorflow as tf\n",
    "import keras\n",
    "from keras import layers, ops\n",
    "from keras.layers import TextVectorization\n",
    "\n",
    "# Visualization\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Set seeds for reproducibility\n",
    "random.seed(42)\n",
    "np.random.seed(42)\n",
    "tf.random.set_seed(42)\n",
    "\n",
    "print(f\"âœ“ TensorFlow {tf.__version__}\")\n",
    "print(f\"âœ“ Keras {keras.__version__}\")\n",
    "print(f\"âœ“ NumPy {np.__version__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Real Football Data API Integration\n",
    "\n",
    "**Data Sources:**\n",
    "- StatsBomb Open Data (La Liga, Champions League, World Cup)\n",
    "- Football-Data.org API\n",
    "- Automatic fallback to synthetic data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FootballDataLoader:\n",
    "    \"\"\"Load real football match data from multiple APIs\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.statsbomb_base = \"https://raw.githubusercontent.com/statsbomb/open-data/master/data\"\n",
    "        self.competitions = {\n",
    "            'La Liga 2020/21': (11, 90),\n",
    "            'Premier League 2003/04': (2, 44),\n",
    "            'Champions League 2018/19': (16, 4),\n",
    "            'World Cup 2018': (43, 3)\n",
    "        }\n",
    "    \n",
    "    def fetch_matches(self, competition='La Liga 2020/21'):\n",
    "        \"\"\"Fetch match data from StatsBomb\"\"\"\n",
    "        try:\n",
    "            comp_id, season_id = self.competitions.get(competition, (11, 90))\n",
    "            url = f\"{self.statsbomb_base}/matches/{comp_id}/{season_id}.json\"\n",
    "            response = requests.get(url, timeout=10)\n",
    "            response.raise_for_status()\n",
    "            matches = response.json()\n",
    "            print(f\"âœ“ Loaded {len(matches)} matches from {competition}\")\n",
    "            return matches\n",
    "        except Exception as e:\n",
    "            print(f\"âš  API unavailable: {str(e)}\")\n",
    "            return None\n",
    "    \n",
    "    def fetch_events(self, match_id):\n",
    "        \"\"\"Fetch detailed event data from a match\"\"\"\n",
    "        try:\n",
    "            url = f\"{self.statsbomb_base}/events/{match_id}.json\"\n",
    "            response = requests.get(url, timeout=10)\n",
    "            response.raise_for_status()\n",
    "            return response.json()\n",
    "        except:\n",
    "            return None\n",
    "    \n",
    "    def extract_formation(self, events):\n",
    "        \"\"\"Extract team formation from events\"\"\"\n",
    "        for event in events[:50]:\n",
    "            if event.get('type', {}).get('name') == 'Starting XI':\n",
    "                formation = event.get('tactics', {}).get('formation')\n",
    "                if formation:\n",
    "                    f = str(formation)\n",
    "                    if len(f) == 3:\n",
    "                        return f\"{f[0]}-{f[1]}-{f[2]}\"\n",
    "                    return formation\n",
    "        return '4-3-3'\n",
    "    \n",
    "    def process_events_to_tactics(self, events, max_sequences=10):\n",
    "        \"\"\"Convert match events to tactical sequences\"\"\"\n",
    "        tactics = []\n",
    "        \n",
    "        # Map StatsBomb events to tactical actions\n",
    "        action_map = {\n",
    "            'Pass': 'pass',\n",
    "            'Dribble': 'dribble', \n",
    "            'Shot': 'shoot',\n",
    "            'Pressure': 'press',\n",
    "            'Interception': 'intercept',\n",
    "            'Tackle': 'tackle',\n",
    "            'Clearance': 'clear',\n",
    "            'Cross': 'cross'\n",
    "        }\n",
    "        \n",
    "        position_map = {\n",
    "            'Goalkeeper': 'GK',\n",
    "            'Left Back': 'LB',\n",
    "            'Right Back': 'RB',\n",
    "            'Center Back': 'CB',\n",
    "            'Left Wing Back': 'LWB',\n",
    "            'Right Wing Back': 'RWB',\n",
    "            'Defensive Midfield': 'CDM',\n",
    "            'Center Midfield': 'CM',\n",
    "            'Left Midfield': 'LM',\n",
    "            'Right Midfield': 'RM',\n",
    "            'Attacking Midfield': 'CAM',\n",
    "            'Left Wing': 'LW',\n",
    "            'Right Wing': 'RW',\n",
    "            'Center Forward': 'CF',\n",
    "            'Striker': 'ST'\n",
    "        }\n",
    "        \n",
    "        current_sequence = []\n",
    "        last_zone = 'midfield'\n",
    "        \n",
    "        for event in events:\n",
    "            event_type = event.get('type', {}).get('name', '')\n",
    "            \n",
    "            if event_type in action_map:\n",
    "                # Get location for ball zone\n",
    "                location = event.get('location', [52, 34])\n",
    "                x = location[0] if len(location) > 0 else 52\n",
    "                \n",
    "                if x < 35:\n",
    "                    zone = 'defense'\n",
    "                elif x < 75:\n",
    "                    zone = 'midfield'\n",
    "                else:\n",
    "                    zone = 'attack'\n",
    "                \n",
    "                last_zone = zone\n",
    "                \n",
    "                # Get position (simplified)\n",
    "                player_position = event.get('position', {}).get('name', '')\n",
    "                position = position_map.get(player_position, random.choice(['DEF', 'MID', 'FWD']))\n",
    "                \n",
    "                # Get action\n",
    "                action = action_map[event_type]\n",
    "                \n",
    "                # Get direction\n",
    "                if 'pass' in event and 'end_location' in event['pass']:\n",
    "                    end_x = event['pass']['end_location'][0]\n",
    "                    if end_x > x + 10:\n",
    "                        direction = 'forward'\n",
    "                    elif end_x < x - 10:\n",
    "                        direction = 'back'\n",
    "                    else:\n",
    "                        end_y = event['pass']['end_location'][1]\n",
    "                        direction = 'left' if end_y < 34 else 'right'\n",
    "                else:\n",
    "                    direction = random.choice(['forward', 'center', 'left', 'right'])\n",
    "                \n",
    "                current_sequence.append(f\"{position} {action} {direction}\")\n",
    "                \n",
    "                # Complete sequence every 3-5 actions\n",
    "                if len(current_sequence) >= random.randint(3, 5):\n",
    "                    tactics.append({\n",
    "                        'sequence': ' , '.join(current_sequence),\n",
    "                        'zone': last_zone\n",
    "                    })\n",
    "                    current_sequence = []\n",
    "                    \n",
    "                    if len(tactics) >= max_sequences:\n",
    "                        break\n",
    "        \n",
    "        return tactics\n",
    "    \n",
    "    def load_training_data(self, num_matches=20, competition='La Liga 2020/21'):\n",
    "        \"\"\"Load and process training data from real matches\"\"\"\n",
    "        matches = self.fetch_matches(competition)\n",
    "        \n",
    "        if not matches:\n",
    "            return self.generate_synthetic_data()\n",
    "        \n",
    "        training_data = []\n",
    "        \n",
    "        for match in matches[:num_matches]:\n",
    "            match_id = match.get('match_id')\n",
    "            home_team = match.get('home_team', {}).get('home_team_name', 'Team A')\n",
    "            away_team = match.get('away_team', {}).get('away_team_name', 'Team B')\n",
    "            home_score = match.get('home_score', 0)\n",
    "            away_score = match.get('away_score', 0)\n",
    "            \n",
    "            # Determine status\n",
    "            if home_score > away_score:\n",
    "                status = 'winning'\n",
    "            elif home_score < away_score:\n",
    "                status = 'losing'\n",
    "            else:\n",
    "                status = 'drawing'\n",
    "            \n",
    "            # Get events\n",
    "            events = self.fetch_events(match_id)\n",
    "            if not events:\n",
    "                continue\n",
    "            \n",
    "            formation = self.extract_formation(events)\n",
    "            tactics_list = self.process_events_to_tactics(events, max_sequences=8)\n",
    "            \n",
    "            for tactic in tactics_list:\n",
    "                game_state = f\"formation {formation} ball {tactic['zone']} status {status}\"\n",
    "                training_data.append({\n",
    "                    'game_state': game_state,\n",
    "                    'tactics': f\"[start] {tactic['sequence']} [end]\",\n",
    "                    'team': home_team,\n",
    "                    'match_id': match_id\n",
    "                })\n",
    "        \n",
    "        print(f\"âœ“ Processed {len(training_data)} tactical sequences from real matches\")\n",
    "        return training_data\n",
    "    \n",
    "    def generate_synthetic_data(self, num_samples=800):\n",
    "        \"\"\"Generate high-quality synthetic data as fallback\"\"\"\n",
    "        print(\"âš  Using synthetic data (APIs unavailable)\")\n",
    "        \n",
    "        formations = ['4-4-2', '4-3-3', '3-5-2', '4-2-3-1', '5-3-2', '3-4-3']\n",
    "        zones = ['defense', 'midfield', 'attack']\n",
    "        statuses = ['losing', 'drawing', 'winning']\n",
    "        positions = ['GK', 'LB', 'CB', 'RB', 'CDM', 'CM', 'CAM', 'LW', 'RW', 'ST', 'CF']\n",
    "        actions = ['pass', 'dribble', 'shoot', 'cross', 'tackle', 'intercept', 'press', 'clear']\n",
    "        directions = ['left', 'right', 'center', 'forward', 'back', 'wide']\n",
    "        \n",
    "        data = []\n",
    "        for i in range(num_samples):\n",
    "            formation = random.choice(formations)\n",
    "            zone = random.choice(zones)\n",
    "            status = random.choice(statuses)\n",
    "            \n",
    "            game_state = f\"formation {formation} ball {zone} status {status}\"\n",
    "            \n",
    "            num_actions = random.randint(3, 6)\n",
    "            sequence = []\n",
    "            for _ in range(num_actions):\n",
    "                pos = random.choice(positions)\n",
    "                act = random.choice(actions)\n",
    "                dir = random.choice(directions)\n",
    "                sequence.append(f\"{pos} {act} {dir}\")\n",
    "            \n",
    "            data.append({\n",
    "                'game_state': game_state,\n",
    "                'tactics': f\"[start] {' , '.join(sequence)} [end]\",\n",
    "                'team': f\"Team_{i % 20}\",\n",
    "                'match_id': f\"synthetic_{i}\"\n",
    "            })\n",
    "        \n",
    "        return data\n",
    "\n",
    "# Initialize data loader\n",
    "data_loader = FootballDataLoader()\n",
    "print(\"âœ“ Football Data Loader initialized\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Load Real Match Data from APIs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load training data from StatsBomb API\n",
    "print(\"Loading match data from StatsBomb API...\\n\")\n",
    "\n",
    "training_data = data_loader.load_training_data(\n",
    "    num_matches=25,\n",
    "    competition='La Liga 2020/21'\n",
    ")\n",
    "\n",
    "# Convert to DataFrame\n",
    "df = pd.DataFrame(training_data)\n",
    "\n",
    "print(f\"\\nâœ“ Loaded {len(df)} training samples\")\n",
    "print(f\"\\nDataset info:\")\n",
    "print(f\"  Teams: {df['team'].nunique()}\")\n",
    "print(f\"  Matches: {df['match_id'].nunique()}\")\n",
    "print(f\"\\nSample data:\")\n",
    "print(df.head(3).to_string())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Data Preparation\n",
    "\n",
    "Following the pattern from Notebook 8B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract sequences\n",
    "game_states = df['game_state'].tolist()\n",
    "tactic_sequences = df['tactics'].tolist()\n",
    "\n",
    "# Create pairs and shuffle\n",
    "text_pairs = list(zip(game_states, tactic_sequences))\n",
    "random.shuffle(text_pairs)\n",
    "\n",
    "# Split into train/val\n",
    "val_samples = int(0.15 * len(text_pairs))\n",
    "train_pairs = text_pairs[:-val_samples]\n",
    "val_pairs = text_pairs[-val_samples:]\n",
    "\n",
    "print(f\"Training samples: {len(train_pairs)}\")\n",
    "print(f\"Validation samples: {len(val_pairs)}\")\n",
    "print(f\"\\nExample:\")\n",
    "print(f\"  Game State: {train_pairs[0][0]}\")\n",
    "print(f\"  Tactics: {train_pairs[0][1]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Custom standardization function (from 8B)\n",
    "strip_chars = string.punctuation\n",
    "strip_chars = strip_chars.replace(\"[\", \"\")\n",
    "strip_chars = strip_chars.replace(\"]\", \"\")\n",
    "\n",
    "def custom_standardization(input_string):\n",
    "    lowercase = tf.strings.lower(input_string)\n",
    "    return tf.strings.regex_replace(\n",
    "        lowercase, f\"[{re.escape(strip_chars)}]\", \"\"\n",
    "    )\n",
    "\n",
    "# Configuration\n",
    "vocab_size = 500\n",
    "sequence_length = 20\n",
    "batch_size = 64\n",
    "\n",
    "# Text vectorization layers (from 8B)\n",
    "source_vectorization = TextVectorization(\n",
    "    max_tokens=vocab_size,\n",
    "    output_mode=\"int\",\n",
    "    output_sequence_length=sequence_length,\n",
    "    standardize=custom_standardization,\n",
    ")\n",
    "\n",
    "target_vectorization = TextVectorization(\n",
    "    max_tokens=vocab_size,\n",
    "    output_mode=\"int\",\n",
    "    output_sequence_length=sequence_length + 1,\n",
    "    standardize=custom_standardization,\n",
    ")\n",
    "\n",
    "# Adapt to training data\n",
    "train_source_texts = [pair[0] for pair in train_pairs]\n",
    "train_target_texts = [pair[1] for pair in train_pairs]\n",
    "\n",
    "source_vectorization.adapt(train_source_texts)\n",
    "target_vectorization.adapt(train_target_texts)\n",
    "\n",
    "print(f\"Source vocab size: {source_vectorization.vocabulary_size()}\")\n",
    "print(f\"Target vocab size: {target_vectorization.vocabulary_size()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create datasets (from 8B pattern)\n",
    "def format_dataset(eng, spa):\n",
    "    eng = source_vectorization(eng)\n",
    "    spa = target_vectorization(spa)\n",
    "    return ({\"english\": eng, \"spanish\": spa[:, :-1]}, spa[:, 1:])\n",
    "\n",
    "def make_dataset(pairs):\n",
    "    eng_texts, spa_texts = zip(*pairs)\n",
    "    eng_texts = list(eng_texts)\n",
    "    spa_texts = list(spa_texts)\n",
    "    dataset = tf.data.Dataset.from_tensor_slices((eng_texts, spa_texts))\n",
    "    dataset = dataset.batch(batch_size)\n",
    "    dataset = dataset.map(format_dataset, num_parallel_calls=4)\n",
    "    return dataset.shuffle(2048).prefetch(16).cache()\n",
    "\n",
    "train_ds = make_dataset(train_pairs)\n",
    "val_ds = make_dataset(val_pairs)\n",
    "\n",
    "print(\"âœ“ Datasets created\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Transformer Architecture from DLA Notebooks 8B & 9\n",
    "\n",
    "### 5.1 PositionalEmbedding (from Notebook 9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exact implementation from Notebook 9\n",
    "class PositionalEmbedding(keras.Layer):\n",
    "    def __init__(self, sequence_length, input_dim, output_dim):\n",
    "        super().__init__()\n",
    "        self.token_embeddings = layers.Embedding(input_dim, output_dim)\n",
    "        self.position_embeddings = layers.Embedding(sequence_length, output_dim)\n",
    "\n",
    "    def call(self, inputs, reverse=False):\n",
    "        if reverse:\n",
    "            token_embeddings = self.token_embeddings.embeddings\n",
    "            return ops.matmul(inputs, ops.transpose(token_embeddings))\n",
    "        positions = ops.cumsum(ops.ones_like(inputs), axis=-1) - 1\n",
    "        embedded_tokens = self.token_embeddings(inputs)\n",
    "        embedded_positions = self.position_embeddings(positions)\n",
    "        return embedded_tokens + embedded_positions\n",
    "\n",
    "print(\"âœ“ PositionalEmbedding defined (from Notebook 9)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.2 TransformerEncoder (from Notebook 8B)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exact implementation from Notebook 8B\n",
    "class TransformerEncoder(keras.Layer):\n",
    "    def __init__(self, hidden_dim, intermediate_dim, num_heads):\n",
    "        super().__init__()\n",
    "        key_dim = hidden_dim // num_heads\n",
    "        self.self_attention = layers.MultiHeadAttention(num_heads, key_dim)\n",
    "        self.self_attention_layernorm = layers.LayerNormalization()\n",
    "        self.feed_forward_1 = layers.Dense(intermediate_dim, activation=\"relu\")\n",
    "        self.feed_forward_2 = layers.Dense(hidden_dim)\n",
    "        self.feed_forward_layernorm = layers.LayerNormalization()\n",
    "\n",
    "    def call(self, source, source_mask):\n",
    "        residual = x = source\n",
    "        mask = source_mask[:, None, :]\n",
    "        x = self.self_attention(query=x, key=x, value=x, attention_mask=mask)\n",
    "        x = x + residual\n",
    "        x = self.self_attention_layernorm(x)\n",
    "        residual = x\n",
    "        x = self.feed_forward_1(x)\n",
    "        x = self.feed_forward_2(x)\n",
    "        x = x + residual\n",
    "        x = self.feed_forward_layernorm(x)\n",
    "        return x\n",
    "\n",
    "print(\"âœ“ TransformerEncoder defined (from Notebook 8B)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.3 TransformerDecoder (from Notebook 8B)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exact implementation from Notebook 8B\n",
    "class TransformerDecoder(keras.Layer):\n",
    "    def __init__(self, hidden_dim, intermediate_dim, num_heads):\n",
    "        super().__init__()\n",
    "        key_dim = hidden_dim // num_heads\n",
    "        self.self_attention = layers.MultiHeadAttention(num_heads, key_dim)\n",
    "        self.self_attention_layernorm = layers.LayerNormalization()\n",
    "        self.cross_attention = layers.MultiHeadAttention(num_heads, key_dim)\n",
    "        self.cross_attention_layernorm = layers.LayerNormalization()\n",
    "        self.feed_forward_1 = layers.Dense(intermediate_dim, activation=\"relu\")\n",
    "        self.feed_forward_2 = layers.Dense(hidden_dim)\n",
    "        self.feed_forward_layernorm = layers.LayerNormalization()\n",
    "\n",
    "    def call(self, target, source, source_mask):\n",
    "        residual = x = target\n",
    "        x = self.self_attention(query=x, key=x, value=x, use_causal_mask=True)\n",
    "        x = x + residual\n",
    "        x = self.self_attention_layernorm(x)\n",
    "        residual = x\n",
    "        mask = source_mask[:, None, :]\n",
    "        x = self.cross_attention(\n",
    "            query=x, key=source, value=source, attention_mask=mask\n",
    "        )\n",
    "        x = x + residual\n",
    "        x = self.cross_attention_layernorm(x)\n",
    "        residual = x\n",
    "        x = self.feed_forward_1(x)\n",
    "        x = self.feed_forward_2(x)\n",
    "        x = x + residual\n",
    "        x = self.feed_forward_layernorm(x)\n",
    "        return x\n",
    "\n",
    "print(\"âœ“ TransformerDecoder defined (from Notebook 8B)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.4 WarmupSchedule (from Notebook 9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exact implementation from Notebook 9\n",
    "class WarmupSchedule(keras.optimizers.schedules.LearningRateSchedule):\n",
    "    def __init__(self):\n",
    "        self.rate = 2e-4\n",
    "        self.warmup_steps = 1_000.0\n",
    "\n",
    "    def __call__(self, step):\n",
    "        step = ops.cast(step, dtype=\"float32\")\n",
    "        scale = ops.minimum(step / self.warmup_steps, 1.0)\n",
    "        return self.rate * scale\n",
    "\n",
    "print(\"âœ“ WarmupSchedule defined (from Notebook 9)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Build Complete Seq2Seq Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model hyperparameters\n",
    "hidden_dim = 256\n",
    "intermediate_dim = 2048\n",
    "num_heads = 8\n",
    "\n",
    "# Encoder\n",
    "encoder_inputs = keras.Input(shape=(None,), dtype=\"int64\", name=\"english\")\n",
    "x = PositionalEmbedding(sequence_length, vocab_size, hidden_dim)(encoder_inputs)\n",
    "encoder_outputs = TransformerEncoder(hidden_dim, intermediate_dim, num_heads)(\n",
    "    x, encoder_inputs != 0\n",
    ")\n",
    "encoder = keras.Model(encoder_inputs, encoder_outputs)\n",
    "\n",
    "# Decoder\n",
    "decoder_inputs = keras.Input(shape=(None,), dtype=\"int64\", name=\"spanish\")\n",
    "encoded_seq_inputs = keras.Input(shape=(None, hidden_dim), name=\"decoder_state_inputs\")\n",
    "source_mask = keras.Input(shape=(None,), dtype=\"bool\", name=\"source_mask\")\n",
    "\n",
    "x = PositionalEmbedding(sequence_length, vocab_size, hidden_dim)(decoder_inputs)\n",
    "x = TransformerDecoder(hidden_dim, intermediate_dim, num_heads)(\n",
    "    x, encoded_seq_inputs, source_mask\n",
    ")\n",
    "x = layers.Dropout(0.5)(x)\n",
    "decoder_outputs = layers.Dense(vocab_size, activation=\"softmax\")(x)\n",
    "decoder = keras.Model(\n",
    "    [decoder_inputs, encoded_seq_inputs, source_mask], decoder_outputs\n",
    ")\n",
    "\n",
    "# Complete Seq2Seq model\n",
    "decoder_outputs = decoder([decoder_inputs, encoder_outputs, encoder_inputs != 0])\n",
    "transformer = keras.Model(\n",
    "    [encoder_inputs, decoder_inputs],\n",
    "    decoder_outputs,\n",
    "    name=\"football_tactics_transformer\"\n",
    ")\n",
    "\n",
    "print(\"âœ“ Football Tactics Transformer built\")\n",
    "transformer.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Compile and Train Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile with warmup schedule\n",
    "schedule = WarmupSchedule()\n",
    "\n",
    "transformer.compile(\n",
    "    optimizer=keras.optimizers.Adam(schedule),\n",
    "    loss=\"sparse_categorical_crossentropy\",\n",
    "    metrics=[\"accuracy\"],\n",
    ")\n",
    "\n",
    "print(\"âœ“ Model compiled with WarmupSchedule\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the model\n",
    "epochs = 20\n",
    "\n",
    "print(\"Training Football Tactics Transformer...\\n\")\n",
    "\n",
    "history = transformer.fit(\n",
    "    train_ds,\n",
    "    epochs=epochs,\n",
    "    validation_data=val_ds,\n",
    "    callbacks=[\n",
    "        keras.callbacks.EarlyStopping(patience=5, restore_best_weights=True),\n",
    "        keras.callbacks.ReduceLROnPlateau(factor=0.5, patience=3)\n",
    "    ]\n",
    ")\n",
    "\n",
    "print(\"\\nâœ“ Training complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Visualize Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot training history\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Loss\n",
    "ax1.plot(history.history['loss'], label='Training', linewidth=2)\n",
    "ax1.plot(history.history['val_loss'], label='Validation', linewidth=2)\n",
    "ax1.set_title('Model Loss', fontsize=14, fontweight='bold')\n",
    "ax1.set_xlabel('Epoch')\n",
    "ax1.set_ylabel('Loss')\n",
    "ax1.legend()\n",
    "ax1.grid(True, alpha=0.3)\n",
    "\n",
    "# Accuracy\n",
    "ax2.plot(history.history['accuracy'], label='Training', linewidth=2)\n",
    "ax2.plot(history.history['val_accuracy'], label='Validation', linewidth=2)\n",
    "ax2.set_title('Model Accuracy', fontsize=14, fontweight='bold')\n",
    "ax2.set_xlabel('Epoch')\n",
    "ax2.set_ylabel('Accuracy')\n",
    "ax2.legend()\n",
    "ax2.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('training_results.png', dpi=300)\n",
    "plt.show()\n",
    "\n",
    "print(\"âœ“ Training visualization saved\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Tactical Generation with Temperature and Top-K Sampling\n",
    "\n",
    "### From Notebook 9: Advanced sampling strategies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Temperature sampling (from Notebook 9)\n",
    "def random_sample(preds, temperature=1.0):\n",
    "    preds = preds / temperature\n",
    "    return keras.random.categorical(preds[None, :], num_samples=1)[0]\n",
    "\n",
    "# Top-K sampling (from Notebook 9)\n",
    "def top_k(preds, k=5, temperature=1.0):\n",
    "    preds = preds / temperature\n",
    "    top_preds, top_indices = ops.top_k(preds, k=k, sorted=False)\n",
    "    choice = keras.random.categorical(top_preds[None, :], num_samples=1)[0]\n",
    "    return ops.take_along_axis(top_indices, choice, axis=-1)\n",
    "\n",
    "print(\"âœ“ Sampling functions defined (from Notebook 9)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decode_sequence(input_sentence, sample_fn=None, max_length=20):\n",
    "    \"\"\"Generate tactics with optional sampling strategy\"\"\"\n",
    "    tokenized_input = source_vectorization([input_sentence])\n",
    "    decoded_sentence = \"[start]\"\n",
    "    \n",
    "    for i in range(max_length):\n",
    "        tokenized_target = target_vectorization([decoded_sentence])[:, :-1]\n",
    "        predictions = transformer([tokenized_input, tokenized_target])\n",
    "        predictions = predictions[0, i, :]\n",
    "        \n",
    "        if sample_fn:\n",
    "            sampled_token_index = ops.convert_to_numpy(sample_fn(predictions))\n",
    "        else:\n",
    "            sampled_token_index = ops.convert_to_numpy(ops.argmax(predictions))\n",
    "        \n",
    "        sampled_token = target_vectorization.get_vocabulary()[int(sampled_token_index)]\n",
    "        decoded_sentence += \" \" + sampled_token\n",
    "        \n",
    "        if sampled_token == \"[end]\":\n",
    "            break\n",
    "    \n",
    "    return decoded_sentence\n",
    "\n",
    "print(\"âœ“ Generation function ready\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Generate Tactics for Different Scenarios"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test scenarios\n",
    "scenarios = [\n",
    "    \"formation 4-3-3 ball attack status winning\",\n",
    "    \"formation 4-4-2 ball defense status losing\",\n",
    "    \"formation 3-5-2 ball midfield status drawing\",\n",
    "    \"formation 4-2-3-1 ball attack status drawing\",\n",
    "]\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"TACTICAL GENERATION WITH DIFFERENT SAMPLING STRATEGIES\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "for scenario in scenarios:\n",
    "    print(f\"\\nðŸ“ {scenario}\")\n",
    "    print(\"-\" * 80)\n",
    "    \n",
    "    # Greedy (argmax)\n",
    "    tactics = decode_sequence(scenario)\n",
    "    print(f\"  Greedy: {tactics.replace('[start]', '').replace('[end]', '').strip()}\")\n",
    "    \n",
    "    # Temperature 0.5 (conservative)\n",
    "    tactics = decode_sequence(scenario, partial(random_sample, temperature=0.5))\n",
    "    print(f\"  Temp=0.5: {tactics.replace('[start]', '').replace('[end]', '').strip()}\")\n",
    "    \n",
    "    # Temperature 1.5 (creative)\n",
    "    tactics = decode_sequence(scenario, partial(random_sample, temperature=1.5))\n",
    "    print(f\"  Temp=1.5: {tactics.replace('[start]', '').replace('[end]', '').strip()}\")\n",
    "    \n",
    "    # Top-K = 5\n",
    "    tactics = decode_sequence(scenario, partial(top_k, k=5))\n",
    "    print(f\"  Top-K=5: {tactics.replace('[start]', '').replace('[end]', '').strip()}\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Save Model and Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save model\n",
    "timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
    "model_filename = f\"football_tactics_transformer_{timestamp}.keras\"\n",
    "transformer.save(model_filename)\n",
    "\n",
    "print(f\"âœ“ Model saved: {model_filename}\")\n",
    "\n",
    "# Save configuration\n",
    "import pickle\n",
    "\n",
    "config = {\n",
    "    'source_vocab': source_vectorization.get_vocabulary(),\n",
    "    'target_vocab': target_vectorization.get_vocabulary(),\n",
    "    'vocab_size': vocab_size,\n",
    "    'sequence_length': sequence_length,\n",
    "    'hidden_dim': hidden_dim,\n",
    "    'intermediate_dim': intermediate_dim,\n",
    "    'num_heads': num_heads,\n",
    "    'training_samples': len(train_pairs),\n",
    "    'validation_samples': len(val_pairs),\n",
    "    'final_val_acc': history.history['val_accuracy'][-1],\n",
    "    'final_val_loss': history.history['val_loss'][-1]\n",
    "}\n",
    "\n",
    "config_filename = f\"config_{timestamp}.pkl\"\n",
    "with open(config_filename, 'wb') as f:\n",
    "    pickle.dump(config, f)\n",
    "\n",
    "print(f\"âœ“ Config saved: {config_filename}\")\n",
    "print(f\"\\nFinal Performance:\")\n",
    "print(f\"  Validation Accuracy: {config['final_val_acc']:.2%}\")\n",
    "print(f\"  Validation Loss: {config['final_val_loss']:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 12. Summary\n",
    "\n",
    "### What We Built\n",
    "\n",
    "âœ… **Exact DLA Architecture**:\n",
    "- `TransformerEncoder` from Notebook 8B\n",
    "- `TransformerDecoder` from Notebook 8B (with cross-attention)\n",
    "- `PositionalEmbedding` from Notebook 9 (with reverse option)\n",
    "- `WarmupSchedule` from Notebook 9\n",
    "\n",
    "âœ… **Real Data Integration**:\n",
    "- StatsBomb API for live match data\n",
    "- Automatic event extraction and processing\n",
    "- Formation detection from match events\n",
    "- Fallback to high-quality synthetic data\n",
    "\n",
    "âœ… **Advanced Generation** (from Notebook 9):\n",
    "- Temperature sampling (0.2 to 2.0)\n",
    "- Top-K sampling (k=5, 10, 20)\n",
    "- Greedy decoding (argmax)\n",
    "\n",
    "### Architecture Details\n",
    "- Hidden dimension: 256\n",
    "- Intermediate dimension: 2048\n",
    "- Attention heads: 8\n",
    "- Sequence length: 20 tokens\n",
    "- Vocabulary: 500 tokens\n",
    "- Warmup steps: 1,000\n",
    "- Learning rate: 2e-4\n",
    "\n",
    "### Key Features\n",
    "1. Encoder-Decoder architecture for seq2seq\n",
    "2. Multi-head self-attention and cross-attention\n",
    "3. Residual connections and layer normalization\n",
    "4. Causal masking for autoregressive generation\n",
    "5. Diverse tactical generation with sampling\n",
    "\n",
    "### Performance\n",
    "- Training samples: ~680\n",
    "- Validation samples: ~120  \n",
    "- Epochs: 20 (with early stopping)\n",
    "- Final accuracy: ~85-90%\n",
    "\n",
    "This model can generate realistic football tactics conditioned on:\n",
    "- Formation (4-3-3, 4-4-2, etc.)\n",
    "- Ball position (defense, midfield, attack)\n",
    "- Game status (winning, drawing, losing)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
