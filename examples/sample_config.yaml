# Sample Configuration for Enhanced Dialogue Processing
# Copy this file and customize for your use case

# Google Drive configuration
drive:
  # Base path to your Google Drive mount point
  base_path: "/content/drive/MyDrive/DLA_Notebooks_Data_PGPM"
  
  # Folder containing dialogue files
  data_folder: "Dataset"
  
  # Folder for cached data
  cache_folder: "cache"
  
  # Mount timeout in seconds
  mount_timeout: 300
  
  # Number of retry attempts for mounting
  retry_attempts: 3

# Processing configuration
processing:
  # Number of files to process in each batch
  batch_size: 100
  
  # Number of parallel workers (set to 1 for sequential processing)
  parallel_workers: 4
  
  # Maximum number of files to process (null = all files)
  max_files: null
  
  # Memory limit in MB (monitoring only, not enforced)
  memory_limit_mb: 2048
  
  # Show progress bars during processing
  show_progress: true

# Supported file formats
formats:
  # List of file extensions to process
  supported: ["txt", "json", "jsonl", "csv"]
  
  # Encoding options to try when reading files
  encodings: ["utf-8", "latin-1", "cp1252"]
  
  # CSV-specific configuration
  csv_config:
    # Column name for context/prompt
    context_column: "context"
    
    # Column name for response/completion
    response_column: "response"
    
    # Whether CSV has a header row
    has_header: true

# Quality filtering rules
quality:
  # Minimum character length for dialogues
  min_dialogue_length: 5
  
  # Maximum character length for dialogues
  max_dialogue_length: 150
  
  # Minimum number of times a character must appear in dataset
  min_char_occurrence: 3
  
  # Remove duplicate dialogues
  remove_duplicates: true
  
  # Minimum word count per dialogue
  min_word_count: 2
  
  # Maximum word count per dialogue
  max_word_count: 100

# Tokenizer configuration
tokenizer:
  # Maximum vocabulary size
  max_vocab_size: 5000
  
  # Minimum word frequency to include in vocabulary
  min_word_freq: 2
  
  # Special tokens
  oov_token: "<UNK>"
  pad_token: "<PAD>"
  start_token: "<START>"
  end_token: "<END>"

# Data augmentation settings
augmentation:
  # Enable/disable augmentation
  enabled: true
  
  # List of augmentation techniques to apply
  # Options: "synonym_replace", "context_shuffle"
  techniques: ["synonym_replace", "context_shuffle"]
  
  # Ratio of original data to augment (0.3 = 30% more data)
  augment_ratio: 0.3
  
  # Probability of replacing a word with synonym
  synonym_replace_prob: 0.3
  
  # Maximum number of words to replace per dialogue
  max_synonym_replacements: 3

# Caching configuration
cache:
  # Enable/disable caching
  enabled: true
  
  # Directory for cache files
  cache_dir: "./cache"
  
  # Create checkpoint every N files
  checkpoint_interval: 500
  
  # Validate cache before loading
  validate_cache: true
  
  # Cache expiration time in hours
  cache_expiry_hours: 168  # 1 week

# Monitoring and logging
monitoring:
  # Log level: DEBUG, INFO, WARNING, ERROR
  log_level: "INFO"
  
  # Log file name
  log_file: "processing.log"
  
  # Save statistics to file
  save_stats: true
  
  # Statistics output file
  stats_file: "processing_stats.json"
  
  # Error log file (CSV format)
  error_log: "failed_files.csv"
