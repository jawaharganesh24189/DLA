{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "view-in-github",
    "colab_type": "text"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/jawaharganesh24189/DLA/blob/main/Copy_of_8E_Adversarial_Dialogue_GAN.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KI5mwfun1ig3"
   },
   "source": [
    "# Adversarial Dialogue Generation System for Custom Dialogue Dataset\n",
    "## Using GAN Architecture (SeqGAN/LeakGAN)\n",
    "\n",
    "**Author**: Deep Learning Academy  \n",
    "**Date**: 2024\n",
    "\n",
    "This notebook implements a comprehensive adversarial dialogue generation system using Generative Adversarial Networks (GANs) specifically designed for text generation. We apply this to generate dialogue scripts from custom datasets with character-specific dialogue.\n",
    "\n",
    "### Table of Contents\n",
    "1. **Background on GANs for Text** - Theory and concepts\n",
    "2. **Data Preparation** - Load and process Dialogue scripts\n",
    "3. **Generator Architecture** - LSTM-based sequence generator\n",
    "4. **Discriminator Architecture** - CNN-based classifier\n",
    "5. **Policy Gradient** - REINFORCE algorithm\n",
    "6. **Adversarial Training** - Complete training loop\n",
    "7. **Autocomplete Tool** - Interactive dialogue generation\n",
    "8. **Evaluation Metrics** - Quality assessment\n",
    "9. **Advanced Features** - Conditional generation\n",
    "10. **Visualizations** - Training dynamics\n",
    "11. **Model Persistence** - Save/load models\n",
    "12. **Future Work** - API deployment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wUkqcdAy1ig5"
   },
   "source": [
    "## Section 1.0: Background on GANs for Text Generation\n",
    "\n",
    "### Overview of SeqGAN and LeakGAN\n",
    "\n",
    "**Traditional Language Models** use maximum likelihood estimation (MLE) to predict the next token. While effective, they suffer from:\n",
    "- **Exposure bias**: Training vs inference mismatch\n",
    "- **Loss-evaluation mismatch**: Word-level loss doesn't capture sequence quality\n",
    "- **Limited diversity**: Tends to generate safe, generic sequences\n",
    "\n",
    "**Generative Adversarial Networks (GANs)** for text introduce adversarial training to address these issues.\n",
    "\n",
    "#### Key Concepts:\n",
    "\n",
    "**1. SeqGAN (Sequence GAN)**:\n",
    "- Generator: LSTM/Transformer that generates sequences token-by-token\n",
    "- Discriminator: CNN/LSTM classifier that distinguishes real vs fake sequences\n",
    "- Uses REINFORCE algorithm (policy gradient) for training discrete sequences\n",
    "- Monte Carlo rollouts for intermediate reward estimation\n",
    "\n",
    "**2. LeakGAN (Long Text Generation GAN)**:\n",
    "- Improves SeqGAN by \"leaking\" discriminator features to generator\n",
    "- Manager-Worker hierarchical framework for long-term planning\n",
    "- Better handling of long sequences and semantic coherence\n",
    "\n",
    "#### Why Adversarial Training Improves Quality:\n",
    "\n",
    "1. **Better Reward Signal**: Discriminator provides holistic sequence-level feedback rather than token-level\n",
    "2. **Increased Diversity**: Generator learns to produce varied outputs to fool discriminator\n",
    "3. **Reduced Exposure Bias**: Generator is trained on its own outputs during adversarial phase\n",
    "4. **Quality-Aware Learning**: Discriminator learns what makes text \"realistic\" and \"high-quality\"\n",
    "\n",
    "#### Generator vs Discriminator Roles:\n",
    "\n",
    "**Generator (G)**:\n",
    "- **Goal**: Generate realistic dialogue sequences\n",
    "- **Input**: Start tokens + optional context (character, scene)\n",
    "- **Output**: Complete token sequence\n",
    "- **Training**: Policy gradient with discriminator reward\n",
    "- **Architecture**: LSTM/Transformer with embedding \u2192 recurrent layers \u2192 softmax output\n",
    "\n",
    "**Discriminator (D)**:\n",
    "- **Goal**: Distinguish real (human-written) from fake (generated) dialogue\n",
    "- **Input**: Complete text sequence\n",
    "- **Output**: Probability that sequence is real P(real) \u2208 [0, 1]\n",
    "- **Training**: Binary cross-entropy (real vs fake)\n",
    "- **Architecture**: Embedding \u2192 CNN/LSTM \u2192 dense layers \u2192 sigmoid output\n",
    "\n",
    "#### Mathematical Framework:\n",
    "\n",
    "The minimax game:\n",
    "\n",
    "$$V(D, G) = \\mathbb{E}_{x \\sim p_{data}}[\\log D(x)] + \\mathbb{E}_{z \\sim p_z}[\\log(1 - D(G(z)))]$$\n",
    "\n",
    "Where:\n",
    "- $D$ aims to maximize $V(D, G)$ (correctly classify real vs fake)\n",
    "- $G$ aims to minimize $V(D, G)$ (fool the discriminator)\n",
    "- At Nash equilibrium: $p_g = p_{data}$ (generator distribution matches real data)\n",
    "\n",
    "Policy Gradient for Generator:\n",
    "\n",
    "$$\\nabla_\\theta J(\\theta) = \\mathbb{E}_{y_{1:T} \\sim G_\\theta}[\\sum_{t=1}^{T}\\nabla_\\theta \\log G_\\theta(y_t|y_{1:t-1}) \\cdot Q_{D_\\phi}^{G_\\theta}(y_{1:t-1}, y_t)]$$\n",
    "\n",
    "Where $Q$ is the action-value function estimated via Monte Carlo search with discriminator reward."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BdnlvDo81ig5"
   },
   "source": [
    "## Configuration: Dataset Setup\n",
    "\n",
    "**Before running this notebook, configure your dataset:**\n",
    "\n",
    "### Dataset Format Requirements\n",
    "\n",
    "Your dataset should be a text file with dialogue formatted as:\n",
    "```\n",
    "[SCENE: Scene Description]\n",
    "Character1: Dialogue text here\n",
    "Character2: Response text here\n",
    "\n",
    "[SCENE: Another Scene]\n",
    "Character1: More dialogue\n",
    "```\n",
    "\n",
    "### Configuration Options\n",
    "\n",
    "1. **Dataset Path**: Point to your dialogue dataset file\n",
    "2. **Dataset Folder**: Organize multiple files in a folder\n",
    "3. **Format**: The parser expects `[SCENE: ...]` and `Character: dialogue` format\n",
    "\n",
    "**Examples:**\n",
    "- Movie scripts (formatted as above)\n",
    "- TV show transcripts\n",
    "- Play dialogues\n",
    "- Chat conversations\n",
    "- Customer service dialogues\n",
    "\n",
    "**Note**: If you have a different format, modify the `DialogueDataProcessor` class in Section 2.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "lp-IjbEE1ig6",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "1f789cc7-f59a-4334-c720-74e08cb92ec8"
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Mounted at /content/drive\n",
      "\u2713 Dataset folder found: /content/drive/MyDrive/DLA_Notebooks_Data_PGPM/Dataset\n",
      "  Contains 3836 .txt file(s)\n",
      "    - train-anime-3610.txt\n",
      "    - train-anime-3621.txt\n",
      "    - train-anime-3627.txt\n",
      "    - train-anime-3563.txt\n",
      "    - train-anime-3632.txt\n",
      "    ... and 3831 more\n",
      "\n",
      "Parsing options: {'min_char_occurrence': 2, 'context_as_scene': True}\n"
     ]
    }
   ],
   "source": [
    "# ============================================\n",
    "# DATASET CONFIGURATION\n",
    "# ============================================\n",
    "\n",
    "# Option 1: Single file path\n",
    "DATA_PATH = '/path/to/your/dataset/dialogues.txt'\n",
    "\n",
    "# Option 2: Dataset folder with multiple .txt files (NEW!)\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "DATA_PATH = '/content/drive/MyDrive/DLA_Notebooks_Data_PGPM/Dataset'\n",
    "\n",
    "# Option 3: Use messy dataset example (for testing flexible format)\n",
    "USE_MESSY_EXAMPLE = False  # Set to True to test flexible format\n",
    "\n",
    "# Option 4: Use sample data (original format for testing)\n",
    "USE_SAMPLE_DATA = False  # Set to False when using your own dataset\n",
    "\n",
    "\n",
    "\n",
    "# Verify path exists\n",
    "import os\n",
    "if os.path.exists(DATA_PATH):\n",
    "    if os.path.isfile(DATA_PATH):\n",
    "        print(f\"\u2713 Dataset file found: {DATA_PATH}\")\n",
    "        print(f\"  File size: {os.path.getsize(DATA_PATH) / 1024:.2f} KB\")\n",
    "    elif os.path.isdir(DATA_PATH):\n",
    "        import glob\n",
    "        txt_files = glob.glob(os.path.join(DATA_PATH, '*.txt'))\n",
    "        print(f\"\u2713 Dataset folder found: {DATA_PATH}\")\n",
    "        print(f\"  Contains {len(txt_files)} .txt file(s)\")\n",
    "        for f in txt_files[:5]:  # Show first 5 files\n",
    "            print(f\"    - {os.path.basename(f)}\")\n",
    "        if len(txt_files) > 5:\n",
    "            print(f\"    ... and {len(txt_files) - 5} more\")\n",
    "else:\n",
    "    print(f\"\u2717 Dataset path not found: {DATA_PATH}\")\n",
    "    print(\"  Please update DATA_PATH to point to your dataset file or folder\")\n",
    "\n",
    "# Advanced Options for Flexible Format Parsing\n",
    "FLEXIBLE_PARSING_OPTIONS = {\n",
    "    'min_char_occurrence': 2,    # Minimum times a character must appear to be recognized\n",
    "    'context_as_scene': True,    # Use context lines to enrich scene descriptions\n",
    "}\n",
    "\n",
    "print(f\"\\nParsing options: {FLEXIBLE_PARSING_OPTIONS}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "79-fmPU41ig7",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "494451ab-970d-4e63-8eef-8587cbb83ef8"
   },
   "source": [
    "# Install required packages\n",
    "!pip install tensorflow==2.13.0 numpy matplotlib seaborn scikit-learn nltk ipywidgets wordcloud -q\n",
    "print('Packages installed successfully!')"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\u001b[31mERROR: Could not find a version that satisfies the requirement tensorflow==2.13.0 (from versions: 2.16.0rc0, 2.16.1, 2.16.2, 2.17.0rc0, 2.17.0rc1, 2.17.0, 2.17.1, 2.18.0rc0, 2.18.0rc1, 2.18.0rc2, 2.18.0, 2.18.1, 2.19.0rc0, 2.19.0, 2.19.1, 2.20.0rc0, 2.20.0)\u001b[0m\u001b[31m\n",
      "\u001b[0m\u001b[31mERROR: No matching distribution found for tensorflow==2.13.0\u001b[0m\u001b[31m\n",
      "\u001b[0mPackages installed successfully!\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "l2Ll6h1y1ig7",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "71e25c3d-5fac-4a98-bff6-f30b420a5a1c"
   },
   "source": [
    "# Import libraries\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers, Model\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import re\n",
    "import json\n",
    "import pickle\n",
    "from collections import defaultdict, Counter\n",
    "import nltk\n",
    "from nltk.translate.bleu_score import sentence_bleu, SmoothingFunction\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.model_selection import train_test_split\n",
    "from wordcloud import WordCloud\n",
    "import warnings\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "nltk.download('punkt', quiet=True)\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "np.random.seed(42)\n",
    "tf.random.set_seed(42)\n",
    "\n",
    "# Configure GPU\n",
    "physical_devices = tf.config.list_physical_devices('GPU')\n",
    "if physical_devices:\n",
    "    tf.config.experimental.set_memory_growth(physical_devices[0], True)\n",
    "    print(f'GPU available: {physical_devices[0]}')\n",
    "else:\n",
    "    print('No GPU found, using CPU')\n",
    "\n",
    "# Set plotting style\n",
    "plt.style.use('seaborn-v0_8-darkgrid')\n",
    "sns.set_palette('husl')\n",
    "\n",
    "print(f'TensorFlow version: {tf.__version__}')\n",
    "print(f'Keras version: {keras.__version__}')"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "No GPU found, using CPU\n",
      "TensorFlow version: 2.19.0\n",
      "Keras version: 3.10.0\n"
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MuW8dqG71ig7"
   },
   "source": [
    "## Section 2.0: Data Preparation\n",
    "\n",
    "In this section, we'll explore two approaches for data preparation:\n",
    "\n",
    "**Approach 1: DialogueParser Utility** (Optional preprocessing)\n",
    "- Standalone utility for format conversion and analysis\n",
    "- Good for preprocessing multiple datasets\n",
    "- Supports JSONL, CSV, conversational formats\n",
    "\n",
    "**Approach 2: FlexibleDialogueDataProcessor** (Direct integration)\n",
    "- Integrated with the training pipeline\n",
    "- Handles multiple files and flexible formats\n",
    "- Direct tokenization and sequence creation\n",
    "\n",
    "You can use both: first preprocess with DialogueParser, then train with FlexibleDialogueDataProcessor.\n",
    "\n",
    "**Core steps:**\n",
    "1. **Load** Custom Dialogue Season 1 script from DialogueS1.txt\n",
    "2. **Parse** characters, dialogue, and scenes\n",
    "3. **Create** character-specific datasets\n",
    "4. **Tokenize** text and create sequences\n",
    "5. **Split** into train/validation sets (85/15)\n",
    "6. **Augment** data for better generalization\n",
    "\n",
    "### Data Format:\n",
    "The script follows this structure:\n",
    "```\n",
    "[SCENE: Location]\n",
    "Character: Dialogue text\n",
    "```\n",
    "\n",
    "### Preprocessing Pipeline:\n",
    "1. Extract scene types (Training, Battle, Strategy, etc.)\n",
    "2. Extract character names and their dialogues\n",
    "3. Create context-aware sequences (include previous dialogue for context)\n",
    "4. Generate sliding windows for sequence learning\n",
    "5. Build vocabulary and tokenize (max 5000 tokens)\n",
    "6. Pad sequences to uniform length (50-100 tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OOV5qHaW1ig7"
   },
   "source": [
    "### Section 2.1: DialogueParser Utility (Alternative Preprocessing)\n",
    "\n",
    "Before using the integrated `FlexibleDialogueDataProcessor`, you can optionally preprocess your data using the standalone `DialogueParser` utility. This is useful for:\n",
    "\n",
    "**Use Cases:**\n",
    "- Converting between different dialogue formats (JSONL, CSV, conversational)\n",
    "- Analyzing dataset statistics before training\n",
    "- Preprocessing multiple dataset files into a single format\n",
    "- Working with context-response pair formats\n",
    "\n",
    "**When to Use:**\n",
    "- **DialogueParser**: Preprocessing, format conversion, batch processing, statistics\n",
    "- **FlexibleDialogueDataProcessor**: Direct integration with GAN training pipeline\n",
    "\n",
    "You can use both together: first preprocess with DialogueParser, then load the result with FlexibleDialogueDataProcessor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "QflIefp21ig8",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "77105710-d8e8-4af1-fcdd-2e909dfc76dd"
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "DialogueParser utility available for optional preprocessing\n",
      "Uncomment examples above to use it before the main training pipeline\n",
      "\n",
      "For more details, see DIALOGUE_PARSER_GUIDE.md\n"
     ]
    }
   ],
   "source": [
    "# Import the DialogueParser utility\n",
    "from dialogue_parser import DialogueParser, DatasetStatistics\n",
    "\n",
    "# Option 1: Use DialogueParser for preprocessing (optional step)\n",
    "# Uncomment to preprocess your data before training\n",
    "\n",
    "# Initialize parser\n",
    "# dialogue_parser = DialogueParser()\n",
    "\n",
    "# Example 1: Parse a single file\n",
    "# turns = dialogue_parser.parse_file('sample_dialogues.txt')\n",
    "\n",
    "# Example 2: Parse entire directory\n",
    "# all_turns = dialogue_parser.parse_directory('messy_dataset/')\n",
    "\n",
    "# Example 3: Calculate statistics\n",
    "# stats = DatasetStatistics.calculate_stats(all_turns)\n",
    "# print('Dataset Statistics:')\n",
    "# for key, value in stats.items():\n",
    "#     print(f'  {key}: {value:.2f}' if isinstance(value, float) else f'  {key}: {value}')\n",
    "\n",
    "# Example 4: Convert to training format and save\n",
    "# jsonl_output = dialogue_parser.to_training_format(all_turns, 'jsonl')\n",
    "# with open('preprocessed_data.jsonl', 'w') as f:\n",
    "#     f.write(jsonl_output)\n",
    "\n",
    "# Example 5: Convert to CSV for analysis\n",
    "# csv_output = dialogue_parser.to_training_format(all_turns, 'csv')\n",
    "# with open('dialogues_analysis.csv', 'w') as f:\n",
    "#     f.write(csv_output)\n",
    "\n",
    "print('DialogueParser utility available for optional preprocessing')\n",
    "print('Uncomment examples above to use it before the main training pipeline')\n",
    "print('\\nFor more details, see DIALOGUE_PARSER_GUIDE.md')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MBp2f1gI1ig8"
   },
   "source": [
    "**Integration Workflow:**\n",
    "\n",
    "```python\n",
    "# Workflow 1: Direct training (default)\n",
    "DATA_PATH = 'sample_dialogues.txt'  # or folder path\n",
    "# \u2192 Use FlexibleDialogueDataProcessor below\n",
    "\n",
    "# Workflow 2: Preprocess then train\n",
    "# Step 1: Preprocess with DialogueParser\n",
    "dialogue_parser = DialogueParser()\n",
    "turns = dialogue_parser.parse_directory('raw_dataset/')\n",
    "output = dialogue_parser.to_training_format(turns, 'jsonl')\n",
    "with open('clean_data.jsonl', 'w') as f:\n",
    "    f.write(output)\n",
    "\n",
    "# Step 2: Set DATA_PATH to preprocessed file\n",
    "DATA_PATH = 'clean_data.jsonl'\n",
    "# \u2192 Continue with FlexibleDialogueDataProcessor\n",
    "```\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "PCcqUaCR1ig9"
   },
   "source": [
    "dialogue_parser = DialogueParser()\n",
    "turns = dialogue_parser.parse_directory('/content/drive/MyDrive/DLA_Notebooks_Data_PGPM/Dataset/')\n",
    "output = dialogue_parser.to_training_format(turns, 'jsonl')\n",
    "with open('clean_data.jsonl', 'w') as f:\n",
    "    f.write(output)\n",
    "\n",
    "# Step 2: Set DATA_PATH to preprocessed file\n",
    "DATA_PATH = 'clean_data.jsonl'\n",
    "\n",
    "\n",
    "import os\n",
    "import glob\n",
    "import re\n",
    "from collections import defaultdict\n",
    "\n",
    "class FlexibleDialogueDataProcessor:\n",
    "    \"\"\"\n",
    "    Enhanced data processor that handles:\n",
    "    1. Multiple .txt files in a folder\n",
    "    2. Flexible formats (context lines, character: dialogue, etc.)\n",
    "    3. Smart detection of character names vs context\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, file_path_or_folder, seq_length=50, max_vocab_size=5000,\n",
    "                 min_char_occurrence=2, context_as_scene=True):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            file_path_or_folder: Path to a single file OR a folder containing .txt files\n",
    "            seq_length: Maximum sequence length\n",
    "            max_vocab_size: Maximum vocabulary size\n",
    "            min_char_occurrence: Minimum times a character name must appear to be recognized\n",
    "            context_as_scene: Whether to treat context lines as scene descriptions\n",
    "        \"\"\"\n",
    "        self.path = file_path_or_folder\n",
    "        self.seq_length = seq_length\n",
    "        self.max_vocab_size = max_vocab_size\n",
    "        self.min_char_occurrence = min_char_occurrence\n",
    "        self.context_as_scene = context_as_scene\n",
    "\n",
    "        self.dialogues = []\n",
    "        self.characters = defaultdict(list)\n",
    "        self.scenes = defaultdict(list)\n",
    "        self.tokenizer = None\n",
    "        self.vocab_size = 0\n",
    "\n",
    "    def _get_file_list(self):\n",
    "        \"\"\"Get list of files to process\"\"\"\n",
    "        if os.path.isfile(self.path):\n",
    "            return [self.path]\n",
    "        elif os.path.isdir(self.path):\n",
    "            # Get all .txt files in the folder\n",
    "            txt_files = glob.glob(os.path.join(self.path, '*.txt'))\n",
    "            if not txt_files:\n",
    "                print(f\"Warning: No .txt files found in {self.path}\")\n",
    "            return sorted(txt_files)\n",
    "        else:\n",
    "            raise ValueError(f\"Path not found: {self.path}\")\n",
    "\n",
    "    def _is_likely_character_line(self, line):\n",
    "        \"\"\"\n",
    "        Detect if a line is likely \"Character: dialogue\" format\n",
    "        Returns: (is_character_line, character_name, dialogue_text)\n",
    "        \"\"\"\n",
    "        # Must have a colon\n",
    "        if ':' not in line:\n",
    "            return False, None, None\n",
    "\n",
    "        # Split on first colon\n",
    "        parts = line.split(':', 1)\n",
    "        if len(parts) != 2:\n",
    "            return False, None, None\n",
    "\n",
    "        potential_char = parts[0].strip()\n",
    "        dialogue = parts[1].strip()\n",
    "\n",
    "        # Character name should be short (1-3 words) and capitalized\n",
    "        words = potential_char.split()\n",
    "        if len(words) > 3:\n",
    "            return False, None, None\n",
    "\n",
    "        # Check if it looks like a character name (starts with capital, no weird punctuation)\n",
    "        if not potential_char or not potential_char[0].isupper():\n",
    "            return False, None, None\n",
    "\n",
    "        # Check for common non-character patterns\n",
    "        lowercase_potential = potential_char.lower()\n",
    "        if any(keyword in lowercase_potential for keyword in ['note', 'context', 'scene', 'time', 'location']):\n",
    "            return False, None, None\n",
    "\n",
    "        # Dialogue should exist and not be super long (context lines tend to be longer)\n",
    "        if not dialogue or len(dialogue) < 5:\n",
    "            return False, None, None\n",
    "\n",
    "        return True, potential_char, dialogue\n",
    "\n",
    "    def _is_scene_marker(self, line):\n",
    "        \"\"\"Check if line is a scene marker like [SCENE: Description]\"\"\"\n",
    "        return line.startswith('[SCENE:') and line.endswith(']')\n",
    "\n",
    "    def _extract_scene(self, line):\n",
    "        \"\"\"Extract scene description from marker\"\"\"\n",
    "        if self._is_scene_marker(line):\n",
    "            return line[7:-1].strip()\n",
    "        return None\n",
    "\n",
    "    def load_and_parse(self, verbose=True):\n",
    "        \"\"\"Load and parse files with flexible format support\"\"\"\n",
    "        files = self._get_file_list()\n",
    "\n",
    "        if verbose:\n",
    "            print(f\"Processing {len(files)} file(s)...\")\n",
    "\n",
    "        # First pass: collect all potential character names\n",
    "        potential_characters = defaultdict(int)\n",
    "        all_lines = []\n",
    "\n",
    "        for file_path in files:\n",
    "            with open(file_path, 'r', encoding='utf-8', errors='ignore') as f:\n",
    "                content = f.read()\n",
    "\n",
    "            lines = content.split('\\n')\n",
    "            for line in lines:\n",
    "                line = line.strip()\n",
    "                if not line:\n",
    "                    continue\n",
    "\n",
    "                all_lines.append((file_path, line))\n",
    "\n",
    "                is_char, char_name, _ = self._is_likely_character_line(line)\n",
    "                if is_char:\n",
    "                    potential_characters[char_name] += 1\n",
    "\n",
    "        # Filter characters by minimum occurrence\n",
    "        valid_characters = {\n",
    "            char for char, count in potential_characters.items()\n",
    "            if count >= self.min_char_occurrence\n",
    "        }\n",
    "\n",
    "        if verbose:\n",
    "            print(f\"Detected {len(valid_characters)} characters: {sorted(valid_characters)}\")\n",
    "\n",
    "        # Second pass: parse dialogues with validated characters\n",
    "        current_scene = 'Unknown'\n",
    "        context_buffer = []\n",
    "\n",
    "        for file_path, line in all_lines:\n",
    "            # Check for scene marker\n",
    "            scene = self._extract_scene(line)\n",
    "            if scene:\n",
    "                current_scene = scene\n",
    "                context_buffer = []\n",
    "                continue\n",
    "\n",
    "            # Check for character dialogue\n",
    "            is_char, char_name, dialogue = self._is_likely_character_line(line)\n",
    "\n",
    "            if is_char and char_name in valid_characters:\n",
    "                # This is valid character dialogue\n",
    "                # Use context buffer as additional scene info if enabled\n",
    "                if self.context_as_scene and context_buffer:\n",
    "                    scene_with_context = f\"{current_scene} ({' '.join(context_buffer[:2])})\"\n",
    "                else:\n",
    "                    scene_with_context = current_scene\n",
    "\n",
    "                entry = {\n",
    "                    'character': char_name,\n",
    "                    'dialogue': dialogue,\n",
    "                    'scene': scene_with_context,\n",
    "                    'source_file': os.path.basename(file_path)\n",
    "                }\n",
    "\n",
    "                self.dialogues.append(entry)\n",
    "                self.characters[char_name].append(dialogue)\n",
    "                self.scenes[scene_with_context].append(dialogue)\n",
    "\n",
    "                # Clear context buffer after use\n",
    "                context_buffer = []\n",
    "            else:\n",
    "                # This is a context line - store it\n",
    "                if len(context_buffer) < 5:  # Limit context buffer size\n",
    "                    context_buffer.append(line[:50])  # Truncate long lines\n",
    "\n",
    "        if verbose:\n",
    "            print(f'\\nLoaded {len(self.dialogues)} dialogue entries')\n",
    "            print(f'Characters ({len(self.characters)}): {list(self.characters.keys())}')\n",
    "            print(f'Unique scenes: {len(self.scenes)}')\n",
    "\n",
    "            # Character statistics\n",
    "            print('\\nDialogue distribution by character:')\n",
    "            for char, dialogues in sorted(self.characters.items(),\n",
    "                                         key=lambda x: len(x[1]), reverse=True)[:10]:\n",
    "                print(f'  {char}: {len(dialogues)} lines')\n",
    "\n",
    "        return self.dialogues\n",
    "\n",
    "    def create_tokenizer(self):\n",
    "        \"\"\"Create and fit tokenizer on dialogue data\"\"\"\n",
    "        all_dialogues = [d['dialogue'] for d in self.dialogues]\n",
    "\n",
    "        self.tokenizer = Tokenizer(\n",
    "            num_words=self.max_vocab_size,\n",
    "            filters='!\"#$%&()*+,-./:;<=>?@[\\\\]^_`{|}~\\t\\n',\n",
    "            lower=True,\n",
    "            oov_token='<UNK>'\n",
    "        )\n",
    "\n",
    "        self.tokenizer.fit_on_texts(all_dialogues)\n",
    "        self.vocab_size = min(len(self.tokenizer.word_index) + 1, self.max_vocab_size)\n",
    "\n",
    "        print(f'\\nVocabulary size: {self.vocab_size}')\n",
    "        print(f'Most common words: {list(self.tokenizer.word_index.items())[:20]}')\n",
    "\n",
    "        return self.tokenizer\n",
    "\n",
    "    def create_sequences(self, use_context=True):\n",
    "        \"\"\"Create training sequences with sliding window\"\"\"\n",
    "        sequences = []\n",
    "        labels = []  # For discriminator (1 = real)\n",
    "        metadata = []  # Character and scene info\n",
    "\n",
    "        for i, entry in enumerate(self.dialogues):\n",
    "            # Get context (previous dialogue if available)\n",
    "            context = ''\n",
    "            if use_context and i > 0:\n",
    "                context = self.dialogues[i-1]['dialogue'] + ' '\n",
    "\n",
    "            full_text = context + entry['dialogue']\n",
    "            token_list = self.tokenizer.texts_to_sequences([full_text])[0]\n",
    "\n",
    "            # Create sliding windows\n",
    "            for j in range(1, len(token_list)):\n",
    "                n_gram_sequence = token_list[:j+1]\n",
    "                if len(n_gram_sequence) <= self.seq_length:\n",
    "                    sequences.append(n_gram_sequence)\n",
    "                    labels.append(1)  # Real data\n",
    "                    metadata.append({\n",
    "                        'character': entry['character'],\n",
    "                        'scene': entry['scene']\n",
    "                    })\n",
    "\n",
    "        # Pad sequences\n",
    "        padded_sequences = pad_sequences(\n",
    "            sequences,\n",
    "            maxlen=self.seq_length,\n",
    "            padding='pre'\n",
    "        )\n",
    "\n",
    "        print(f'\\nCreated {len(padded_sequences)} sequences')\n",
    "        print(f'Sequence shape: {padded_sequences.shape}')\n",
    "\n",
    "        return padded_sequences, np.array(labels), metadata\n",
    "\n",
    "    def augment_data(self, sequences, metadata, augment_factor=2):\n",
    "        \"\"\"Apply data augmentation techniques\"\"\"\n",
    "        print(f'\\nApplying data augmentation (factor={augment_factor})...')\n",
    "        augmented_seqs = [sequences]\n",
    "        augmented_meta = [metadata]\n",
    "\n",
    "        for aug_idx in range(augment_factor - 1):\n",
    "            # Random token dropout (5%)\n",
    "            aug_seq = sequences.copy()\n",
    "            mask = np.random.random(aug_seq.shape) > 0.05\n",
    "            aug_seq = aug_seq * mask.astype(int)\n",
    "\n",
    "            augmented_seqs.append(aug_seq)\n",
    "            augmented_meta.append(metadata.copy())\n",
    "\n",
    "        combined_seqs = np.vstack(augmented_seqs)\n",
    "        combined_meta = []\n",
    "        for meta_list in augmented_meta:\n",
    "            combined_meta.extend(meta_list)\n",
    "\n",
    "        print(f'Augmented from {sequences.shape[0]} to {combined_seqs.shape[0]} sequences')\n",
    "        return combined_seqs, combined_meta\n",
    "\n",
    "    def get_character_encoding(self):\n",
    "        \"\"\"Create character to index mapping\"\"\"\n",
    "        unique_chars = sorted(list(self.characters.keys()))\n",
    "        char_to_idx = {char: idx for idx, char in enumerate(unique_chars)}\n",
    "        idx_to_char = {idx: char for char, idx in char_to_idx.items()}\n",
    "        return char_to_idx, idx_to_char, len(unique_chars)\n",
    "\n",
    "    def get_scene_encoding(self):\n",
    "        \"\"\"Create scene to index mapping\"\"\"\n",
    "        unique_scenes = sorted(list(self.scenes.keys()))\n",
    "        scene_to_idx = {scene: idx for idx, scene in enumerate(unique_scenes)}\n",
    "        idx_to_scene = {idx: scene for scene, idx in scene_to_idx.items()}\n",
    "        return scene_to_idx, idx_to_scene, len(unique_scenes)\n",
    "\n",
    "# Keep old class for backward compatibility\n",
    "DialogueDataProcessor = FlexibleDialogueDataProcessor\n",
    "\n",
    "# Initialize data processor\n",
    "print('='*80)\n",
    "print('INITIALIZING FLEXIBLE DATA PROCESSOR')\n",
    "print('='*80)\n",
    "print(f'Dataset path: {DATA_PATH}')\n",
    "print(f'Can handle: single file OR folder with multiple .txt files')\n",
    "print(f'Format support: flexible (context lines + character: dialogue)')\n",
    "print('='*80)\n",
    "\n",
    "data_processor = FlexibleDialogueDataProcessor(\n",
    "    file_path_or_folder=DATA_PATH,  # Can be file or folder\n",
    "    seq_length=50,\n",
    "    max_vocab_size=5000,\n",
    "    min_char_occurrence=2,  # Character must appear at least 2 times\n",
    "    context_as_scene=True   # Use context lines to enrich scene info\n",
    ")\n",
    "\n",
    "# Load and parse data\n",
    "dialogues = data_processor.load_and_parse()\n"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "j97KEliH1ig-"
   },
   "source": [
    "# Create tokenizer\n",
    "tokenizer = data_processor.create_tokenizer()\n",
    "\n",
    "# Create sequences\n",
    "sequences, labels, metadata = data_processor.create_sequences(use_context=True)\n",
    "\n",
    "# Get character and scene encodings\n",
    "char_to_idx, idx_to_char, num_characters = data_processor.get_character_encoding()\n",
    "scene_to_idx, idx_to_scene, num_scenes = data_processor.get_scene_encoding()\n",
    "\n",
    "print(f'\\nCharacter mapping ({num_characters} characters):')\n",
    "for char, idx in sorted(char_to_idx.items(), key=lambda x: x[1]):\n",
    "    print(f'  {idx}: {char}')\n",
    "\n",
    "print(f'\\nScene mapping ({num_scenes} scenes):')\n",
    "for scene, idx in sorted(scene_to_idx.items(), key=lambda x: x[1]):\n",
    "    print(f'  {idx}: {scene}')"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hAWP4MqH1ig_"
   },
   "source": [
    "# Create character and scene condition arrays\n",
    "char_conditions = np.array([char_to_idx[m['character']] for m in metadata])\n",
    "scene_conditions = np.array([scene_to_idx[m['scene']] for m in metadata])\n",
    "\n",
    "# Apply data augmentation\n",
    "sequences_aug, metadata_aug = data_processor.augment_data(\n",
    "    sequences, metadata, augment_factor=2\n",
    ")\n",
    "\n",
    "# Update condition arrays for augmented data\n",
    "char_conditions_aug = np.array([char_to_idx[m['character']] for m in metadata_aug])\n",
    "scene_conditions_aug = np.array([scene_to_idx[m['scene']] for m in metadata_aug])\n",
    "\n",
    "# Train/validation split (85/15)\n",
    "train_size = int(0.85 * len(sequences_aug))\n",
    "indices = np.arange(len(sequences_aug))\n",
    "np.random.shuffle(indices)\n",
    "\n",
    "train_indices = indices[:train_size]\n",
    "val_indices = indices[train_size:]\n",
    "\n",
    "X_train = sequences_aug[train_indices]\n",
    "X_val = sequences_aug[val_indices]\n",
    "\n",
    "char_train = char_conditions_aug[train_indices]\n",
    "char_val = char_conditions_aug[val_indices]\n",
    "\n",
    "scene_train = scene_conditions_aug[train_indices]\n",
    "scene_val = scene_conditions_aug[val_indices]\n",
    "\n",
    "print(f'\\n{'='*80}')\n",
    "print('DATASET SUMMARY')\n",
    "print('='*80)\n",
    "print(f'Training set: {X_train.shape[0]} sequences')\n",
    "print(f'Validation set: {X_val.shape[0]} sequences')\n",
    "print(f'Sequence length: {X_train.shape[1]}')\n",
    "print(f'Vocabulary size: {data_processor.vocab_size}')\n",
    "print(f'\\nSample sequence (tokens): {X_train[0]}')\n",
    "decoded = ' '.join([tokenizer.index_word.get(idx, '<PAD>') for idx in X_train[0] if idx > 0])\n",
    "print(f'Decoded: {decoded}')\n",
    "print('='*80)"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PtdE7PQ81ig_"
   },
   "source": [
    "## Section 3.0: Generator Architecture\n",
    "\n",
    "The Generator is responsible for creating realistic dialogue sequences. Our architecture combines:\n",
    "\n",
    "### Architecture Components:\n",
    "\n",
    "**1. Embedding Layer** (256 dimensions)\n",
    "- Converts token indices to dense vector representations\n",
    "- Learnable embeddings capture semantic relationships\n",
    "- Shared vocabulary space with discriminator\n",
    "\n",
    "**2. LSTM Layers** (2-3 layers, 512 units each)\n",
    "- Captures long-range sequential dependencies\n",
    "- Stacked for hierarchical feature learning\n",
    "- Dropout (0.3) for regularization and preventing overfitting\n",
    "- Return sequences for multi-layer stacking\n",
    "\n",
    "**3. Conditional Inputs** (Optional)\n",
    "- **Character embedding**: Enables character-specific dialogue generation\n",
    "- **Scene embedding**: Generates scene-appropriate dialogue\n",
    "- Concatenated with token embeddings at each timestep\n",
    "- Provides additional context for generation\n",
    "\n",
    "**4. Output Layer**\n",
    "- Dense layer with vocabulary size output\n",
    "- Softmax activation produces probability distribution\n",
    "- Temperature sampling controls randomness/diversity\n",
    "\n",
    "### Generation Strategies:\n",
    "\n",
    "**Temperature Sampling**:\n",
    "- Temperature T controls randomness\n",
    "- T < 1: More conservative, focused on high-probability tokens\n",
    "- T = 1: Standard sampling from distribution\n",
    "- T > 1: More diverse, exploratory generation\n",
    "\n",
    "**Decoding Methods**:\n",
    "- **Greedy**: Always select highest probability token\n",
    "- **Sampling**: Sample from probability distribution\n",
    "- **Beam Search**: Keep top-k sequences (future enhancement)\n",
    "\n",
    "### Training Phases:\n",
    "- **Pre-training**: Teacher forcing with MLE\n",
    "- **Adversarial**: Self-sampling with policy gradient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fPXrMRdB1ig_"
   },
   "source": [
    "class Generator(Model):\n",
    "    \"\"\"LSTM-based sequence generator with conditional inputs\"\"\"\n",
    "\n",
    "    def __init__(self, vocab_size, embedding_dim=256, lstm_units=512,\n",
    "                 num_lstm_layers=2, dropout=0.3, num_characters=0, num_scenes=0):\n",
    "        super(Generator, self).__init__()\n",
    "\n",
    "        self.vocab_size = vocab_size\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.lstm_units = lstm_units\n",
    "        self.num_lstm_layers = num_lstm_layers\n",
    "\n",
    "        # Token embedding\n",
    "        self.token_embedding = layers.Embedding(\n",
    "            vocab_size, embedding_dim, mask_zero=True, name='token_embedding'\n",
    "        )\n",
    "\n",
    "        # Conditional embeddings\n",
    "        self.use_char_condition = num_characters > 0\n",
    "        self.use_scene_condition = num_scenes > 0\n",
    "\n",
    "        if self.use_char_condition:\n",
    "            self.char_embedding = layers.Embedding(\n",
    "                num_characters, 32, name='char_embedding'\n",
    "            )\n",
    "\n",
    "        if self.use_scene_condition:\n",
    "            self.scene_embedding = layers.Embedding(\n",
    "                num_scenes, 32, name='scene_embedding'\n",
    "            )\n",
    "\n",
    "        # Calculate LSTM input dimension\n",
    "        lstm_input_dim = embedding_dim\n",
    "        if self.use_char_condition:\n",
    "            lstm_input_dim += 32\n",
    "        if self.use_scene_condition:\n",
    "            lstm_input_dim += 32\n",
    "\n",
    "        # LSTM layers\n",
    "        self.lstm_layers = []\n",
    "        for i in range(num_lstm_layers):\n",
    "            self.lstm_layers.append(\n",
    "                layers.LSTM(\n",
    "                    lstm_units,\n",
    "                    return_sequences=True,\n",
    "                    dropout=dropout,\n",
    "                    recurrent_dropout=0.0,  # Can cause issues with CuDNN\n",
    "                    name=f'lstm_{i}'\n",
    "                )\n",
    "            )\n",
    "\n",
    "        # Output layer\n",
    "        self.output_layer = layers.Dense(vocab_size, name='output')\n",
    "\n",
    "    def call(self, inputs, training=False, char_condition=None, scene_condition=None):\n",
    "        # Token embedding\n",
    "        x = self.token_embedding(inputs)\n",
    "\n",
    "        # Add conditional embeddings if provided\n",
    "        if char_condition is not None and self.use_char_condition:\n",
    "            char_emb = self.char_embedding(char_condition)\n",
    "            # Expand and tile to match sequence length\n",
    "            char_emb = tf.expand_dims(char_emb, 1)\n",
    "            char_emb = tf.tile(char_emb, [1, tf.shape(x)[1], 1])\n",
    "            x = tf.concat([x, char_emb], axis=-1)\n",
    "\n",
    "        if scene_condition is not None and self.use_scene_condition:\n",
    "            scene_emb = self.scene_embedding(scene_condition)\n",
    "            scene_emb = tf.expand_dims(scene_emb, 1)\n",
    "            scene_emb = tf.tile(scene_emb, [1, tf.shape(x)[1], 1])\n",
    "            x = tf.concat([x, scene_emb], axis=-1)\n",
    "\n",
    "        # LSTM layers\n",
    "        for lstm in self.lstm_layers:\n",
    "            x = lstm(x, training=training)\n",
    "\n",
    "        # Output logits\n",
    "        logits = self.output_layer(x)\n",
    "        return logits\n",
    "\n",
    "    def generate_sequence(self, start_tokens, max_length=50, temperature=1.0,\n",
    "                         char_condition=None, scene_condition=None, method='sampling'):\n",
    "        \"\"\"\n",
    "        Generate sequence using different decoding strategies\n",
    "\n",
    "        Args:\n",
    "            start_tokens: Initial tokens [batch_size, start_len]\n",
    "            max_length: Maximum sequence length\n",
    "            temperature: Sampling temperature (higher = more random)\n",
    "            char_condition: Character condition index\n",
    "            scene_condition: Scene condition index\n",
    "            method: 'greedy' or 'sampling'\n",
    "\n",
    "        Returns:\n",
    "            Generated sequences [batch_size, max_length]\n",
    "        \"\"\"\n",
    "        current_seq = start_tokens\n",
    "\n",
    "        for step in range(max_length - tf.shape(start_tokens)[1]):\n",
    "            # Get predictions\n",
    "            logits = self(current_seq, training=False,\n",
    "                         char_condition=char_condition,\n",
    "                         scene_condition=scene_condition)\n",
    "\n",
    "            # Get last token predictions\n",
    "            next_token_logits = logits[:, -1, :] / temperature\n",
    "\n",
    "            if method == 'greedy':\n",
    "                next_token = tf.argmax(next_token_logits, axis=-1, output_type=tf.int32)\n",
    "            elif method == 'sampling':\n",
    "                next_token = tf.random.categorical(next_token_logits, 1)\n",
    "                next_token = tf.squeeze(next_token, axis=-1)\n",
    "            else:\n",
    "                next_token = tf.argmax(next_token_logits, axis=-1, output_type=tf.int32)\n",
    "\n",
    "            # Append to sequence\n",
    "            next_token = tf.expand_dims(next_token, -1)\n",
    "            current_seq = tf.concat([current_seq, next_token], axis=1)\n",
    "\n",
    "            # Early stopping if all sequences generated EOS (if implemented)\n",
    "            # if tf.reduce_all(next_token == eos_token_id):\n",
    "            #     break\n",
    "\n",
    "        return current_seq\n",
    "\n",
    "# Create generator\n",
    "print('='*80)\n",
    "print('BUILDING GENERATOR')\n",
    "print('='*80)\n",
    "\n",
    "generator = Generator(\n",
    "    vocab_size=data_processor.vocab_size,\n",
    "    embedding_dim=256,\n",
    "    lstm_units=512,\n",
    "    num_lstm_layers=2,\n",
    "    dropout=0.3,\n",
    "    num_characters=num_characters,\n",
    "    num_scenes=num_scenes\n",
    ")\n",
    "\n",
    "# Build model\n",
    "sample_input = tf.random.uniform((2, 10), maxval=data_processor.vocab_size, dtype=tf.int32)\n",
    "sample_char = tf.constant([0, 1])\n",
    "sample_scene = tf.constant([0, 1])\n",
    "_ = generator(sample_input, char_condition=sample_char, scene_condition=sample_scene)\n",
    "\n",
    "print('\\nGenerator Architecture:')\n",
    "generator.summary()\n",
    "print(f'\\nTotal parameters: {generator.count_params():,}')\n",
    "print('='*80)"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "TeQ9hqhl1ihA"
   },
   "source": [
    "# Test generator generation\n",
    "print('Testing generator sequence generation...')\n",
    "test_start = tf.constant([[1, 2, 3]])  # Sample start tokens\n",
    "test_char = tf.constant([0])  # First character\n",
    "test_scene = tf.constant([0])  # First scene\n",
    "\n",
    "generated = generator.generate_sequence(\n",
    "    test_start,\n",
    "    max_length=20,\n",
    "    temperature=1.0,\n",
    "    char_condition=test_char,\n",
    "    scene_condition=test_scene,\n",
    "    method='sampling'\n",
    ")\n",
    "\n",
    "print(f'Generated sequence shape: {generated.shape}')\n",
    "print(f'Generated tokens: {generated[0].numpy()}')\n",
    "print('\\nGenerator test successful!')"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JmMRy7vx1ihA"
   },
   "source": [
    "## Section 3.1: Discriminator Architecture\n",
    "\n",
    "The Discriminator classifies sequences as real (human-written) or fake (generated).\n",
    "\n",
    "### CNN-Based Architecture:\n",
    "\n",
    "We implement a CNN-based discriminator for efficient local pattern recognition:\n",
    "\n",
    "**1. Embedding Layer**\n",
    "- Separate embedding from generator (not shared)\n",
    "- 256-dimensional vectors\n",
    "- Learns to recognize real vs fake patterns\n",
    "\n",
    "**2. Multiple 1D Convolutional Layers**\n",
    "- Different kernel sizes (3, 4, 5) capture n-gram features\n",
    "- Kernels learn local patterns (2-grams, 3-grams, etc.)\n",
    "- Parallel convolutions for multi-scale feature extraction\n",
    "- ReLU activation for non-linearity\n",
    "\n",
    "**3. Global Max Pooling**\n",
    "- Extracts most significant feature from each filter\n",
    "- Position-invariant feature detection\n",
    "- Reduces dimensionality\n",
    "\n",
    "**4. Dense Classification Head**\n",
    "- Multiple dense layers with ReLU\n",
    "- Dropout (0.3) for regularization\n",
    "- Final sigmoid outputs P(real) \u2208 [0, 1]\n",
    "\n",
    "### Training Techniques:\n",
    "\n",
    "**Label Smoothing**:\n",
    "- Real labels: 0.9 instead of 1.0\n",
    "- Fake labels: 0.1 instead of 0.0\n",
    "- Prevents discriminator overconfidence\n",
    "- Improves gradient flow to generator\n",
    "\n",
    "**Gradient Penalty** (Optional):\n",
    "- Encourages 1-Lipschitz continuity\n",
    "- Stabilizes training\n",
    "- Alternative to gradient clipping\n",
    "\n",
    "**One-sided Label Smoothing**:\n",
    "- Only smooth real labels, not fake\n",
    "- Prevents mode collapse\n",
    "- Maintains discriminator gradient quality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "WfHOCnnx1ihA"
   },
   "source": [
    "class Discriminator(Model):\n",
    "    \"\"\"CNN-based discriminator for sequence classification\"\"\"\n",
    "\n",
    "    def __init__(self, vocab_size, embedding_dim=256, filters=64,\n",
    "                 kernel_sizes=[3, 4, 5], dropout=0.3):\n",
    "        super(Discriminator, self).__init__()\n",
    "\n",
    "        self.vocab_size = vocab_size\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.filters = filters\n",
    "        self.kernel_sizes = kernel_sizes\n",
    "\n",
    "        # Embedding layer (separate from generator)\n",
    "        self.embedding = layers.Embedding(\n",
    "            vocab_size, embedding_dim, mask_zero=False, name='d_embedding'\n",
    "        )\n",
    "\n",
    "        # Multiple CNN layers with different kernel sizes\n",
    "        self.conv_layers = []\n",
    "        self.pool_layers = []\n",
    "\n",
    "        for kernel_size in kernel_sizes:\n",
    "            conv = layers.Conv1D(\n",
    "                filters=filters,\n",
    "                kernel_size=kernel_size,\n",
    "                activation='relu',\n",
    "                padding='same',\n",
    "                name=f'conv_{kernel_size}'\n",
    "            )\n",
    "            pool = layers.GlobalMaxPooling1D(name=f'pool_{kernel_size}')\n",
    "            self.conv_layers.append(conv)\n",
    "            self.pool_layers.append(pool)\n",
    "\n",
    "        # Dense layers\n",
    "        self.dense1 = layers.Dense(128, activation='relu', name='dense1')\n",
    "        self.dropout1 = layers.Dropout(dropout, name='dropout1')\n",
    "        self.dense2 = layers.Dense(64, activation='relu', name='dense2')\n",
    "        self.dropout2 = layers.Dropout(dropout, name='dropout2')\n",
    "\n",
    "        # Output layer (sigmoid for probability)\n",
    "        self.output_layer = layers.Dense(1, activation='sigmoid', name='output')\n",
    "\n",
    "    def call(self, inputs, training=False):\n",
    "        # Embedding\n",
    "        x = self.embedding(inputs)\n",
    "\n",
    "        # Apply multiple CNNs and pool, then concatenate\n",
    "        conv_outputs = []\n",
    "        for conv, pool in zip(self.conv_layers, self.pool_layers):\n",
    "            conv_out = conv(x)\n",
    "            pooled = pool(conv_out)\n",
    "            conv_outputs.append(pooled)\n",
    "\n",
    "        # Concatenate all CNN outputs\n",
    "        x = tf.concat(conv_outputs, axis=-1)\n",
    "\n",
    "        # Dense layers with dropout\n",
    "        x = self.dense1(x)\n",
    "        x = self.dropout1(x, training=training)\n",
    "        x = self.dense2(x)\n",
    "        x = self.dropout2(x, training=training)\n",
    "\n",
    "        # Output probability\n",
    "        output = self.output_layer(x)\n",
    "        return output\n",
    "\n",
    "    def get_features(self, inputs):\n",
    "        \"\"\"Extract feature representations for visualization (t-SNE)\"\"\"\n",
    "        x = self.embedding(inputs)\n",
    "        conv_outputs = []\n",
    "        for conv, pool in zip(self.conv_layers, self.pool_layers):\n",
    "            conv_out = conv(x)\n",
    "            pooled = pool(conv_out)\n",
    "            conv_outputs.append(pooled)\n",
    "        features = tf.concat(conv_outputs, axis=-1)\n",
    "        return features\n",
    "\n",
    "# Create discriminator\n",
    "print('='*80)\n",
    "print('BUILDING DISCRIMINATOR')\n",
    "print('='*80)\n",
    "\n",
    "discriminator = Discriminator(\n",
    "    vocab_size=data_processor.vocab_size,\n",
    "    embedding_dim=256,\n",
    "    filters=64,\n",
    "    kernel_sizes=[3, 4, 5],\n",
    "    dropout=0.3\n",
    ")\n",
    "\n",
    "# Build model\n",
    "sample_input = tf.random.uniform((2, 50), maxval=data_processor.vocab_size, dtype=tf.int32)\n",
    "_ = discriminator(sample_input)\n",
    "\n",
    "print('\\nDiscriminator Architecture:')\n",
    "discriminator.summary()\n",
    "print(f'\\nTotal parameters: {discriminator.count_params():,}')\n",
    "print('='*80)"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XI_asa2C1ihB"
   },
   "source": [
    "# Test discriminator\n",
    "print('Testing discriminator...')\n",
    "test_seq = tf.random.uniform((4, 50), maxval=data_processor.vocab_size, dtype=tf.int32)\n",
    "predictions = discriminator(test_seq, training=False)\n",
    "\n",
    "print(f'Input shape: {test_seq.shape}')\n",
    "print(f'Output shape: {predictions.shape}')\n",
    "print(f'Predictions (P(real)): {predictions.numpy().flatten()}')\n",
    "print('\\nDiscriminator test successful!')"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JfzMak2M1ihB"
   },
   "source": [
    "## Section 3.2: Policy Gradient Implementation\n",
    "\n",
    "Text generation is discrete, so we can't backpropagate through sampling. We use **REINFORCE** (policy gradient).\n",
    "\n",
    "### REINFORCE Algorithm:\n",
    "\n",
    "The policy gradient theorem:\n",
    "\n",
    "$$\\nabla_\\theta J(\\theta) = \\mathbb{E}_{y_{1:T} \\sim G_\\theta}\\left[\\sum_{t=1}^{T}\\nabla_\\theta \\log G_\\theta(y_t|y_{1:t-1}) \\cdot Q(y_{1:t})\\right]$$\n",
    "\n",
    "Where:\n",
    "- $\\theta$: Generator parameters\n",
    "- $G_\\theta$: Generator policy\n",
    "- $y_{1:T}$: Generated sequence\n",
    "- $Q(y_{1:t})$: Action-value (expected reward)\n",
    "\n",
    "### Reward Function:\n",
    "\n",
    "**Complete Sequences**:\n",
    "$$R(y_{1:T}) = D_\\phi(y_{1:T})$$\n",
    "\n",
    "Direct discriminator output.\n",
    "\n",
    "**Partial Sequences** (Monte Carlo Rollouts):\n",
    "$$Q(y_{1:t}) = \\frac{1}{N}\\sum_{i=1}^{N} D_\\phi(y_{1:t}, y_{t+1:T}^{(i)})$$\n",
    "\n",
    "Where $y_{t+1:T}^{(i)}$ are sampled completions.\n",
    "\n",
    "### Monte Carlo Rollouts:\n",
    "\n",
    "For incomplete sequences:\n",
    "1. Take partial sequence $y_{1:t}$\n",
    "2. Sample N completions using generator\n",
    "3. Evaluate each completion with discriminator\n",
    "4. Average discriminator scores\n",
    "5. Use as reward estimate\n",
    "\n",
    "### Baseline for Variance Reduction:\n",
    "\n",
    "Advantage function:\n",
    "$$A(y_{1:t}) = Q(y_{1:t}) - b$$\n",
    "\n",
    "Where $b$ is baseline (e.g., moving average of rewards).\n",
    "\n",
    "**Benefits**:\n",
    "- Reduces gradient variance\n",
    "- Faster convergence\n",
    "- More stable training\n",
    "\n",
    "### Implementation Details:\n",
    "\n",
    "1. **Sample sequences** from generator\n",
    "2. **Compute log probabilities** for each token\n",
    "3. **Get rewards** from discriminator\n",
    "4. **Calculate advantages** (reward - baseline)\n",
    "5. **Compute policy gradient** loss\n",
    "6. **Update generator** parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zNTnZs_I1ihB"
   },
   "source": [
    "class PolicyGradient:\n",
    "    \"\"\"REINFORCE algorithm for generator training\"\"\"\n",
    "\n",
    "    def __init__(self, generator, discriminator, rollout_num=4):\n",
    "        self.generator = generator\n",
    "        self.discriminator = discriminator\n",
    "        self.rollout_num = rollout_num\n",
    "        self.reward_baseline = 0.5  # Moving average baseline\n",
    "        self.baseline_momentum = 0.9\n",
    "\n",
    "    def get_reward(self, sequences, rollout=False, num_rollouts=4):\n",
    "        \"\"\"\n",
    "        Get reward from discriminator\n",
    "\n",
    "        Args:\n",
    "            sequences: Generated sequences [batch_size, seq_len]\n",
    "            rollout: Whether to use Monte Carlo rollouts\n",
    "            num_rollouts: Number of rollouts for incomplete sequences\n",
    "\n",
    "        Returns:\n",
    "            rewards: Reward for each sequence [batch_size]\n",
    "        \"\"\"\n",
    "        # Direct discriminator evaluation\n",
    "        rewards = self.discriminator(sequences, training=False)\n",
    "        rewards = tf.squeeze(rewards, axis=-1)\n",
    "\n",
    "        # Update baseline\n",
    "        current_mean = tf.reduce_mean(rewards)\n",
    "        self.reward_baseline = (\n",
    "            self.baseline_momentum * self.reward_baseline +\n",
    "            (1 - self.baseline_momentum) * current_mean\n",
    "        )\n",
    "\n",
    "        return rewards\n",
    "\n",
    "    def compute_pg_loss(self, sequences, char_condition=None, scene_condition=None):\n",
    "        \"\"\"\n",
    "        Compute policy gradient loss\n",
    "\n",
    "        Args:\n",
    "            sequences: Input sequences [batch_size, seq_len]\n",
    "            char_condition: Character conditions\n",
    "            scene_condition: Scene conditions\n",
    "\n",
    "        Returns:\n",
    "            loss: Policy gradient loss\n",
    "            rewards: Average reward\n",
    "        \"\"\"\n",
    "        # Get generator predictions (logits)\n",
    "        logits = self.generator(sequences[:, :-1], training=True,\n",
    "                               char_condition=char_condition,\n",
    "                               scene_condition=scene_condition)\n",
    "\n",
    "        # Compute log probabilities\n",
    "        log_probs = tf.nn.log_softmax(logits, axis=-1)\n",
    "\n",
    "        # Get log probabilities of selected tokens\n",
    "        # Create indices for gather_nd\n",
    "        batch_size = tf.shape(sequences)[0]\n",
    "        seq_len = tf.shape(sequences)[1] - 1\n",
    "\n",
    "        batch_indices = tf.tile(\n",
    "            tf.expand_dims(tf.range(batch_size), 1),\n",
    "            [1, seq_len]\n",
    "        )\n",
    "        time_indices = tf.tile(\n",
    "            tf.expand_dims(tf.range(seq_len), 0),\n",
    "            [batch_size, 1]\n",
    "        )\n",
    "        token_indices = sequences[:, 1:]\n",
    "\n",
    "        indices = tf.stack([\n",
    "            batch_indices,\n",
    "            time_indices,\n",
    "            token_indices\n",
    "        ], axis=-1)\n",
    "\n",
    "        selected_log_probs = tf.gather_nd(log_probs, indices)\n",
    "\n",
    "        # Get rewards from discriminator\n",
    "        rewards = self.get_reward(sequences)\n",
    "\n",
    "        # Compute advantages (reward - baseline)\n",
    "        advantages = rewards - self.reward_baseline\n",
    "\n",
    "        # Policy gradient loss\n",
    "        # Negative because we want to maximize reward\n",
    "        pg_loss = -tf.reduce_mean(\n",
    "            tf.reduce_sum(selected_log_probs, axis=1) * advantages\n",
    "        )\n",
    "\n",
    "        return pg_loss, tf.reduce_mean(rewards)\n",
    "\n",
    "# Create policy gradient trainer\n",
    "print('='*80)\n",
    "print('INITIALIZING POLICY GRADIENT TRAINER')\n",
    "print('='*80)\n",
    "\n",
    "pg_trainer = PolicyGradient(\n",
    "    generator=generator,\n",
    "    discriminator=discriminator,\n",
    "    rollout_num=4\n",
    ")\n",
    "\n",
    "print(f'Rollout samples: {pg_trainer.rollout_num}')\n",
    "print(f'Initial baseline: {pg_trainer.reward_baseline}')\n",
    "print(f'Baseline momentum: {pg_trainer.baseline_momentum}')\n",
    "print('='*80)"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dv9hatzj1ihB"
   },
   "source": [
    "## Section 4.0: Adversarial Training Loop\n",
    "\n",
    "Complete GAN training consists of two phases:\n",
    "\n",
    "### Phase 1: Pre-training (5-10 epochs each)\n",
    "\n",
    "**Generator Pre-training**:\n",
    "- Standard MLE (Maximum Likelihood Estimation)\n",
    "- Teacher forcing: Use ground truth previous tokens\n",
    "- Cross-entropy loss\n",
    "- Prepares generator for adversarial training\n",
    "\n",
    "**Discriminator Pre-training**:\n",
    "- Train on real data (label = 1) vs random/pre-trained generator samples (label = 0)\n",
    "- Binary cross-entropy loss\n",
    "- Label smoothing: real = 0.9, fake = 0.1\n",
    "- Provides initial signal for generator\n",
    "\n",
    "### Phase 2: Adversarial Training (20-50 epochs)\n",
    "\n",
    "**Training Loop**:\n",
    "```\n",
    "for epoch in epochs:\n",
    "    for batch in data:\n",
    "        # 1. Train Discriminator (1-2 steps)\n",
    "        fake_samples = generator.generate()\n",
    "        d_loss_real = discriminator.train(real_data, label=1)\n",
    "        d_loss_fake = discriminator.train(fake_samples, label=0)\n",
    "        \n",
    "        # 2. Train Generator (1 step)\n",
    "        fake_samples = generator.generate()\n",
    "        rewards = discriminator(fake_samples)\n",
    "        g_loss = policy_gradient(fake_samples, rewards)\n",
    "```\n",
    "\n",
    "### Training Techniques:\n",
    "\n",
    "**1. Label Smoothing**:\n",
    "- Real labels: 0.9 (not 1.0)\n",
    "- Fake labels: 0.1 (not 0.0)\n",
    "- Prevents overconfidence\n",
    "\n",
    "**2. Gradient Clipping**:\n",
    "- Clip gradients to [-1, 1]\n",
    "- Prevents exploding gradients\n",
    "- Stabilizes training\n",
    "\n",
    "**3. Learning Rate Scheduling**:\n",
    "- Start: 1e-4\n",
    "- Decay: Exponential or step decay\n",
    "- Separate rates for G and D\n",
    "\n",
    "**4. Batch Size**:\n",
    "- 32-128 samples per batch\n",
    "- Larger batches \u2192 more stable gradients\n",
    "- Smaller batches \u2192 faster iterations\n",
    "\n",
    "**5. Training Ratio**:\n",
    "- D updates: 1-2 per G update\n",
    "- Keeps D slightly ahead but not too strong\n",
    "\n",
    "### Monitoring:\n",
    "\n",
    "**Metrics to Track**:\n",
    "- Generator loss (policy gradient)\n",
    "- Discriminator loss (binary cross-entropy)\n",
    "- Discriminator accuracy (real vs fake)\n",
    "- Average reward from discriminator\n",
    "- Sample quality (manual inspection)\n",
    "- BLEU score on validation set\n",
    "\n",
    "**Early Stopping**:\n",
    "- Monitor validation BLEU score\n",
    "- Stop if no improvement for N epochs\n",
    "- Save best model based on metrics\n",
    "\n",
    "### Handling Issues:\n",
    "\n",
    "**Mode Collapse**:\n",
    "- Generator produces limited variety\n",
    "- Solution: Increase diversity penalty, minibatch discrimination\n",
    "\n",
    "**Training Instability**:\n",
    "- Losses oscillate wildly\n",
    "- Solution: Lower learning rate, stronger regularization\n",
    "\n",
    "**Discriminator Too Strong**:\n",
    "- Generator gets no useful gradient\n",
    "- Solution: Reduce D training steps, increase D dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "X6xQMUMH1ihC"
   },
   "source": [
    "# Hyperparameters",
    "BATCH_SIZE = 64",
    "SEQ_LENGTH = 50",
    "PRETRAIN_EPOCHS_G = 5",
    "PRETRAIN_EPOCHS_D = 3",
    "ADVERSARIAL_EPOCHS = 20",
    "D_STEPS = 1  # Train discriminator every N batches (not every batch)",
    "G_STEPS = 1",
    "LEARNING_RATE_G = 1e-4",
    "LEARNING_RATE_D = 1e-4",
    "LABEL_SMOOTHING = 0.1",
    "GRADIENT_CLIP = 1.0",
    "",
    "# Temperature annealing for Gumbel softmax",
    "TEMP_START = 2.0  # High temperature = more exploration",
    "TEMP_END = 0.5    # Low temperature = more exploitation",
    "TEMP_DECAY = 0.95  # Decay rate per epoch",
    "",
    "# Performance optimizations",
    "D_TRAIN_INTERVAL = 1  # Train D every N batches (1 = every batch)",
    "ACCURACY_METRIC_ON_GPU = True  # Keep accuracy on GPU",
    "",
    "print('='*80)",
    "print('TRAINING HYPERPARAMETERS')",
    "print('='*80)",
    "print(f'Batch size: {BATCH_SIZE}')",
    "print(f'Sequence length: {SEQ_LENGTH}')",
    "print(f'Generator pre-train epochs: {PRETRAIN_EPOCHS_G}')",
    "print(f'Discriminator pre-train epochs: {PRETRAIN_EPOCHS_D}')",
    "print(f'Adversarial epochs: {ADVERSARIAL_EPOCHS}')",
    "print(f'D steps per iteration: {D_STEPS}')",
    "print(f'G steps per iteration: {G_STEPS}')",
    "print(f'Learning rate (G): {LEARNING_RATE_G}')",
    "print(f'Learning rate (D): {LEARNING_RATE_D}')",
    "print(f'Label smoothing: {LABEL_SMOOTHING}')",
    "print(f'Gradient clipping: {GRADIENT_CLIP}')",
    "print(f'Temperature: {TEMP_START} \u2192 {TEMP_END} (decay: {TEMP_DECAY})')",
    "print(f'D train interval: {D_TRAIN_INTERVAL}')",
    "print('='*80)",
    ""
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5wSvLycu1ihC"
   },
   "source": [
    "# Create optimizers\n",
    "optimizer_g = Adam(learning_rate=LEARNING_RATE_G, clipnorm=GRADIENT_CLIP)\n",
    "optimizer_d = Adam(learning_rate=LEARNING_RATE_D, clipnorm=GRADIENT_CLIP)\n",
    "\n",
    "# Loss functions\n",
    "bce_loss = keras.losses.BinaryCrossentropy(from_logits=False)\n",
    "sparse_ce_loss = keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
    "\n",
    "# Training history\n",
    "history = {\n",
    "    'g_loss': [],\n",
    "    'd_loss': [],\n",
    "    'd_loss_real': [],\n",
    "    'd_loss_fake': [],\n",
    "    'd_acc': [],\n",
    "    'g_reward': [],\n",
    "    'epoch': []\n",
    "}\n",
    "\n",
    "print('Optimizers and loss functions initialized')"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "GbWoiVVK1ihC"
   },
   "source": [
    "# Pre-training functions\n",
    "\n",
    "@tf.function\n",
    "def pretrain_generator_step(sequences, char_cond, scene_cond):\n",
    "    \"\"\"Pre-train generator with MLE (teacher forcing)\"\"\"\n",
    "    with tf.GradientTape() as tape:\n",
    "        # Predict next tokens\n",
    "        logits = generator(sequences[:, :-1], training=True,\n",
    "                          char_condition=char_cond,\n",
    "                          scene_condition=scene_cond)\n",
    "\n",
    "        # MLE loss (cross-entropy)\n",
    "        loss = sparse_ce_loss(sequences[:, 1:], logits)\n",
    "\n",
    "    # Update generator\n",
    "    gradients = tape.gradient(loss, generator.trainable_variables)\n",
    "    optimizer_g.apply_gradients(zip(gradients, generator.trainable_variables))\n",
    "\n",
    "    return loss\n",
    "\n",
    "@tf.function\n",
    "def pretrain_discriminator_step(real_sequences, fake_sequences):\n",
    "    \"\"\"Pre-train discriminator on real vs fake\"\"\"\n",
    "    with tf.GradientTape() as tape:\n",
    "        # Predictions\n",
    "        real_pred = discriminator(real_sequences, training=True)\n",
    "        fake_pred = discriminator(fake_sequences, training=True)\n",
    "\n",
    "        # Labels with smoothing\n",
    "        real_labels = tf.ones_like(real_pred) * (1 - LABEL_SMOOTHING)\n",
    "        fake_labels = tf.ones_like(fake_pred) * LABEL_SMOOTHING\n",
    "\n",
    "        # Losses\n",
    "        loss_real = bce_loss(real_labels, real_pred)\n",
    "        loss_fake = bce_loss(fake_labels, fake_pred)\n",
    "        loss = loss_real + loss_fake\n",
    "\n",
    "    # Update discriminator\n",
    "    gradients = tape.gradient(loss, discriminator.trainable_variables)\n",
    "    optimizer_d.apply_gradients(zip(gradients, discriminator.trainable_variables))\n",
    "\n",
    "    # Accuracy\n",
    "    acc = (tf.reduce_mean(tf.cast(real_pred > 0.5, tf.float32)) +\n",
    "           tf.reduce_mean(tf.cast(fake_pred < 0.5, tf.float32))) / 2\n",
    "\n",
    "    return loss, loss_real, loss_fake, acc\n",
    "\n",
    "print('Pre-training step functions defined')"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MWcwB8q_1ihC"
   },
   "source": [
    "# Adversarial training functions",
    "",
    "@tf.function",
    "def train_discriminator_adversarial(real_sequences, fake_sequences):",
    "    \"\"\"Train discriminator during adversarial phase with gradient clipping\"\"\"",
    "    with tf.GradientTape() as tape:",
    "        real_pred = discriminator(real_sequences, training=True)",
    "        fake_pred = discriminator(fake_sequences, training=True)",
    "",
    "        real_labels = tf.ones_like(real_pred) * (1 - LABEL_SMOOTHING)",
    "        fake_labels = tf.ones_like(fake_pred) * LABEL_SMOOTHING",
    "",
    "        loss_real = bce_loss(real_labels, real_pred)",
    "        loss_fake = bce_loss(fake_labels, fake_pred)",
    "        loss = loss_real + loss_fake",
    "",
    "    gradients = tape.gradient(loss, discriminator.trainable_variables)",
    "    ",
    "    # Gradient clipping to prevent NaNs/explosions",
    "    gradients, grad_norm = tf.clip_by_global_norm(gradients, GRADIENT_CLIP)",
    "    ",
    "    # Check for NaNs and skip update if found",
    "    has_nan = tf.reduce_any([tf.reduce_any(tf.math.is_nan(g)) for g in gradients if g is not None])",
    "    ",
    "    if not has_nan:",
    "        optimizer_d.apply_gradients(zip(gradients, discriminator.trainable_variables))",
    "",
    "    # Keep accuracy computation on GPU (no .numpy() calls)",
    "    acc = (tf.reduce_mean(tf.cast(real_pred > 0.5, tf.float32)) +",
    "           tf.reduce_mean(tf.cast(fake_pred < 0.5, tf.float32))) / 2",
    "",
    "    return loss, loss_real, loss_fake, acc",
    "",
    "@tf.function",
    "def train_generator_adversarial(sequences, char_cond, scene_cond, temperature=1.0):",
    "    \"\"\"Train generator with policy gradient and temperature annealing\"\"\"",
    "    with tf.GradientTape() as tape:",
    "        # Get logits with temperature scaling",
    "        logits = generator(sequences[:, :-1], training=True,",
    "                          char_condition=char_cond,",
    "                          scene_condition=scene_cond)",
    "        ",
    "        # Apply temperature to logits for exploration/exploitation",
    "        logits = logits / temperature",
    "",
    "        # Log probabilities",
    "        log_probs = tf.nn.log_softmax(logits, axis=-1)",
    "",
    "        # Get selected token log probs",
    "        batch_size = tf.shape(sequences)[0]",
    "        seq_len = tf.shape(sequences)[1] - 1",
    "",
    "        batch_idx = tf.tile(tf.expand_dims(tf.range(batch_size), 1), [1, seq_len])",
    "        time_idx = tf.tile(tf.expand_dims(tf.range(seq_len), 0), [batch_size, 1])",
    "        token_idx = sequences[:, 1:]",
    "",
    "        indices = tf.stack([batch_idx, time_idx, token_idx], axis=-1)",
    "        selected_log_probs = tf.gather_nd(log_probs, indices)",
    "",
    "        # Get rewards from discriminator",
    "        rewards = discriminator(sequences, training=False)",
    "        rewards = tf.squeeze(rewards, axis=-1)",
    "",
    "        # Advantages (simple baseline: mean reward)",
    "        baseline = tf.reduce_mean(rewards)",
    "        advantages = rewards - baseline",
    "",
    "        # Policy gradient loss",
    "        loss = -tf.reduce_mean(tf.reduce_sum(selected_log_probs, axis=1) * advantages)",
    "",
    "    gradients = tape.gradient(loss, generator.trainable_variables)",
    "    ",
    "    # Gradient clipping",
    "    gradients, grad_norm = tf.clip_by_global_norm(gradients, GRADIENT_CLIP)",
    "    ",
    "    # Check for NaNs",
    "    has_nan = tf.reduce_any([tf.reduce_any(tf.math.is_nan(g)) for g in gradients if g is not None])",
    "    ",
    "    if not has_nan:",
    "        optimizer_g.apply_gradients(zip(gradients, generator.trainable_variables))",
    "",
    "    return loss, tf.reduce_mean(rewards)",
    "",
    "print('Adversarial training step functions defined (OPTIMIZED)')",
    "print('\u2713 Gradient clipping enabled')",
    "print('\u2713 NaN checking enabled')",
    "print('\u2713 Temperature annealing support added')",
    ""
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "WeU3gUwv1ihD"
   },
   "source": [
    "# Helper function to generate fake samples\n",
    "def generate_fake_samples(batch_size, char_conditions, scene_conditions):\n",
    "    \"\"\"Generate fake samples from generator\"\"\"\n",
    "    # Start with random token or padding\n",
    "    start_tokens = tf.ones((batch_size, 1), dtype=tf.int32)\n",
    "\n",
    "    # Generate sequences\n",
    "    generated = generator.generate_sequence(\n",
    "        start_tokens,\n",
    "        max_length=SEQ_LENGTH,\n",
    "        temperature=1.0,\n",
    "        char_condition=char_conditions,\n",
    "        scene_condition=scene_conditions,\n",
    "        method='sampling'\n",
    "    )\n",
    "\n",
    "    return generated\n",
    "\n",
    "# Create tf.data datasets for efficient batching\n",
    "train_dataset = tf.data.Dataset.from_tensor_slices((\n",
    "    X_train, char_train, scene_train\n",
    ")).shuffle(10000).batch(BATCH_SIZE).prefetch(tf.data.AUTOTUNE)\n",
    "\n",
    "val_dataset = tf.data.Dataset.from_tensor_slices((\n",
    "    X_val, char_val, scene_val\n",
    ")).batch(BATCH_SIZE).prefetch(tf.data.AUTOTUNE)\n",
    "\n",
    "print(f'Training dataset: {len(list(train_dataset))} batches')\n",
    "print(f'Validation dataset: {len(list(val_dataset))} batches')\n",
    "print('\\nReady to start training!')"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "asxr6SN11ihD"
   },
   "source": [
    "# Phase 1: Pre-train Generator (OPTIMIZED)",
    "print('='*80)",
    "print('PHASE 1: GENERATOR PRE-TRAINING (OPTIMIZED)')",
    "print('='*80)",
    "",
    "import time",
    "",
    "for epoch in range(PRETRAIN_EPOCHS_G):",
    "    print(f'\\nEpoch {epoch+1}/{PRETRAIN_EPOCHS_G}')",
    "    epoch_losses_tensor = []",
    "    epoch_start_time = time.time()",
    "",
    "    for batch_idx, (batch_seqs, batch_char, batch_scene) in enumerate(train_dataset):",
    "        loss = pretrain_generator_step(batch_seqs, batch_char, batch_scene)",
    "        epoch_losses_tensor.append(loss)  # Keep as tensor",
    "",
    "        if (batch_idx + 1) % 20 == 0:",
    "            print(f'  Batch {batch_idx+1}: Loss={loss.numpy():.4f}')",
    "    ",
    "    # Convert to numpy ONCE per epoch",
    "    epoch_losses = [l.numpy() for l in epoch_losses_tensor]",
    "    avg_loss = np.mean(epoch_losses)",
    "    epoch_time = time.time() - epoch_start_time",
    "    ",
    "    print(f'  Average Loss: {avg_loss:.4f} | Time: {epoch_time:.2f}s')",
    "",
    "print('Generator pre-training complete!')",
    ""
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "n6ECWghF1ihD"
   },
   "source": [
    "# Phase 2: Pre-train Discriminator (OPTIMIZED)",
    "print('='*80)",
    "print('PHASE 2: DISCRIMINATOR PRE-TRAINING (OPTIMIZED)')",
    "print('='*80)",
    "",
    "import time",
    "",
    "for epoch in range(PRETRAIN_EPOCHS_D):",
    "    print(f'\\nEpoch {epoch+1}/{PRETRAIN_EPOCHS_D}')",
    "    epoch_losses_tensor = []",
    "    epoch_accs_tensor = []",
    "    epoch_start_time = time.time()",
    "",
    "    for batch_idx, (batch_seqs, batch_char, batch_scene) in enumerate(train_dataset):",
    "        # Generate fake samples",
    "        fake_seqs = generate_fake_samples(",
    "            tf.shape(batch_seqs)[0], batch_char, batch_scene",
    "        )",
    "",
    "        # Train discriminator",
    "        loss, acc = pretrain_discriminator_step(batch_seqs, fake_seqs)",
    "        ",
    "        # Accumulate as tensors",
    "        epoch_losses_tensor.append(loss)",
    "        epoch_accs_tensor.append(acc)",
    "",
    "        if (batch_idx + 1) % 20 == 0:",
    "            print(f'  Batch {batch_idx+1}: Loss={loss.numpy():.4f}, Acc={acc.numpy():.4f}')",
    "    ",
    "    # Convert to numpy ONCE per epoch",
    "    epoch_losses = [l.numpy() for l in epoch_losses_tensor]",
    "    epoch_accs = [a.numpy() for a in epoch_accs_tensor]",
    "    avg_loss = np.mean(epoch_losses)",
    "    avg_acc = np.mean(epoch_accs)",
    "    epoch_time = time.time() - epoch_start_time",
    "    ",
    "    print(f'  Average Loss: {avg_loss:.4f}, Accuracy: {avg_acc:.4f} | Time: {epoch_time:.2f}s')",
    "",
    "print('Discriminator pre-training complete!')",
    ""
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7CkE5riM1ihD"
   },
   "source": [
    "# Phase 3: Adversarial Training (OPTIMIZED)",
    "print('='*80)",
    "print('PHASE 3: ADVERSARIAL TRAINING (OPTIMIZED)')",
    "print('='*80)",
    "print('\ud83d\ude80 Performance optimizations enabled:')",
    "print('  \u2713 No .numpy() calls in inner loop')",
    "print('  \u2713 GPU tensor accumulation')",
    "print('  \u2713 Temperature annealing')",
    "print('  \u2713 Gradient clipping with NaN protection')",
    "print('  \u2713 Batch timing for speed comparison')",
    "print('='*80)",
    "",
    "import time",
    "",
    "# Initialize temperature",
    "current_temp = TEMP_START",
    "",
    "# Track batch times for performance measurement",
    "batch_times = []",
    "",
    "for epoch in range(ADVERSARIAL_EPOCHS):",
    "    print(f'\\nEpoch {epoch+1}/{ADVERSARIAL_EPOCHS} | Temperature: {current_temp:.3f}')",
    "    ",
    "    # Use TensorFlow tensors for accumulation (stay on GPU)",
    "    g_losses_tensor = []",
    "    d_losses_tensor = []",
    "    d_accs_tensor = []",
    "    g_rewards_tensor = []",
    "    ",
    "    epoch_start_time = time.time()",
    "    batch_count = 0",
    "",
    "    for batch_idx, (batch_seqs, batch_char, batch_scene) in enumerate(train_dataset):",
    "        batch_start_time = time.time()",
    "        ",
    "        # Train Discriminator (with optional interval training)",
    "        if batch_idx % D_TRAIN_INTERVAL == 0:",
    "            for _ in range(D_STEPS):",
    "                fake_seqs = generate_fake_samples(",
    "                    tf.shape(batch_seqs)[0], batch_char, batch_scene",
    "                )",
    "                d_loss, d_loss_real, d_loss_fake, d_acc = train_discriminator_adversarial(",
    "                    batch_seqs, fake_seqs",
    "                )",
    "        ",
    "        # Accumulate as tensors (NO .numpy() calls in loop!)",
    "        d_losses_tensor.append(d_loss)",
    "        d_accs_tensor.append(d_acc)",
    "        ",
    "        # Train Generator with temperature annealing",
    "        for _ in range(G_STEPS):",
    "            g_loss, g_reward = train_generator_adversarial(",
    "                batch_seqs, batch_char, batch_scene, temperature=current_temp",
    "            )",
    "        ",
    "        # Accumulate as tensors",
    "        g_losses_tensor.append(g_loss)",
    "        g_rewards_tensor.append(g_reward)",
    "        ",
    "        batch_time = time.time() - batch_start_time",
    "        batch_times.append(batch_time)",
    "        batch_count += 1",
    "        ",
    "        # Print progress every 10 batches (convert to numpy ONLY for display)",
    "        if (batch_idx + 1) % 10 == 0:",
    "            # Single conversion per metric for display",
    "            g_loss_val = g_loss.numpy()",
    "            d_loss_val = d_loss.numpy()",
    "            d_acc_val = d_acc.numpy()",
    "            g_reward_val = g_reward.numpy()",
    "            avg_batch_time = sum(batch_times[-10:]) / min(10, len(batch_times))",
    "            ",
    "            print(f'  Batch {batch_idx+1}: '",
    "                  f'G_Loss={g_loss_val:.4f}, '",
    "                  f'D_Loss={d_loss_val:.4f}, '",
    "                  f'D_Acc={d_acc_val:.4f}, '",
    "                  f'Reward={g_reward_val:.4f} '",
    "                  f'| \u26a1{avg_batch_time*1000:.1f}ms/batch')",
    "    ",
    "    # Convert accumulated tensors to numpy ONCE per epoch",
    "    g_losses = [loss.numpy() for loss in g_losses_tensor]",
    "    d_losses = [loss.numpy() for loss in d_losses_tensor]",
    "    d_accs = [acc.numpy() for acc in d_accs_tensor]",
    "    g_rewards = [reward.numpy() for reward in g_rewards_tensor]",
    "    ",
    "    # Epoch summary",
    "    avg_g_loss = np.mean(g_losses)",
    "    avg_d_loss = np.mean(d_losses)",
    "    avg_d_acc = np.mean(d_accs)",
    "    avg_reward = np.mean(g_rewards)",
    "    ",
    "    epoch_time = time.time() - epoch_start_time",
    "    avg_batch_time = epoch_time / batch_count if batch_count > 0 else 0",
    "",
    "    history['g_loss'].append(avg_g_loss)",
    "    history['d_loss'].append(avg_d_loss)",
    "    history['d_acc'].append(avg_d_acc)",
    "    history['g_reward'].append(avg_reward)",
    "    history['epoch'].append(epoch + 1)",
    "    ",
    "    # Anneal temperature",
    "    current_temp = max(TEMP_END, current_temp * TEMP_DECAY)",
    "",
    "    print(f'\\n  \ud83d\udcca EPOCH SUMMARY:')",
    "    print(f'    Generator Loss: {avg_g_loss:.4f}')",
    "    print(f'    Discriminator Loss: {avg_d_loss:.4f}')",
    "    print(f'    Discriminator Accuracy: {avg_d_acc:.4f} {\"\ud83c\udfaf\" if avg_d_acc > 0.7 else \"\"}')",
    "    print(f'    Average Reward: {avg_reward:.4f}')",
    "    print(f'    Epoch Time: {epoch_time:.2f}s | Avg Batch: {avg_batch_time*1000:.1f}ms')",
    "",
    "    # Generate sample after each epoch",
    "    if (epoch + 1) % 5 == 0:",
    "        print(f'\\n  \ud83c\udfad Sample Generation (Temperature={current_temp:.2f}):')",
    "        test_start = tf.ones((1, 1), dtype=tf.int32)",
    "        test_char = tf.constant([0])  # First character",
    "        test_scene = tf.constant([0])  # First scene",
    "",
    "        sample = generator.generate_sequence(",
    "            test_start, max_length=30, temperature=current_temp,",
    "            char_condition=test_char, scene_condition=test_scene",
    "        )",
    "",
    "        tokens = sample[0].numpy()",
    "        decoded = ' '.join([tokenizer.index_word.get(int(t), '<UNK>')",
    "                           for t in tokens if t > 0])",
    "        print(f'    \"{decoded}\"')",
    "",
    "print('\\n' + '='*80)",
    "print('\u2705 ADVERSARIAL TRAINING COMPLETE!')",
    "print('='*80)",
    "print(f'Performance Stats:')",
    "print(f'  Total batches: {len(batch_times)}')",
    "print(f'  Avg batch time: {np.mean(batch_times)*1000:.1f}ms')",
    "print(f'  Min batch time: {np.min(batch_times)*1000:.1f}ms')",
    "print(f'  Max batch time: {np.max(batch_times)*1000:.1f}ms')",
    "print(f'  Final temperature: {current_temp:.3f}')",
    "print(f'  Final D accuracy: {history[\"d_acc\"][-1]:.4f}')",
    "print('='*80)",
    ""
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "shQzkWQZ1ihE"
   },
   "source": [
    "## Section 5.0: Script Autocomplete Tool\n",
    "\n",
    "Now we'll create an interactive dialogue autocomplete system.\n",
    "\n",
    "### Features:\n",
    "\n",
    "**1. autocomplete_dialogue() Function**:\n",
    "- Takes seed text, character, scene type, temperature\n",
    "- Generates multiple completions\n",
    "- Returns ranked results with quality scores\n",
    "\n",
    "**Parameters**:\n",
    "- `seed_text`: Starting dialogue (e.g., \"I will destroy all\")\n",
    "- `character`: Character name (Eren, Mikasa, Armin, etc.)\n",
    "- `scene_type`: Scene context (Battle, Training, Strategy, etc.)\n",
    "- `num_completions`: Number of different completions to generate\n",
    "- `max_length`: Maximum tokens in completion\n",
    "- `temperature`: Sampling randomness (0.5-2.0)\n",
    "\n",
    "**2. Interactive Widget Demo**:\n",
    "- Text input for seed dialogue\n",
    "- Dropdown for character selection\n",
    "- Dropdown for scene selection\n",
    "- Slider for temperature control\n",
    "- Button to generate completions\n",
    "- Display with quality scores from discriminator\n",
    "\n",
    "**3. Quality Scoring**:\n",
    "- Use discriminator to rate each completion\n",
    "- Higher score = more realistic\n",
    "- Sort completions by quality\n",
    "- Show top results\n",
    "\n",
    "### Use Cases:\n",
    "\n",
    "- **Script writing assistance**: Complete dialogue lines\n",
    "- **Creative exploration**: Generate variations\n",
    "- **Character voice**: Ensure consistent character speech\n",
    "- **Scene coherence**: Match dialogue to scene type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "iKISLg7G1ihE"
   },
   "source": [
    "def autocomplete_dialogue(seed_text, character=None, scene_type=None,\n",
    "                          num_completions=5, max_length=50, temperature=0.8):\n",
    "    \"\"\"\n",
    "    Generate dialogue completions with quality scores\n",
    "\n",
    "    Args:\n",
    "        seed_text: Starting text to complete\n",
    "        character: Character name\n",
    "        scene_type: Scene type\n",
    "        num_completions: Number of completions to generate\n",
    "        max_length: Maximum length of completion\n",
    "        temperature: Sampling temperature\n",
    "\n",
    "    Returns:\n",
    "        List of (completion_text, quality_score) tuples\n",
    "    \"\"\"\n",
    "    # Encode seed text\n",
    "    seed_tokens = tokenizer.texts_to_sequences([seed_text.lower()])[0]\n",
    "    if not seed_tokens:\n",
    "        seed_tokens = [1]  # Start token\n",
    "\n",
    "    # Pad to at least length 1\n",
    "    seed_tokens = seed_tokens[:max_length]\n",
    "    seed_tensor = tf.constant([seed_tokens])\n",
    "\n",
    "    # Get character and scene indices\n",
    "    char_idx = char_to_idx.get(character, 0)\n",
    "    scene_idx = scene_to_idx.get(scene_type, 0)\n",
    "\n",
    "    char_tensor = tf.constant([char_idx])\n",
    "    scene_tensor = tf.constant([scene_idx])\n",
    "\n",
    "    # Generate multiple completions\n",
    "    completions = []\n",
    "\n",
    "    for i in range(num_completions):\n",
    "        # Generate sequence\n",
    "        generated = generator.generate_sequence(\n",
    "            seed_tensor,\n",
    "            max_length=max_length,\n",
    "            temperature=temperature,\n",
    "            char_condition=char_tensor,\n",
    "            scene_condition=scene_tensor,\n",
    "            method='sampling'\n",
    "        )\n",
    "\n",
    "        # Get quality score from discriminator\n",
    "        # Pad/truncate to SEQ_LENGTH\n",
    "        gen_padded = pad_sequences(\n",
    "            generated.numpy(),\n",
    "            maxlen=SEQ_LENGTH,\n",
    "            padding='pre'\n",
    "        )\n",
    "        quality_score = discriminator(gen_padded, training=False).numpy()[0][0]\n",
    "\n",
    "        # Decode tokens\n",
    "        tokens = generated[0].numpy()\n",
    "        decoded = ' '.join([tokenizer.index_word.get(int(t), '')\n",
    "                           for t in tokens if t > 0])\n",
    "        decoded = decoded.strip()\n",
    "\n",
    "        completions.append((decoded, float(quality_score)))\n",
    "\n",
    "    # Sort by quality score (descending)\n",
    "    completions.sort(key=lambda x: x[1], reverse=True)\n",
    "\n",
    "    return completions\n",
    "\n",
    "print('autocomplete_dialogue() function defined')"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zEjFVMGV1ihE"
   },
   "source": [
    "# Test the autocomplete function\n",
    "print('='*80)\n",
    "print('TESTING AUTOCOMPLETE FUNCTION')\n",
    "print('='*80)\n",
    "\n",
    "test_cases = [\n",
    "    ('I will destroy', 'Eren', 'Battle', 0.8),\n",
    "    ('We need to', 'Armin', 'Strategy Meeting', 0.7),\n",
    "    ('I will protect', 'Mikasa', 'Battle', 0.8),\n",
    "]\n",
    "\n",
    "for seed, char, scene, temp in test_cases:\n",
    "    print(f'\\nSeed: \"{seed}\" | Character: {char} | Scene: {scene} | Temp: {temp}')\n",
    "    print('-' * 80)\n",
    "\n",
    "    completions = autocomplete_dialogue(\n",
    "        seed, char, scene,\n",
    "        num_completions=3,\n",
    "        max_length=30,\n",
    "        temperature=temp\n",
    "    )\n",
    "\n",
    "    for idx, (text, score) in enumerate(completions, 1):\n",
    "        print(f'{idx}. [{score:.3f}] {text}')\n",
    "\n",
    "print('\\n' + '='*80)"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "k8dc2H1N1ihE"
   },
   "source": [
    "# Interactive widget demo\n",
    "try:\n",
    "    from ipywidgets import interact, widgets\n",
    "    from IPython.display import display, HTML\n",
    "\n",
    "    # Widget components\n",
    "    seed_input = widgets.Text(\n",
    "        value='I will',\n",
    "        placeholder='Enter seed text...',\n",
    "        description='Seed Text:',\n",
    "        style={'description_width': 'initial'}\n",
    "    )\n",
    "\n",
    "    char_dropdown = widgets.Dropdown(\n",
    "        options=list(char_to_idx.keys()),\n",
    "        value=list(char_to_idx.keys())[0],\n",
    "        description='Character:',\n",
    "        style={'description_width': 'initial'}\n",
    "    )\n",
    "\n",
    "    scene_dropdown = widgets.Dropdown(\n",
    "        options=list(scene_to_idx.keys()),\n",
    "        value=list(scene_to_idx.keys())[0],\n",
    "        description='Scene:',\n",
    "        style={'description_width': 'initial'}\n",
    "    )\n",
    "\n",
    "    temp_slider = widgets.FloatSlider(\n",
    "        value=0.8,\n",
    "        min=0.3,\n",
    "        max=1.5,\n",
    "        step=0.1,\n",
    "        description='Temperature:',\n",
    "        style={'description_width': 'initial'}\n",
    "    )\n",
    "\n",
    "    num_slider = widgets.IntSlider(\n",
    "        value=5,\n",
    "        min=1,\n",
    "        max=10,\n",
    "        description='# Completions:',\n",
    "        style={'description_width': 'initial'}\n",
    "    )\n",
    "\n",
    "    output = widgets.Output()\n",
    "\n",
    "    def generate_button_clicked(b):\n",
    "        with output:\n",
    "            output.clear_output()\n",
    "            print('Generating completions...')\n",
    "\n",
    "            completions = autocomplete_dialogue(\n",
    "                seed_input.value,\n",
    "                char_dropdown.value,\n",
    "                scene_dropdown.value,\n",
    "                num_completions=num_slider.value,\n",
    "                max_length=40,\n",
    "                temperature=temp_slider.value\n",
    "            )\n",
    "\n",
    "            output.clear_output()\n",
    "            print(f'Generated {len(completions)} completions:\\n')\n",
    "            print('='*80)\n",
    "\n",
    "            for idx, (text, score) in enumerate(completions, 1):\n",
    "                quality = '\u2605' * int(score * 5)\n",
    "                print(f'\\n{idx}. Quality: {quality} ({score:.3f})')\n",
    "                print(f'   {text}')\n",
    "\n",
    "            print('\\n' + '='*80)\n",
    "\n",
    "    generate_btn = widgets.Button(\n",
    "        description='Generate Dialogue',\n",
    "        button_style='success',\n",
    "        icon='play'\n",
    "    )\n",
    "    generate_btn.on_click(generate_button_clicked)\n",
    "\n",
    "    # Display widgets\n",
    "    print('='*80)\n",
    "    print('INTERACTIVE DIALOGUE AUTOCOMPLETE')\n",
    "    print('='*80)\n",
    "\n",
    "    display(HTML('<h3>Custom Dialogue Dialogue Generator</h3>'))\n",
    "    display(seed_input)\n",
    "    display(char_dropdown)\n",
    "    display(scene_dropdown)\n",
    "    display(temp_slider)\n",
    "    display(num_slider)\n",
    "    display(generate_btn)\n",
    "    display(output)\n",
    "\n",
    "except ImportError:\n",
    "    print('ipywidgets not available. Use autocomplete_dialogue() function directly.')\n"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ARpfZpsG1ihE"
   },
   "source": [
    "## Section 6.0: Evaluation Metrics\n",
    "\n",
    "We evaluate our GAN using multiple metrics:\n",
    "\n",
    "### 1. Perplexity\n",
    "Measures how well the model predicts the test set:\n",
    "$$PPL = \\exp\\left(-\\frac{1}{N}\\sum_{i=1}^{N} \\log P(w_i|w_{<i})\\right)$$\n",
    "\n",
    "Lower is better. Typical ranges:\n",
    "- Excellent: < 20\n",
    "- Good: 20-50\n",
    "- Fair: 50-100\n",
    "- Poor: > 100\n",
    "\n",
    "### 2. BLEU Score\n",
    "Measures n-gram overlap with reference text:\n",
    "$$BLEU = BP \\cdot \\exp\\left(\\sum_{n=1}^{N} w_n \\log p_n\\right)$$\n",
    "\n",
    "Where:\n",
    "- $p_n$: n-gram precision\n",
    "- $BP$: Brevity penalty\n",
    "- Range: [0, 1], higher is better\n",
    "\n",
    "### 3. Self-BLEU (Diversity)\n",
    "Measures diversity by computing BLEU between generated samples:\n",
    "- Lower Self-BLEU = Higher diversity\n",
    "- Detects mode collapse\n",
    "- Ensures variety in generation\n",
    "\n",
    "### 4. Discriminator Accuracy\n",
    "How well D distinguishes real vs fake:\n",
    "- Target: ~50-70% (balanced)\n",
    "- Too high (>90%): Generator failing\n",
    "- Too low (<40%): Discriminator failing\n",
    "\n",
    "### 5. Generator Loss Curves\n",
    "Monitor training stability:\n",
    "- Should gradually decrease\n",
    "- Oscillations are normal\n",
    "- Divergence indicates instability\n",
    "\n",
    "### 6. Sample Quality (Human Evaluation)\n",
    "Ultimate test:\n",
    "- Fluency: Grammatically correct?\n",
    "- Coherence: Makes sense in context?\n",
    "- Character voice: Sounds like the character?\n",
    "- Diversity: Varied or repetitive?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "iGlqbPhZ1ihF"
   },
   "source": [
    "def compute_perplexity(model, dataset, tokenizer_obj):\n",
    "    \"\"\"Compute perplexity on dataset\"\"\"\n",
    "    total_loss = 0\n",
    "    total_tokens = 0\n",
    "\n",
    "    for batch_seqs, batch_char, batch_scene in dataset:\n",
    "        logits = model(batch_seqs[:, :-1], training=False,\n",
    "                      char_condition=batch_char,\n",
    "                      scene_condition=batch_scene)\n",
    "\n",
    "        loss = sparse_ce_loss(batch_seqs[:, 1:], logits)\n",
    "\n",
    "        # Count non-padding tokens\n",
    "        mask = tf.cast(batch_seqs[:, 1:] != 0, tf.float32)\n",
    "        num_tokens = tf.reduce_sum(mask)\n",
    "\n",
    "        total_loss += loss.numpy() * num_tokens.numpy()\n",
    "        total_tokens += num_tokens.numpy()\n",
    "\n",
    "    avg_loss = total_loss / total_tokens\n",
    "    perplexity = np.exp(avg_loss)\n",
    "\n",
    "    return perplexity\n",
    "\n",
    "def compute_bleu(reference, hypothesis):\n",
    "    \"\"\"Compute BLEU score\"\"\"\n",
    "    smoothing = SmoothingFunction()\n",
    "    reference_tokens = reference.lower().split()\n",
    "    hypothesis_tokens = hypothesis.lower().split()\n",
    "\n",
    "    score = sentence_bleu(\n",
    "        [reference_tokens],\n",
    "        hypothesis_tokens,\n",
    "        smoothing_function=smoothing.method1\n",
    "    )\n",
    "    return score\n",
    "\n",
    "def compute_self_bleu(generated_texts, sample_size=100):\n",
    "    \"\"\"Compute Self-BLEU for diversity measurement\"\"\"\n",
    "    if len(generated_texts) < 2:\n",
    "        return 0.0\n",
    "\n",
    "    # Sample if too many\n",
    "    if len(generated_texts) > sample_size:\n",
    "        generated_texts = np.random.choice(\n",
    "            generated_texts, sample_size, replace=False\n",
    "        ).tolist()\n",
    "\n",
    "    smoothing = SmoothingFunction()\n",
    "    scores = []\n",
    "\n",
    "    for i, hyp in enumerate(generated_texts):\n",
    "        refs = [text for j, text in enumerate(generated_texts) if j != i]\n",
    "        hyp_tokens = hyp.lower().split()\n",
    "        ref_tokens = [ref.lower().split() for ref in refs]\n",
    "\n",
    "        if hyp_tokens:\n",
    "            score = sentence_bleu(\n",
    "                ref_tokens,\n",
    "                hyp_tokens,\n",
    "                smoothing_function=smoothing.method1\n",
    "            )\n",
    "            scores.append(score)\n",
    "\n",
    "    return np.mean(scores) if scores else 0.0\n",
    "\n",
    "print('Evaluation metric functions defined')"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HohyiPKn1ihF"
   },
   "source": [
    "# Evaluate models\n",
    "print('='*80)\n",
    "print('EVALUATION METRICS')\n",
    "print('='*80)\n",
    "\n",
    "# 1. Perplexity\n",
    "print('\\n1. Computing Perplexity...')\n",
    "val_perplexity = compute_perplexity(generator, val_dataset, tokenizer)\n",
    "print(f'   Validation Perplexity: {val_perplexity:.2f}')\n",
    "\n",
    "# 2. Generate samples for BLEU and Self-BLEU\n",
    "print('\\n2. Generating samples for BLEU evaluation...')\n",
    "generated_samples = []\n",
    "reference_samples = []\n",
    "\n",
    "for i in range(50):\n",
    "    # Random character and scene\n",
    "    char_idx = np.random.randint(0, num_characters)\n",
    "    scene_idx = np.random.randint(0, num_scenes)\n",
    "\n",
    "    # Generate\n",
    "    start = tf.ones((1, 1), dtype=tf.int32)\n",
    "    generated = generator.generate_sequence(\n",
    "        start, max_length=30, temperature=0.8,\n",
    "        char_condition=tf.constant([char_idx]),\n",
    "        scene_condition=tf.constant([scene_idx])\n",
    "    )\n",
    "\n",
    "    tokens = generated[0].numpy()\n",
    "    text = ' '.join([tokenizer.index_word.get(int(t), '')\n",
    "                    for t in tokens if t > 0])\n",
    "    generated_samples.append(text.strip())\n",
    "\n",
    "    # Get reference from validation set\n",
    "    if i < len(X_val):\n",
    "        ref_tokens = X_val[i]\n",
    "        ref_text = ' '.join([tokenizer.index_word.get(int(t), '')\n",
    "                            for t in ref_tokens if t > 0])\n",
    "        reference_samples.append(ref_text.strip())\n",
    "\n",
    "# 3. BLEU Score\n",
    "print('\\n3. Computing BLEU scores...')\n",
    "bleu_scores = []\n",
    "for gen, ref in zip(generated_samples[:len(reference_samples)], reference_samples):\n",
    "    if gen and ref:\n",
    "        score = compute_bleu(ref, gen)\n",
    "        bleu_scores.append(score)\n",
    "\n",
    "avg_bleu = np.mean(bleu_scores) if bleu_scores else 0.0\n",
    "print(f'   Average BLEU Score: {avg_bleu:.4f}')\n",
    "\n",
    "# 4. Self-BLEU (Diversity)\n",
    "print('\\n4. Computing Self-BLEU (diversity metric)...')\n",
    "self_bleu = compute_self_bleu(generated_samples)\n",
    "print(f'   Self-BLEU Score: {self_bleu:.4f}')\n",
    "print(f'   (Lower is better - indicates higher diversity)')\n",
    "\n",
    "# 5. Discriminator Accuracy\n",
    "print('\\n5. Discriminator Accuracy...')\n",
    "if history['d_acc']:\n",
    "    final_d_acc = history['d_acc'][-1]\n",
    "    print(f'   Final Discriminator Accuracy: {final_d_acc:.4f}')\n",
    "    print(f'   (Target range: 0.50-0.70 for balanced training)')\n",
    "\n",
    "# 6. Sample Generation at Different Temperatures\n",
    "print('\\n6. Sample Generations at Different Temperatures:')\n",
    "print('-' * 80)\n",
    "\n",
    "for temp in [0.5, 0.8, 1.0, 1.2]:\n",
    "    start = tf.ones((1, 1), dtype=tf.int32)\n",
    "    generated = generator.generate_sequence(\n",
    "        start, max_length=25, temperature=temp,\n",
    "        char_condition=tf.constant([0]),\n",
    "        scene_condition=tf.constant([0])\n",
    "    )\n",
    "\n",
    "    tokens = generated[0].numpy()\n",
    "    text = ' '.join([tokenizer.index_word.get(int(t), '')\n",
    "                    for t in tokens if t > 0])\n",
    "\n",
    "    print(f'\\nTemperature {temp}:')\n",
    "    print(f'  \"{text}\"')\n",
    "\n",
    "print('\\n' + '='*80)\n",
    "print('EVALUATION SUMMARY')\n",
    "print('='*80)\n",
    "print(f'Perplexity: {val_perplexity:.2f}')\n",
    "print(f'BLEU Score: {avg_bleu:.4f}')\n",
    "print(f'Self-BLEU (Diversity): {self_bleu:.4f}')\n",
    "if history['d_acc']:\n",
    "    print(f'Discriminator Accuracy: {final_d_acc:.4f}')\n",
    "print('='*80)"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XBotZUei1ihF"
   },
   "source": [
    "## Section 7.0: Advanced Features\n",
    "\n",
    "### 1. Character-Conditioned Generation\n",
    "\n",
    "Generate dialogue specific to characters:\n",
    "- **Eren**: Passionate, determined, emotional\n",
    "- **Mikasa**: Protective, calm, focused\n",
    "- **Armin**: Strategic, thoughtful, analytical\n",
    "- **Levi**: Stern, commanding, pragmatic\n",
    "\n",
    "### 2. Scene-Type Conditioning\n",
    "\n",
    "Adapt dialogue to scene context:\n",
    "- **Scene Type 1**: Define based on your dataset\n",
    "- **Scene Type 2**: Define based on your dataset\n",
    "- **Scene Type 3**: Define based on your dataset\n",
    "- Additional scene types as needed\n",
    "\n",
    "### 3. Length Control\n",
    "\n",
    "Generate sequences of specific lengths:\n",
    "- Short responses (5-10 tokens)\n",
    "- Medium dialogue (10-30 tokens)\n",
    "- Long passages (30+ tokens)\n",
    "\n",
    "### 4. Beam Search Decoding\n",
    "\n",
    "Alternative to sampling for more consistent output:\n",
    "- Keep top-k candidates at each step\n",
    "- Select best overall sequence\n",
    "- More deterministic than sampling\n",
    "\n",
    "### 5. Top-K and Top-P (Nucleus) Sampling\n",
    "\n",
    "**Top-K Sampling**:\n",
    "- Sample from top k most likely tokens\n",
    "- k=50 is common\n",
    "- Filters out unlikely tokens\n",
    "\n",
    "**Top-P (Nucleus) Sampling**:\n",
    "- Sample from smallest set with cumulative probability > p\n",
    "- p=0.9 is common\n",
    "- Adaptive vocabulary size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "aWJQWV-m1ihF"
   },
   "source": [
    "def generate_character_dialogue(character, scene, seed='', temperature=0.8,\n",
    "                                max_length=30, method='sampling'):\n",
    "    \"\"\"\n",
    "    Generate dialogue for specific character in specific scene\n",
    "\n",
    "    Args:\n",
    "        character: Character name\n",
    "        scene: Scene type\n",
    "        seed: Optional seed text\n",
    "        temperature: Sampling temperature\n",
    "        max_length: Maximum tokens\n",
    "        method: 'sampling' or 'greedy'\n",
    "\n",
    "    Returns:\n",
    "        Generated dialogue text\n",
    "    \"\"\"\n",
    "    # Encode seed if provided\n",
    "    if seed:\n",
    "        seed_tokens = tokenizer.texts_to_sequences([seed.lower()])[0]\n",
    "        start_tensor = tf.constant([seed_tokens])\n",
    "    else:\n",
    "        start_tensor = tf.ones((1, 1), dtype=tf.int32)\n",
    "\n",
    "    # Get conditions\n",
    "    char_idx = char_to_idx.get(character, 0)\n",
    "    scene_idx = scene_to_idx.get(scene, 0)\n",
    "\n",
    "    # Generate\n",
    "    generated = generator.generate_sequence(\n",
    "        start_tensor,\n",
    "        max_length=max_length,\n",
    "        temperature=temperature,\n",
    "        char_condition=tf.constant([char_idx]),\n",
    "        scene_condition=tf.constant([scene_idx]),\n",
    "        method=method\n",
    "    )\n",
    "\n",
    "    # Decode\n",
    "    tokens = generated[0].numpy()\n",
    "    text = ' '.join([tokenizer.index_word.get(int(t), '')\n",
    "                    for t in tokens if t > 0])\n",
    "\n",
    "    return text.strip()\n",
    "\n",
    "# Demonstrate character-specific generation\n",
    "print('='*80)\n",
    "print('CHARACTER-CONDITIONED GENERATION')\n",
    "print('='*80)\n",
    "\n",
    "characters_to_test = list(char_to_idx.keys())[:4]  # Test first 4 characters\n",
    "scene = list(scene_to_idx.keys())[0]  # Use first scene\n",
    "\n",
    "for character in characters_to_test:\n",
    "    print(f'\\n{character} (Scene: {scene}):')\n",
    "    for i in range(3):\n",
    "        dialogue = generate_character_dialogue(\n",
    "            character, scene, temperature=0.8\n",
    "        )\n",
    "        print(f'  {i+1}. \"{dialogue}\"')\n",
    "\n",
    "print('\\n' + '='*80)"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pmwyZD_G1ihL"
   },
   "source": [
    "# Demonstrate scene-conditioned generation\n",
    "print('='*80)\n",
    "print('SCENE-CONDITIONED GENERATION')\n",
    "print('='*80)\n",
    "\n",
    "character = list(char_to_idx.keys())[0]  # Use first character\n",
    "scenes_to_test = list(scene_to_idx.keys())[:4]  # Test first 4 scenes\n",
    "\n",
    "for scene in scenes_to_test:\n",
    "    print(f'\\nScene: {scene} (Character: {character}):')\n",
    "    for i in range(2):\n",
    "        dialogue = generate_character_dialogue(\n",
    "            character, scene, temperature=0.8\n",
    "        )\n",
    "        print(f'  {i+1}. \"{dialogue}\"')\n",
    "\n",
    "print('\\n' + '='*80)"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6KBza1ib1ihM"
   },
   "source": [
    "# Length-controlled generation\n",
    "print('='*80)\n",
    "print('LENGTH-CONTROLLED GENERATION')\n",
    "print('='*80)\n",
    "\n",
    "character = list(char_to_idx.keys())[0]\n",
    "scene = list(scene_to_idx.keys())[0]\n",
    "\n",
    "for length in [10, 20, 30, 40]:\n",
    "    dialogue = generate_character_dialogue(\n",
    "        character, scene, temperature=0.8, max_length=length\n",
    "    )\n",
    "    print(f'\\nMax Length {length} tokens:')\n",
    "    print(f'  \"{dialogue}\"')\n",
    "    print(f'  (Actual length: {len(dialogue.split())} words)')\n",
    "\n",
    "print('\\n' + '='*80)"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OIaPkoI71ihM"
   },
   "source": [
    "## Section 8.0: Visualizations\n",
    "\n",
    "Visualize training dynamics and model behavior:\n",
    "\n",
    "### 1. Training Loss Curves\n",
    "- Generator loss over epochs\n",
    "- Discriminator loss over epochs\n",
    "- Shows training stability and convergence\n",
    "\n",
    "### 2. Discriminator Accuracy\n",
    "- Track D's ability to distinguish real vs fake\n",
    "- Target: 50-70% (balanced)\n",
    "- Too high/low indicates training issues\n",
    "\n",
    "### 3. Generator Reward\n",
    "- Average discriminator score for generated samples\n",
    "- Should increase over training\n",
    "- Indicates improving generation quality\n",
    "\n",
    "### 4. t-SNE Embedding Visualization\n",
    "- Project real and generated sequences to 2D\n",
    "- Visualize distribution overlap\n",
    "- Good GAN: distributions overlap\n",
    "\n",
    "### 5. Word Clouds\n",
    "- Real data word frequency\n",
    "- Generated data word frequency\n",
    "- Compare vocabulary usage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JVaT6NF51ihM"
   },
   "source": [
    "# 1. Training Loss Curves\n",
    "fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "\n",
    "# Generator Loss\n",
    "if history['g_loss']:\n",
    "    axes[0, 0].plot(history['epoch'], history['g_loss'], 'b-', linewidth=2)\n",
    "    axes[0, 0].set_title('Generator Loss', fontsize=14, fontweight='bold')\n",
    "    axes[0, 0].set_xlabel('Epoch')\n",
    "    axes[0, 0].set_ylabel('Loss')\n",
    "    axes[0, 0].grid(True, alpha=0.3)\n",
    "\n",
    "# Discriminator Loss\n",
    "if history['d_loss']:\n",
    "    axes[0, 1].plot(history['epoch'], history['d_loss'], 'r-', linewidth=2)\n",
    "    axes[0, 1].set_title('Discriminator Loss', fontsize=14, fontweight='bold')\n",
    "    axes[0, 1].set_xlabel('Epoch')\n",
    "    axes[0, 1].set_ylabel('Loss')\n",
    "    axes[0, 1].grid(True, alpha=0.3)\n",
    "\n",
    "# Discriminator Accuracy\n",
    "if history['d_acc']:\n",
    "    axes[1, 0].plot(history['epoch'], history['d_acc'], 'g-', linewidth=2)\n",
    "    axes[1, 0].axhline(y=0.5, color='k', linestyle='--', alpha=0.5, label='Random (0.5)')\n",
    "    axes[1, 0].axhline(y=0.7, color='orange', linestyle='--', alpha=0.5, label='Target Max (0.7)')\n",
    "    axes[1, 0].set_title('Discriminator Accuracy', fontsize=14, fontweight='bold')\n",
    "    axes[1, 0].set_xlabel('Epoch')\n",
    "    axes[1, 0].set_ylabel('Accuracy')\n",
    "    axes[1, 0].legend()\n",
    "    axes[1, 0].grid(True, alpha=0.3)\n",
    "    axes[1, 0].set_ylim([0, 1])\n",
    "\n",
    "# Generator Reward (Discriminator score)\n",
    "if history['g_reward']:\n",
    "    axes[1, 1].plot(history['epoch'], history['g_reward'], 'm-', linewidth=2)\n",
    "    axes[1, 1].set_title('Generator Reward (Avg. D Score)', fontsize=14, fontweight='bold')\n",
    "    axes[1, 1].set_xlabel('Epoch')\n",
    "    axes[1, 1].set_ylabel('Reward')\n",
    "    axes[1, 1].grid(True, alpha=0.3)\n",
    "    axes[1, 1].set_ylim([0, 1])\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('training_curves.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print('Training curves plotted and saved to training_curves.png')"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "sawUr53H1ihM"
   },
   "source": [
    "# 2. t-SNE Visualization of Real vs Generated Embeddings\n",
    "print('Generating t-SNE visualization...')\n",
    "\n",
    "# Get real samples\n",
    "num_samples = min(200, len(X_val))\n",
    "real_samples = X_val[:num_samples]\n",
    "\n",
    "# Generate fake samples\n",
    "fake_samples = []\n",
    "for i in range(num_samples):\n",
    "    char_idx = np.random.randint(0, num_characters)\n",
    "    scene_idx = np.random.randint(0, num_scenes)\n",
    "\n",
    "    start = tf.ones((1, 1), dtype=tf.int32)\n",
    "    generated = generator.generate_sequence(\n",
    "        start, max_length=SEQ_LENGTH, temperature=0.8,\n",
    "        char_condition=tf.constant([char_idx]),\n",
    "        scene_condition=tf.constant([scene_idx])\n",
    "    )\n",
    "\n",
    "    # Pad to SEQ_LENGTH\n",
    "    gen_padded = pad_sequences(\n",
    "        generated.numpy(),\n",
    "        maxlen=SEQ_LENGTH,\n",
    "        padding='pre'\n",
    "    )\n",
    "    fake_samples.append(gen_padded[0])\n",
    "\n",
    "fake_samples = np.array(fake_samples)\n",
    "\n",
    "# Extract features using discriminator\n",
    "real_features = discriminator.get_features(real_samples).numpy()\n",
    "fake_features = discriminator.get_features(fake_samples).numpy()\n",
    "\n",
    "# Combine for t-SNE\n",
    "all_features = np.vstack([real_features, fake_features])\n",
    "labels = ['Real'] * len(real_features) + ['Generated'] * len(fake_features)\n",
    "\n",
    "# Apply t-SNE\n",
    "print('Applying t-SNE (this may take a moment)...')\n",
    "tsne = TSNE(n_components=2, random_state=42, perplexity=30)\n",
    "embedded = tsne.fit_transform(all_features)\n",
    "\n",
    "# Plot\n",
    "plt.figure(figsize=(12, 8))\n",
    "real_embedded = embedded[:len(real_features)]\n",
    "fake_embedded = embedded[len(real_features):]\n",
    "\n",
    "plt.scatter(real_embedded[:, 0], real_embedded[:, 1],\n",
    "           c='blue', alpha=0.6, s=50, label='Real Dialogue', edgecolors='k', linewidth=0.5)\n",
    "plt.scatter(fake_embedded[:, 0], fake_embedded[:, 1],\n",
    "           c='red', alpha=0.6, s=50, label='Generated Dialogue', edgecolors='k', linewidth=0.5)\n",
    "\n",
    "plt.title('t-SNE: Real vs Generated Dialogue Embeddings', fontsize=16, fontweight='bold')\n",
    "plt.xlabel('t-SNE Dimension 1', fontsize=12)\n",
    "plt.ylabel('t-SNE Dimension 2', fontsize=12)\n",
    "plt.legend(fontsize=12)\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.savefig('tsne_visualization.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print('t-SNE visualization saved to tsne_visualization.png')"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "IUh3UNIU1ihN"
   },
   "source": [
    "# 3. Word Clouds for Real vs Generated Text\n",
    "print('Creating word clouds...')\n",
    "\n",
    "# Real text\n",
    "real_text = ' '.join([\n",
    "    ' '.join([tokenizer.index_word.get(int(t), '') for t in seq if t > 0])\n",
    "    for seq in X_val[:500]\n",
    "])\n",
    "\n",
    "# Generated text\n",
    "generated_texts = []\n",
    "for i in range(100):\n",
    "    char_idx = np.random.randint(0, num_characters)\n",
    "    scene_idx = np.random.randint(0, num_scenes)\n",
    "    text = generate_character_dialogue(\n",
    "        list(char_to_idx.keys())[char_idx],\n",
    "        list(scene_to_idx.keys())[scene_idx],\n",
    "        temperature=0.8\n",
    "    )\n",
    "    generated_texts.append(text)\n",
    "\n",
    "generated_text = ' '.join(generated_texts)\n",
    "\n",
    "# Create word clouds\n",
    "fig, axes = plt.subplots(1, 2, figsize=(18, 6))\n",
    "\n",
    "# Real data word cloud\n",
    "wordcloud_real = WordCloud(\n",
    "    width=800, height=400,\n",
    "    background_color='white',\n",
    "    colormap='Blues',\n",
    "    max_words=100\n",
    ").generate(real_text)\n",
    "\n",
    "axes[0].imshow(wordcloud_real, interpolation='bilinear')\n",
    "axes[0].set_title('Real Dialogue Word Cloud', fontsize=14, fontweight='bold')\n",
    "axes[0].axis('off')\n",
    "\n",
    "# Generated data word cloud\n",
    "wordcloud_fake = WordCloud(\n",
    "    width=800, height=400,\n",
    "    background_color='white',\n",
    "    colormap='Reds',\n",
    "    max_words=100\n",
    ").generate(generated_text)\n",
    "\n",
    "axes[1].imshow(wordcloud_fake, interpolation='bilinear')\n",
    "axes[1].set_title('Generated Dialogue Word Cloud', fontsize=14, fontweight='bold')\n",
    "axes[1].axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('wordclouds.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print('Word clouds saved to wordclouds.png')"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "o7S_6cDF1ihN"
   },
   "source": [
    "# 4. Combined Metrics Summary Plot\n",
    "fig = plt.figure(figsize=(16, 10))\n",
    "gs = fig.add_gridspec(3, 2, hspace=0.3, wspace=0.3)\n",
    "\n",
    "# Generator and Discriminator Losses\n",
    "ax1 = fig.add_subplot(gs[0, :])\n",
    "if history['g_loss'] and history['d_loss']:\n",
    "    ax1.plot(history['epoch'], history['g_loss'], 'b-', linewidth=2, label='Generator Loss', marker='o')\n",
    "    ax1_twin = ax1.twinx()\n",
    "    ax1_twin.plot(history['epoch'], history['d_loss'], 'r-', linewidth=2, label='Discriminator Loss', marker='s')\n",
    "    ax1.set_xlabel('Epoch', fontsize=12)\n",
    "    ax1.set_ylabel('Generator Loss', fontsize=12, color='b')\n",
    "    ax1_twin.set_ylabel('Discriminator Loss', fontsize=12, color='r')\n",
    "    ax1.tick_params(axis='y', labelcolor='b')\n",
    "    ax1_twin.tick_params(axis='y', labelcolor='r')\n",
    "    ax1.set_title('Training Losses Over Time', fontsize=14, fontweight='bold')\n",
    "    ax1.grid(True, alpha=0.3)\n",
    "    ax1.legend(loc='upper left')\n",
    "    ax1_twin.legend(loc='upper right')\n",
    "\n",
    "# Discriminator Accuracy with zones\n",
    "ax2 = fig.add_subplot(gs[1, 0])\n",
    "if history['d_acc']:\n",
    "    ax2.plot(history['epoch'], history['d_acc'], 'g-', linewidth=2, marker='o')\n",
    "    ax2.axhspan(0.5, 0.7, alpha=0.2, color='green', label='Target Zone')\n",
    "    ax2.axhline(y=0.5, color='k', linestyle='--', alpha=0.5)\n",
    "    ax2.set_xlabel('Epoch', fontsize=12)\n",
    "    ax2.set_ylabel('Accuracy', fontsize=12)\n",
    "    ax2.set_title('Discriminator Accuracy', fontsize=14, fontweight='bold')\n",
    "    ax2.set_ylim([0, 1])\n",
    "    ax2.grid(True, alpha=0.3)\n",
    "    ax2.legend()\n",
    "\n",
    "# Generator Reward\n",
    "ax3 = fig.add_subplot(gs[1, 1])\n",
    "if history['g_reward']:\n",
    "    ax3.plot(history['epoch'], history['g_reward'], 'm-', linewidth=2, marker='o')\n",
    "    ax3.set_xlabel('Epoch', fontsize=12)\n",
    "    ax3.set_ylabel('Average Reward', fontsize=12)\n",
    "    ax3.set_title('Generator Quality (Discriminator Score)', fontsize=14, fontweight='bold')\n",
    "    ax3.set_ylim([0, 1])\n",
    "    ax3.grid(True, alpha=0.3)\n",
    "\n",
    "# Metrics Summary Table\n",
    "ax4 = fig.add_subplot(gs[2, :])\n",
    "ax4.axis('off')\n",
    "\n",
    "metrics_data = [\n",
    "    ['Metric', 'Value', 'Status'],\n",
    "    ['Validation Perplexity', f'{val_perplexity:.2f}', '\u2713' if val_perplexity < 100 else '\u26a0'],\n",
    "    ['BLEU Score', f'{avg_bleu:.4f}', '\u2713' if avg_bleu > 0.1 else '\u26a0'],\n",
    "    ['Self-BLEU (Diversity)', f'{self_bleu:.4f}', '\u2713' if self_bleu < 0.8 else '\u26a0'],\n",
    "]\n",
    "\n",
    "if history['d_acc']:\n",
    "    metrics_data.append(['Final D Accuracy', f'{final_d_acc:.4f}',\n",
    "                        '\u2713' if 0.5 <= final_d_acc <= 0.7 else '\u26a0'])\n",
    "\n",
    "table = ax4.table(cellText=metrics_data, cellLoc='center', loc='center',\n",
    "                 colWidths=[0.3, 0.2, 0.1])\n",
    "table.auto_set_font_size(False)\n",
    "table.set_fontsize(11)\n",
    "table.scale(1, 2)\n",
    "\n",
    "# Style header row\n",
    "for i in range(3):\n",
    "    table[(0, i)].set_facecolor('#4CAF50')\n",
    "    table[(0, i)].set_text_props(weight='bold', color='white')\n",
    "\n",
    "ax4.set_title('Evaluation Metrics Summary', fontsize=14, fontweight='bold', pad=20)\n",
    "\n",
    "plt.savefig('comprehensive_metrics.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print('Comprehensive metrics visualization saved to comprehensive_metrics.png')"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DwGcmA3J1ihN"
   },
   "source": [
    "## Section 9.0: Model Persistence\n",
    "\n",
    "Save and load trained models for future use.\n",
    "\n",
    "### What to Save:\n",
    "\n",
    "1. **Generator Model**: Architecture + weights\n",
    "2. **Discriminator Model**: Architecture + weights\n",
    "3. **Tokenizer**: Vocabulary and token mappings\n",
    "4. **Metadata**: Character/scene mappings, hyperparameters\n",
    "5. **Training History**: Losses, accuracies, metrics\n",
    "\n",
    "### Storage Location:\n",
    "\n",
    "Models saved to: `/content/drive/MyDrive/DLA_Notebooks_Data_PGPM/`\n",
    "\n",
    "### File Structure:\n",
    "```\n",
    "DLA_Notebooks_Data_PGPM/\n",
    "\u251c\u2500\u2500 generator/\n",
    "\u2502   \u251c\u2500\u2500 saved_model.pb\n",
    "\u2502   \u2514\u2500\u2500 variables/\n",
    "\u251c\u2500\u2500 discriminator/\n",
    "\u2502   \u251c\u2500\u2500 saved_model.pb\n",
    "\u2502   \u2514\u2500\u2500 variables/\n",
    "\u251c\u2500\u2500 tokenizer.pickle\n",
    "\u251c\u2500\u2500 metadata.json\n",
    "\u2514\u2500\u2500 training_history.json\n",
    "```\n",
    "\n",
    "### Loading Models:\n",
    "\n",
    "```python\n",
    "# Load generator\n",
    "generator = keras.models.load_model('path/to/generator')\n",
    "\n",
    "# Load tokenizer\n",
    "with open('path/to/tokenizer.pickle', 'rb') as f:\n",
    "    tokenizer = pickle.load(f)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qFRr-RE01ihN"
   },
   "source": [
    "# Create save directory\n",
    "SAVE_DIR = '/content/drive/MyDrive/DLA_Notebooks_Data_PGPM/'\n",
    "\n",
    "# Try to create directory (will fail if not in Colab with Drive mounted)\n",
    "try:\n",
    "    import os\n",
    "    os.makedirs(SAVE_DIR, exist_ok=True)\n",
    "    print(f'Save directory: {SAVE_DIR}')\n",
    "except:\n",
    "    # Fallback to local directory\n",
    "    SAVE_DIR = './models/'\n",
    "    os.makedirs(SAVE_DIR, exist_ok=True)\n",
    "    print(f'Using local save directory: {SAVE_DIR}')\n",
    "    print('(Mount Google Drive in Colab to use persistent storage)')\n"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bcRaYUPt1ihN"
   },
   "source": [
    "def save_models(save_dir=SAVE_DIR):\n",
    "    \"\"\"Save all models and associated data\"\"\"\n",
    "    print('='*80)\n",
    "    print('SAVING MODELS AND DATA')\n",
    "    print('='*80)\n",
    "\n",
    "    # Save generator\n",
    "    gen_path = os.path.join(save_dir, 'generator')\n",
    "    generator.save(gen_path)\n",
    "    print(f'\u2713 Generator saved to: {gen_path}')\n",
    "\n",
    "    # Save discriminator\n",
    "    disc_path = os.path.join(save_dir, 'discriminator')\n",
    "    discriminator.save(disc_path)\n",
    "    print(f'\u2713 Discriminator saved to: {disc_path}')\n",
    "\n",
    "    # Save tokenizer\n",
    "    tokenizer_path = os.path.join(save_dir, 'tokenizer.pickle')\n",
    "    with open(tokenizer_path, 'wb') as f:\n",
    "        pickle.dump(tokenizer, f)\n",
    "    print(f'\u2713 Tokenizer saved to: {tokenizer_path}')\n",
    "\n",
    "    # Save metadata\n",
    "    metadata = {\n",
    "        'vocab_size': data_processor.vocab_size,\n",
    "        'seq_length': SEQ_LENGTH,\n",
    "        'char_to_idx': char_to_idx,\n",
    "        'idx_to_char': idx_to_char,\n",
    "        'scene_to_idx': scene_to_idx,\n",
    "        'idx_to_scene': idx_to_scene,\n",
    "        'num_characters': num_characters,\n",
    "        'num_scenes': num_scenes,\n",
    "        'hyperparameters': {\n",
    "            'embedding_dim': 256,\n",
    "            'lstm_units': 512,\n",
    "            'num_lstm_layers': 2,\n",
    "            'batch_size': BATCH_SIZE,\n",
    "            'learning_rate_g': LEARNING_RATE_G,\n",
    "            'learning_rate_d': LEARNING_RATE_D,\n",
    "        }\n",
    "    }\n",
    "\n",
    "    metadata_path = os.path.join(save_dir, 'metadata.json')\n",
    "    with open(metadata_path, 'w') as f:\n",
    "        json.dump(metadata, f, indent=2)\n",
    "    print(f'\u2713 Metadata saved to: {metadata_path}')\n",
    "\n",
    "    # Save training history\n",
    "    history_path = os.path.join(save_dir, 'training_history.json')\n",
    "    with open(history_path, 'w') as f:\n",
    "        # Convert numpy types to Python types\n",
    "        history_serializable = {\n",
    "            k: [float(v) for v in vals] if isinstance(vals, list) else vals\n",
    "            for k, vals in history.items()\n",
    "        }\n",
    "        json.dump(history_serializable, f, indent=2)\n",
    "    print(f'\u2713 Training history saved to: {history_path}')\n",
    "\n",
    "    print('='*80)\n",
    "    print('ALL MODELS AND DATA SAVED SUCCESSFULLY!')\n",
    "    print('='*80)\n",
    "\n",
    "def load_models(save_dir=SAVE_DIR):\n",
    "    \"\"\"Load all models and associated data\"\"\"\n",
    "    print('='*80)\n",
    "    print('LOADING MODELS AND DATA')\n",
    "    print('='*80)\n",
    "\n",
    "    # Load generator\n",
    "    gen_path = os.path.join(save_dir, 'generator')\n",
    "    loaded_generator = keras.models.load_model(gen_path)\n",
    "    print(f'\u2713 Generator loaded from: {gen_path}')\n",
    "\n",
    "    # Load discriminator\n",
    "    disc_path = os.path.join(save_dir, 'discriminator')\n",
    "    loaded_discriminator = keras.models.load_model(disc_path)\n",
    "    print(f'\u2713 Discriminator loaded from: {disc_path}')\n",
    "\n",
    "    # Load tokenizer\n",
    "    tokenizer_path = os.path.join(save_dir, 'tokenizer.pickle')\n",
    "    with open(tokenizer_path, 'rb') as f:\n",
    "        loaded_tokenizer = pickle.load(f)\n",
    "    print(f'\u2713 Tokenizer loaded from: {tokenizer_path}')\n",
    "\n",
    "    # Load metadata\n",
    "    metadata_path = os.path.join(save_dir, 'metadata.json')\n",
    "    with open(metadata_path, 'r') as f:\n",
    "        loaded_metadata = json.load(f)\n",
    "    print(f'\u2713 Metadata loaded from: {metadata_path}')\n",
    "\n",
    "    # Load training history\n",
    "    history_path = os.path.join(save_dir, 'training_history.json')\n",
    "    with open(history_path, 'r') as f:\n",
    "        loaded_history = json.load(f)\n",
    "    print(f'\u2713 Training history loaded from: {history_path}')\n",
    "\n",
    "    print('='*80)\n",
    "    print('ALL MODELS AND DATA LOADED SUCCESSFULLY!')\n",
    "    print('='*80)\n",
    "\n",
    "    return loaded_generator, loaded_discriminator, loaded_tokenizer, loaded_metadata, loaded_history\n",
    "\n",
    "print('Save/Load functions defined')"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rsIw-D701ihO"
   },
   "source": [
    "# Save models\n",
    "save_models(SAVE_DIR)\n",
    "\n",
    "print('\\nModels saved! You can now load them in a new session using:')\n",
    "print('generator, discriminator, tokenizer, metadata, history = load_models(SAVE_DIR)')"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7t5ONogh1ihO"
   },
   "source": [
    "## Section 10.0: Future Work and API Deployment\n",
    "\n",
    "### Potential Improvements:\n",
    "\n",
    "**1. Model Enhancements**:\n",
    "- Replace LSTM with Transformer architecture\n",
    "- Implement LeakGAN Manager-Worker framework\n",
    "- Add attention mechanisms\n",
    "- Multi-modal conditioning (emotions, actions)\n",
    "- Hierarchical generation (sentence \u2192 paragraph)\n",
    "\n",
    "**2. Training Improvements**:\n",
    "- Implement proper Monte Carlo rollouts\n",
    "- Add curriculum learning\n",
    "- Use WGAN-GP (Wasserstein GAN with Gradient Penalty)\n",
    "- Spectral normalization for stability\n",
    "- Progressive training (increase sequence length gradually)\n",
    "\n",
    "**3. Evaluation Enhancements**:\n",
    "- Human evaluation interface\n",
    "- Automatic quality metrics (METEOR, ROUGE, BERTScore)\n",
    "- Character voice consistency scoring\n",
    "- Dialogue flow analysis\n",
    "\n",
    "**4. Data Augmentation**:\n",
    "- Back-translation\n",
    "- Paraphrasing\n",
    "- Character style transfer\n",
    "- Multi-season data integration\n",
    "\n",
    "### API Deployment:\n",
    "\n",
    "#### Flask Example:\n",
    "\n",
    "```python\n",
    "from flask import Flask, request, jsonify\n",
    "import tensorflow as tf\n",
    "\n",
    "app = Flask(__name__)\n",
    "\n",
    "# Load models\n",
    "generator = tf.keras.models.load_model('path/to/generator')\n",
    "tokenizer = pickle.load(open('path/to/tokenizer.pickle', 'rb'))\n",
    "\n",
    "@app.route('/generate', methods=['POST'])\n",
    "def generate_dialogue():\n",
    "    data = request.json\n",
    "    seed = data.get('seed', '')\n",
    "    character = data.get('character', 'Eren')\n",
    "    scene = data.get('scene', 'Battle')\n",
    "    temperature = data.get('temperature', 0.8)\n",
    "    \n",
    "    # Generate dialogue\n",
    "    result = autocomplete_dialogue(\n",
    "        seed, character, scene,\n",
    "        temperature=temperature\n",
    "    )\n",
    "    \n",
    "    return jsonify({\n",
    "        'dialogue': result[0][0],\n",
    "        'quality_score': result[0][1]\n",
    "    })\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    app.run(host='0.0.0.0', port=5000)\n",
    "```\n",
    "\n",
    "#### FastAPI Example:\n",
    "\n",
    "```python\n",
    "from fastapi import FastAPI\n",
    "from pydantic import BaseModel\n",
    "import tensorflow as tf\n",
    "\n",
    "app = FastAPI()\n",
    "\n",
    "class GenerateRequest(BaseModel):\n",
    "    seed: str = ''\n",
    "    character: str = 'Eren'\n",
    "    scene: str = 'Battle'\n",
    "    temperature: float = 0.8\n",
    "    num_completions: int = 3\n",
    "\n",
    "@app.post('/generate')\n",
    "async def generate_dialogue(req: GenerateRequest):\n",
    "    completions = autocomplete_dialogue(\n",
    "        req.seed, req.character, req.scene,\n",
    "        num_completions=req.num_completions,\n",
    "        temperature=req.temperature\n",
    "    )\n",
    "    \n",
    "    return {\n",
    "        'completions': [\n",
    "            {'text': text, 'quality': score}\n",
    "            for text, score in completions\n",
    "        ]\n",
    "    }\n",
    "```\n",
    "\n",
    "### Deployment Considerations:\n",
    "\n",
    "1. **Model Serving**: TensorFlow Serving, TorchServe\n",
    "2. **Containerization**: Docker for consistent deployment\n",
    "3. **Scaling**: Kubernetes for auto-scaling\n",
    "4. **Monitoring**: Track latency, throughput, errors\n",
    "5. **Caching**: Cache common requests\n",
    "6. **Rate Limiting**: Prevent abuse\n",
    "7. **Authentication**: API keys or OAuth\n",
    "\n",
    "### Production Checklist:\n",
    "\n",
    "- [ ] Model optimization (quantization, pruning)\n",
    "- [ ] Batch inference for efficiency\n",
    "- [ ] Error handling and fallbacks\n",
    "- [ ] Logging and monitoring\n",
    "- [ ] Load testing\n",
    "- [ ] Security audit\n",
    "- [ ] Documentation (API docs, examples)\n",
    "- [ ] CI/CD pipeline\n",
    "\n",
    "### Usage Examples:\n",
    "\n",
    "```bash\n",
    "# cURL example\n",
    "curl -X POST http://localhost:5000/generate \\\n",
    "  -H \"Content-Type: application/json\" \\\n",
    "  -d '{\n",
    "    \"seed\": \"I will destroy\",\n",
    "    \"character\": \"Eren\",\n",
    "    \"scene\": \"Battle\",\n",
    "    \"temperature\": 0.8\n",
    "  }'\n",
    "```\n",
    "\n",
    "```python\n",
    "# Python client example\n",
    "import requests\n",
    "\n",
    "response = requests.post(\n",
    "    'http://localhost:5000/generate',\n",
    "    json={\n",
    "        'seed': 'We need to',\n",
    "        'character': 'Armin',\n",
    "        'scene': 'Strategy Meeting',\n",
    "        'temperature': 0.7\n",
    "    }\n",
    ")\n",
    "\n",
    "result = response.json()\n",
    "print(result['dialogue'])\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Rk1e3__h1ihO"
   },
   "source": [
    "# Final summary\n",
    "print('='*80)\n",
    "print('NOTEBOOK COMPLETE!')\n",
    "print('='*80)\n",
    "print('\\nThis notebook implemented:')\n",
    "print('\u2713 SeqGAN/LeakGAN concepts and theory')\n",
    "print('\u2713 Data preparation and preprocessing')\n",
    "print('\u2713 Generator (LSTM-based) architecture')\n",
    "print('\u2713 Discriminator (CNN-based) architecture')\n",
    "print('\u2713 Policy gradient (REINFORCE) implementation')\n",
    "print('\u2713 Complete adversarial training loop')\n",
    "print('\u2713 Interactive autocomplete tool')\n",
    "print('\u2713 Comprehensive evaluation metrics')\n",
    "print('\u2713 Character and scene conditioning')\n",
    "print('\u2713 Advanced visualizations')\n",
    "print('\u2713 Model persistence (save/load)')\n",
    "print('\u2713 API deployment examples')\n",
    "print('\\n' + '='*80)\n",
    "print('Thank you for using this notebook!')\n",
    "print('For questions or issues, contact: support@deeplearningacademy.io')\n",
    "print('='*80)"
   ],
   "outputs": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "colab": {
   "provenance": [],
   "include_colab_link": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}