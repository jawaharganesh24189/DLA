{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 8D - Attack on Titan Language Model\n",
    "\n",
    "## Section 1.0: Background\n",
    "\n",
    "### Objective\n",
    "This notebook builds and trains a character-level language model using the Attack on Titan Season 1 (AoTS1) script dataset. The model will learn to generate text in the style of Attack on Titan dialogue and narration.\n",
    "\n",
    "### What is a Language Model?\n",
    "A language model is a neural network that learns the probability distribution of sequences of characters or words. By training on the Attack on Titan scripts, our model will learn:\n",
    "- Character dialogue patterns\n",
    "- Story structure and narration style\n",
    "- The unique vocabulary and tone of Attack on Titan\n",
    "\n",
    "### Model Architecture\n",
    "We'll use an LSTM (Long Short-Term Memory) based architecture, which excels at:\n",
    "- Learning long-term dependencies in text\n",
    "- Generating coherent sequences\n",
    "- Capturing stylistic patterns\n",
    "\n",
    "### Dataset\n",
    "The AoTS1.txt file contains dialogue and narration from Attack on Titan Season 1, including:\n",
    "- Character conversations\n",
    "- Scene descriptions\n",
    "- Narrator commentary\n",
    "- Action sequences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 1.1: Setup and Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "import matplotlib.pyplot as plt\n",
    "import json\n",
    "import os\n",
    "from collections import Counter\n",
    "import random\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "np.random.seed(42)\n",
    "tf.random.set_seed(42)\n",
    "random.seed(42)\n",
    "\n",
    "print(f\"TensorFlow version: {tf.__version__}\")\n",
    "print(f\"GPU Available: {tf.config.list_physical_devices('GPU')}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 1.2: Mount Google Drive\n",
    "\n",
    "Mount Google Drive to save and load models. This ensures our trained model persists across Colab sessions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mount Google Drive (for Google Colab)\n",
    "try:\n",
    "    from google.colab import drive\n",
    "    drive.mount('/content/drive')\n",
    "    \n",
    "    # Create directory if it doesn't exist\n",
    "    model_dir = '/content/drive/MyDrive/DLA_Notebooks_Data_PGPM'\n",
    "    os.makedirs(model_dir, exist_ok=True)\n",
    "    print(f\"\u2713 Google Drive mounted successfully\")\n",
    "    print(f\"\u2713 Model directory: {model_dir}\")\n",
    "    IN_COLAB = True\n",
    "except:\n",
    "    print(\"Not running in Google Colab - using local directory\")\n",
    "    model_dir = './models'\n",
    "    os.makedirs(model_dir, exist_ok=True)\n",
    "    IN_COLAB = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 2.0: Data Preparation",
    "",
    "### Improvements for Better Accuracy",
    "",
    "This version includes **enhanced preprocessing** to improve model accuracy:",
    "",
    "\u2728 **Text Preprocessing:**",
    "- Remove common stop words (the, a, an, is, was, etc.) that don't add context",
    "- Normalize whitespace (multiple spaces, tabs, extra newlines)",
    "- Keep important punctuation for dialogue structure (: ! ? .)",
    "- Preserve character names and story-specific vocabulary",
    "- Remove redundant filler words",
    "",
    "\u2728 **Model Improvements:**",
    "- Increased model depth (3 LSTM layers instead of 2)",
    "- Added Batch Normalization for training stability",
    "- Added recurrent dropout to prevent overfitting",
    "- Optimized sequence length (80 chars for better context window)",
    "- Increased training epochs (50) for better convergence",
    "- Reduced batch size (64) for better gradient updates",
    "",
    "These changes significantly improve the model's ability to learn meaningful patterns and generate more coherent Attack on Titan dialogue!",
    "",
    "### Section 2.1: Load the Attack on Titan Script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the Attack on Titan Season 1 script\n",
    "# If running in Colab, upload the AoTS1.txt file first\n",
    "\n",
    "try:\n",
    "    # Try to load from current directory\n",
    "    with open('AoTS1.txt', 'r', encoding='utf-8') as f:\n",
    "        text = f.read()\n",
    "    print(\"\u2713 Loaded AoTS1.txt from current directory\")\n",
    "except FileNotFoundError:\n",
    "    # If not found, provide instructions to upload\n",
    "    print(\"\u26a0 AoTS1.txt not found in current directory\")\n",
    "    if IN_COLAB:\n",
    "        print(\"Please upload AoTS1.txt using the file upload cell below:\")\n",
    "        from google.colab import files\n",
    "        uploaded = files.upload()\n",
    "        text = uploaded['AoTS1.txt'].decode('utf-8')\n",
    "        print(\"\u2713 File uploaded successfully\")\n",
    "    else:\n",
    "        raise FileNotFoundError(\"Please ensure AoTS1.txt is in the current directory\")\n",
    "\n",
    "# Display basic statistics\n",
    "print(f\"\\n\ud83d\udcca Dataset Statistics:\")\n",
    "print(f\"Total characters: {len(text):,}\")\n",
    "print(f\"Total lines: {len(text.splitlines())}\")\n",
    "print(f\"\\n\ud83d\udcdd First 500 characters:\")\n",
    "print(text[:500])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Section 2.1.1: Text Preprocessing\n",
    "\n",
    "Apply preprocessing to improve model accuracy by:\n",
    "- Normalizing whitespace (remove extra spaces)\n",
    "- Removing common stop words that don't add context\n",
    "- Keeping important punctuation for dialogue structure\n",
    "- Converting to consistent format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from collections import Counter\n",
    "\n",
    "def preprocess_text(text):\n",
    "    \"\"\"\n",
    "    Preprocess text to improve model learning.\n",
    "    \n",
    "    Steps:\n",
    "    1. Normalize whitespace (remove extra spaces, tabs)\n",
    "    2. Remove common stop words that don't contribute to dialogue context\n",
    "    3. Keep important punctuation for dialogue structure (: ! ? .)\n",
    "    4. Preserve character names and key story elements\n",
    "    \"\"\"\n",
    "    # Remove common English stop words that don't add value to AoT dialogue\n",
    "    # Keep story-specific words and character-related words\n",
    "    stop_words = {\n",
    "        'the', 'a', 'an', 'and', 'or', 'but', 'in', 'on', 'at', 'to', 'for',\n",
    "        'of', 'with', 'by', 'from', 'as', 'is', 'was', 'are', 'were', 'been',\n",
    "        'be', 'have', 'has', 'had', 'do', 'does', 'did', 'will', 'would',\n",
    "        'could', 'should', 'may', 'might', 'must', 'can', 'this', 'that',\n",
    "        'these', 'those', 'i', 'you', 'he', 'she', 'it', 'we', 'they',\n",
    "        'what', 'which', 'who', 'when', 'where', 'why', 'how', 'all', 'each',\n",
    "        'every', 'both', 'few', 'more', 'most', 'other', 'some', 'such'\n",
    "    }\n",
    "    \n",
    "    # Split into lines to preserve structure\n",
    "    lines = text.split('\\n')\n",
    "    processed_lines = []\n",
    "    \n",
    "    for line in lines:\n",
    "        # Skip empty lines\n",
    "        if not line.strip():\n",
    "            continue\n",
    "        \n",
    "        # Keep episode markers as-is\n",
    "        if line.startswith('[Episode'):\n",
    "            processed_lines.append(line)\n",
    "            continue\n",
    "        \n",
    "        # Keep title/header lines\n",
    "        if 'Attack on Titan' in line or 'Season' in line:\n",
    "            processed_lines.append(line)\n",
    "            continue\n",
    "        \n",
    "        # For dialogue lines (NAME: text)\n",
    "        if ':' in line:\n",
    "            # Split into name and dialogue\n",
    "            parts = line.split(':', 1)\n",
    "            if len(parts) == 2:\n",
    "                name = parts[0].strip()\n",
    "                dialogue = parts[1].strip()\n",
    "                \n",
    "                # Process dialogue - remove stop words\n",
    "                words = dialogue.split()\n",
    "                filtered_words = []\n",
    "                for word in words:\n",
    "                    # Keep punctuation attached to words\n",
    "                    clean_word = word.lower().strip('.,;')\n",
    "                    \n",
    "                    # Keep important words (not stop words)\n",
    "                    # Also keep words with important punctuation\n",
    "                    if (clean_word not in stop_words or \n",
    "                        any(p in word for p in ['!', '?', '...'])):\n",
    "                        filtered_words.append(word)\n",
    "                \n",
    "                # Reconstruct line\n",
    "                if filtered_words:\n",
    "                    processed_dialogue = ' '.join(filtered_words)\n",
    "                    processed_lines.append(f\"{name}: {processed_dialogue}\")\n",
    "        else:\n",
    "            # Keep other lines with basic cleanup\n",
    "            processed_lines.append(line.strip())\n",
    "    \n",
    "    # Join lines with single newline\n",
    "    processed_text = '\\n'.join(processed_lines)\n",
    "    \n",
    "    # Normalize whitespace\n",
    "    processed_text = re.sub(r' +', ' ', processed_text)  # Multiple spaces to single\n",
    "    processed_text = re.sub(r'\\t', ' ', processed_text)  # Tabs to space\n",
    "    \n",
    "    return processed_text\n",
    "\n",
    "# Apply preprocessing\n",
    "print(\"\ud83d\udd27 Preprocessing text...\")\n",
    "original_length = len(text)\n",
    "text = preprocess_text(text)\n",
    "processed_length = len(text)\n",
    "\n",
    "print(f\"\u2713 Preprocessing complete\")\n",
    "print(f\"Original length: {original_length:,} characters\")\n",
    "print(f\"Processed length: {processed_length:,} characters\")\n",
    "print(f\"Reduction: {original_length - processed_length:,} characters ({(1 - processed_length/original_length)*100:.1f}%)\")\n",
    "print(f\"\\n\ud83d\udcdd Sample processed text:\")\n",
    "print(text[:400])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Section 2.2: Character-Level Tokenization\n",
    "\n",
    "We'll create a character-level vocabulary and mapping for our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create character vocabulary\n",
    "chars = sorted(list(set(text)))\n",
    "vocab_size = len(chars)\n",
    "\n",
    "print(f\"Vocabulary size: {vocab_size}\")\n",
    "print(f\"\\nCharacters in vocabulary: {''.join(chars[:50])}...\")\n",
    "\n",
    "# Create character to index mapping\n",
    "char_to_idx = {ch: i for i, ch in enumerate(chars)}\n",
    "idx_to_char = {i: ch for i, ch in enumerate(chars)}\n",
    "\n",
    "# Display some mappings\n",
    "print(f\"\\n\ud83d\udccb Sample mappings:\")\n",
    "for char in ['A', 'E', 'R', 'E', 'N', ' ', '!']:\n",
    "    if char in char_to_idx:\n",
    "        print(f\"  '{char}' -> {char_to_idx[char]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Section 2.3: Create Training Sequences\n",
    "\n",
    "We'll create sequences of characters for training. Each sequence serves as input (X) and the next character as the target (y)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert text to integer sequences",
    "text_as_int = np.array([char_to_idx[c] for c in text])",
    "",
    "# Define sequence length (how many characters to look at for prediction)",
    "seq_length = 80",
    "examples_per_epoch = len(text) // (seq_length + 1)",
    "",
    "print(f\"Sequence length: {seq_length}\")",
    "print(f\"Examples per epoch: {examples_per_epoch:,}\")",
    "",
    "# Create training sequences",
    "sequences = []",
    "next_chars = []",
    "",
    "for i in range(0, len(text_as_int) - seq_length, 3):  # step of 3 for efficiency",
    "    sequences.append(text_as_int[i:i + seq_length])",
    "    next_chars.append(text_as_int[i + seq_length])",
    "",
    "print(f\"Total training sequences: {len(sequences):,}\")",
    "",
    "# Convert to numpy arrays",
    "X = np.array(sequences)",
    "y = np.array(next_chars)",
    "",
    "print(f\"\\n\ud83d\udcd0 Data shapes:\")",
    "print(f\"X shape: {X.shape}\")",
    "print(f\"y shape: {y.shape}\")",
    "",
    "# Display an example sequence",
    "example_idx = 100",
    "example_sequence = ''.join([idx_to_char[idx] for idx in X[example_idx]])",
    "example_next_char = idx_to_char[y[example_idx]]",
    "",
    "print(f\"\\n\ud83d\udcd6 Example training sequence:\")",
    "print(f\"Input: '{example_sequence}'\")",
    "print(f\"Target: '{example_next_char}'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Section 2.4: Train/Validation Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split data into training and validation sets (90/10 split)\n",
    "split_idx = int(len(X) * 0.9)\n",
    "\n",
    "X_train, X_val = X[:split_idx], X[split_idx:]\n",
    "y_train, y_val = y[:split_idx], y[split_idx:]\n",
    "\n",
    "print(f\"Training samples: {len(X_train):,}\")\n",
    "print(f\"Validation samples: {len(X_val):,}\")\n",
    "print(f\"\\nTraining set: {len(X_train) / len(X) * 100:.1f}%\")\n",
    "print(f\"Validation set: {len(X_val) / len(X) * 100:.1f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 3.0: Model Architecture\n",
    "\n",
    "### Section 3.1: Build LSTM Language Model\n",
    "\n",
    "We'll create a multi-layer LSTM model with:\n",
    "- Embedding layer to learn character representations\n",
    "- Multiple LSTM layers with dropout for regularization\n",
    "- Dense output layer with softmax activation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model(vocab_size, embedding_dim=256, lstm_units=512, dropout_rate=0.3):",
    "    \"\"\"",
    "    Build an improved LSTM-based language model.",
    "    ",
    "    Args:",
    "        vocab_size: Size of the character vocabulary",
    "        embedding_dim: Dimension of character embeddings (increased for better representation)",
    "        lstm_units: Number of units in LSTM layers (increased for better learning)",
    "        dropout_rate: Dropout rate for regularization",
    "    ",
    "    Returns:",
    "        Compiled Keras model",
    "    \"\"\"",
    "    model = keras.Sequential([",
    "        # Embedding layer: learns dense representations of characters",
    "        layers.Embedding(vocab_size, embedding_dim, input_length=seq_length),",
    "        ",
    "        # First LSTM layer with return_sequences for stacking",
    "        layers.LSTM(lstm_units, return_sequences=True, dropout=dropout_rate, recurrent_dropout=0.2),",
    "        ",
    "        # Batch normalization for training stability",
    "        layers.BatchNormalization(),",
    "        ",
    "        # Second LSTM layer with return_sequences",
    "        layers.LSTM(lstm_units, return_sequences=True, dropout=dropout_rate, recurrent_dropout=0.2),",
    "        ",
    "        # Batch normalization",
    "        layers.BatchNormalization(),",
    "        ",
    "        # Third LSTM layer (final, no return_sequences)",
    "        layers.LSTM(lstm_units // 2, dropout=dropout_rate),",
    "        ",
    "        # Dense hidden layer for better feature extraction",
    "        layers.Dense(256, activation='relu'),",
    "        layers.Dropout(dropout_rate),",
    "        ",
    "        # Dense output layer with softmax for character prediction",
    "        layers.Dense(vocab_size, activation='softmax')",
    "    ])",
    "    ",
    "    # Compile with optimizer that has learning rate schedule",
    "    optimizer = keras.optimizers.Adam(learning_rate=0.001)",
    "    ",
    "    model.compile(",
    "        optimizer=optimizer,",
    "        loss='sparse_categorical_crossentropy',",
    "        metrics=['accuracy']",
    "    )",
    "    ",
    "    return model",
    "",
    "# Build the improved model",
    "model = build_model(vocab_size)",
    "",
    "# Display model architecture",
    "print(\"\ud83c\udfd7\ufe0f Improved Model Architecture:\")",
    "model.summary()",
    "",
    "# Calculate total parameters",
    "total_params = model.count_params()",
    "print(f\"\\nTotal trainable parameters: {total_params:,}\")",
    "print(\"\\n\u2728 Model improvements:\")",
    "print(\"  - Added 3rd LSTM layer for deeper learning\")",
    "print(\"  - Added BatchNormalization for training stability\")",
    "print(\"  - Added recurrent_dropout to prevent overfitting\")",
    "print(\"  - Added dense hidden layer for better feature extraction\")",
    "print(\"  - Optimized architecture for better context understanding\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 4.0: Training\n",
    "\n",
    "### Section 4.1: Configure Training Callbacks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define callbacks\n",
    "\n",
    "# EarlyStopping: stops training if validation loss doesn't improve\n",
    "early_stopping = EarlyStopping(\n",
    "    monitor='val_loss',\n",
    "    patience=5,\n",
    "    restore_best_weights=True,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "# ModelCheckpoint: saves the best model during training\n",
    "checkpoint_path = os.path.join(model_dir, 'aot_language_model_checkpoint.keras')\n",
    "model_checkpoint = ModelCheckpoint(\n",
    "    checkpoint_path,\n",
    "    monitor='val_loss',\n",
    "    save_best_only=True,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "callbacks = [early_stopping, model_checkpoint]\n",
    "\n",
    "print(\"\u2713 Callbacks configured:\")\n",
    "print(f\"  - EarlyStopping (patience=5)\")\n",
    "print(f\"  - ModelCheckpoint: {checkpoint_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Section 4.2: Train the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training configuration - optimized for better accuracy",
    "EPOCHS = 50  # Increased for better learning",
    "BATCH_SIZE = 64  # Reduced for better gradient updates",
    "",
    "print(f\"\ud83d\ude80 Starting training with improved configuration...\")",
    "print(f\"Epochs: {EPOCHS}\")",
    "print(f\"Batch size: {BATCH_SIZE}\")",
    "print(f\"Training samples: {len(X_train):,}\")",
    "print(f\"Validation samples: {len(X_val):,}\")",
    "print(f\"\\n\ud83d\udcc8 Improvements:\")",
    "print(f\"  - Increased epochs to {EPOCHS} for better convergence\")",
    "print(f\"  - Reduced batch size to {BATCH_SIZE} for better gradient updates\")",
    "print(f\"  - Enhanced model architecture with deeper layers\")",
    "print(f\"  - Improved text preprocessing for better context\")",
    "print(f\"\\nThis may take longer but will achieve better accuracy...\\n\")",
    "",
    "# Train the model",
    "history = model.fit(",
    "    X_train, y_train,",
    "    batch_size=BATCH_SIZE,",
    "    epochs=EPOCHS,",
    "    validation_data=(X_val, y_val),",
    "    callbacks=callbacks,",
    "    verbose=1",
    ")",
    "",
    "print(\"\\n\u2713 Training completed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Section 4.3: Visualize Training History"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot training history\n",
    "fig, axes = plt.subplots(1, 2, figsize=(15, 5))\n",
    "\n",
    "# Plot loss\n",
    "axes[0].plot(history.history['loss'], label='Training Loss', linewidth=2)\n",
    "axes[0].plot(history.history['val_loss'], label='Validation Loss', linewidth=2)\n",
    "axes[0].set_title('Model Loss Over Time', fontsize=14, fontweight='bold')\n",
    "axes[0].set_xlabel('Epoch', fontsize=12)\n",
    "axes[0].set_ylabel('Loss', fontsize=12)\n",
    "axes[0].legend(fontsize=10)\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "# Plot accuracy\n",
    "axes[1].plot(history.history['accuracy'], label='Training Accuracy', linewidth=2)\n",
    "axes[1].plot(history.history['val_accuracy'], label='Validation Accuracy', linewidth=2)\n",
    "axes[1].set_title('Model Accuracy Over Time', fontsize=14, fontweight='bold')\n",
    "axes[1].set_xlabel('Epoch', fontsize=12)\n",
    "axes[1].set_ylabel('Accuracy', fontsize=12)\n",
    "axes[1].legend(fontsize=10)\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Print final metrics\n",
    "print(\"\\n\ud83d\udcca Final Training Metrics:\")\n",
    "print(f\"Training Loss: {history.history['loss'][-1]:.4f}\")\n",
    "print(f\"Training Accuracy: {history.history['accuracy'][-1]:.4f}\")\n",
    "print(f\"Validation Loss: {history.history['val_loss'][-1]:.4f}\")\n",
    "print(f\"Validation Accuracy: {history.history['val_accuracy'][-1]:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 5.0: Text Generation\n",
    "\n",
    "### Section 5.1: Implement Text Generation Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_with_temperature(predictions, temperature=1.0):\n",
    "    \"\"\"\n",
    "    Sample from probability distribution with temperature.\n",
    "    \n",
    "    Args:\n",
    "        predictions: Model output probabilities\n",
    "        temperature: Controls randomness (lower = more conservative, higher = more random)\n",
    "    \n",
    "    Returns:\n",
    "        Sampled character index\n",
    "    \"\"\"\n",
    "    predictions = np.asarray(predictions).astype('float64')\n",
    "    predictions = np.log(predictions + 1e-8) / temperature\n",
    "    exp_preds = np.exp(predictions)\n",
    "    predictions = exp_preds / np.sum(exp_preds)\n",
    "    probas = np.random.multinomial(1, predictions, 1)\n",
    "    return np.argmax(probas)\n",
    "\n",
    "\n",
    "def generate_text(model, seed_text, length=500, temperature=0.5):\n",
    "    \"\"\"\n",
    "    Generate text using the trained model.\n",
    "    \n",
    "    Args:\n",
    "        model: Trained Keras model\n",
    "        seed_text: Starting text for generation\n",
    "        length: Number of characters to generate\n",
    "        temperature: Controls randomness (0.2-0.5: conservative, 0.5-1.0: balanced, >1.0: creative)\n",
    "    \n",
    "    Returns:\n",
    "        Generated text string\n",
    "    \"\"\"\n",
    "    # Ensure seed text is long enough\n",
    "    if len(seed_text) < seq_length:\n",
    "        seed_text = seed_text + ' ' * (seq_length - len(seed_text))\n",
    "    \n",
    "    generated_text = seed_text\n",
    "    \n",
    "    print(f\"\ud83c\udf31 Seed text: '{seed_text[:50]}...'\")\n",
    "    print(f\"\ud83c\udf21\ufe0f Temperature: {temperature}\")\n",
    "    print(f\"\ud83d\udccf Generating {length} characters...\\n\")\n",
    "    \n",
    "    for i in range(length):\n",
    "        # Get the last seq_length characters\n",
    "        sequence = generated_text[-seq_length:]\n",
    "        \n",
    "        # Convert to indices\n",
    "        x_pred = np.array([char_to_idx.get(c, 0) for c in sequence])\n",
    "        x_pred = np.expand_dims(x_pred, axis=0)\n",
    "        \n",
    "        # Predict next character\n",
    "        predictions = model.predict(x_pred, verbose=0)[0]\n",
    "        \n",
    "        # Sample next character with temperature\n",
    "        next_idx = sample_with_temperature(predictions, temperature)\n",
    "        next_char = idx_to_char[next_idx]\n",
    "        \n",
    "        # Append to generated text\n",
    "        generated_text += next_char\n",
    "    \n",
    "    return generated_text\n",
    "\n",
    "print(\"\u2713 Text generation functions defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Section 5.2: Generate Sample Text\n",
    "\n",
    "Let's generate some Attack on Titan-style text with different temperatures."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define seed texts for generation\n",
    "seed_texts = [\n",
    "    \"EREN: I'm going to destroy every last Titan!\",\n",
    "    \"MIKASA: Where Eren goes, I go.\",\n",
    "    \"ARMIN: We need to think strategically.\",\n",
    "    \"LEVI: Don't get cocky, brat.\"\n",
    "]\n",
    "\n",
    "# Test different temperatures\n",
    "temperatures = [0.3, 0.5, 0.8]\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"\ud83c\udfad GENERATED ATTACK ON TITAN DIALOGUE\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "for seed in seed_texts[:2]:  # Generate for first 2 seeds\n",
    "    for temp in temperatures:\n",
    "        print(f\"\\n{'\u2500'*80}\")\n",
    "        generated = generate_text(model, seed, length=300, temperature=temp)\n",
    "        print(f\"\\n{generated}\")\n",
    "        print(f\"\\n{'\u2500'*80}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Section 5.3: Interactive Text Generation\n",
    "\n",
    "Generate custom text with your own seed and parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Interactive generation (modify these values)\n",
    "custom_seed = \"EREN: The Titans are coming!\"\n",
    "custom_length = 400\n",
    "custom_temperature = 0.6\n",
    "\n",
    "print(\"\ud83c\udfac Custom Text Generation\")\n",
    "print(\"=\"*80)\n",
    "custom_generated = generate_text(\n",
    "    model, \n",
    "    custom_seed, \n",
    "    length=custom_length, \n",
    "    temperature=custom_temperature\n",
    ")\n",
    "print(f\"\\n{custom_generated}\")\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 6.0: Model Persistence\n",
    "\n",
    "### Section 6.1: Save the Trained Model and Vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the model\n",
    "model_path = os.path.join(model_dir, 'aot_language_model.keras')\n",
    "model.save(model_path)\n",
    "print(f\"\u2713 Model saved to: {model_path}\")\n",
    "\n",
    "# Save the vocabulary and mappings\n",
    "vocab_data = {\n",
    "    'chars': chars,\n",
    "    'char_to_idx': char_to_idx,\n",
    "    'idx_to_char': {int(k): v for k, v in idx_to_char.items()},  # Convert keys to int for JSON\n",
    "    'vocab_size': vocab_size,\n",
    "    'seq_length': seq_length\n",
    "}\n",
    "\n",
    "vocab_path = os.path.join(model_dir, 'aot_vocab.json')\n",
    "with open(vocab_path, 'w', encoding='utf-8') as f:\n",
    "    json.dump(vocab_data, f, ensure_ascii=False, indent=2)\n",
    "print(f\"\u2713 Vocabulary saved to: {vocab_path}\")\n",
    "\n",
    "# Save training history\n",
    "history_path = os.path.join(model_dir, 'aot_training_history.json')\n",
    "history_data = {\n",
    "    'loss': [float(x) for x in history.history['loss']],\n",
    "    'val_loss': [float(x) for x in history.history['val_loss']],\n",
    "    'accuracy': [float(x) for x in history.history['accuracy']],\n",
    "    'val_accuracy': [float(x) for x in history.history['val_accuracy']],\n",
    "    'epochs': len(history.history['loss'])\n",
    "}\n",
    "with open(history_path, 'w') as f:\n",
    "    json.dump(history_data, f, indent=2)\n",
    "print(f\"\u2713 Training history saved to: {history_path}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"\u2705 MODEL SAVED SUCCESSFULLY\")\n",
    "print(\"=\"*80)\n",
    "print(f\"Model: {model_path}\")\n",
    "print(f\"Vocabulary: {vocab_path}\")\n",
    "print(f\"History: {history_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Section 6.2: Load Model and Vocabulary\n",
    "\n",
    "Demonstrate how to reload the model for future use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_aot_model(model_dir):\n",
    "    \"\"\"\n",
    "    Load a saved AoT language model and vocabulary.\n",
    "    \n",
    "    Args:\n",
    "        model_dir: Directory containing the saved model and vocabulary\n",
    "    \n",
    "    Returns:\n",
    "        model: Loaded Keras model\n",
    "        vocab_data: Dictionary containing vocabulary and mappings\n",
    "    \"\"\"\n",
    "    # Load model\n",
    "    model_path = os.path.join(model_dir, 'aot_language_model.keras')\n",
    "    model = keras.models.load_model(model_path)\n",
    "    print(f\"\u2713 Model loaded from: {model_path}\")\n",
    "    \n",
    "    # Load vocabulary\n",
    "    vocab_path = os.path.join(model_dir, 'aot_vocab.json')\n",
    "    with open(vocab_path, 'r', encoding='utf-8') as f:\n",
    "        vocab_data = json.load(f)\n",
    "    \n",
    "    # Convert idx_to_char keys back to integers\n",
    "    vocab_data['idx_to_char'] = {int(k): v for k, v in vocab_data['idx_to_char'].items()}\n",
    "    print(f\"\u2713 Vocabulary loaded from: {vocab_path}\")\n",
    "    \n",
    "    return model, vocab_data\n",
    "\n",
    "# Test loading the model\n",
    "print(\"\ud83d\udd04 Testing model reload...\\n\")\n",
    "loaded_model, loaded_vocab = load_aot_model(model_dir)\n",
    "\n",
    "print(f\"\\n\ud83d\udcca Loaded Model Info:\")\n",
    "print(f\"Vocabulary size: {loaded_vocab['vocab_size']}\")\n",
    "print(f\"Sequence length: {loaded_vocab['seq_length']}\")\n",
    "print(f\"Model parameters: {loaded_model.count_params():,}\")\n",
    "\n",
    "print(\"\\n\u2713 Model successfully loaded and ready for text generation!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Section 6.3: Generate Text with Loaded Model\n",
    "\n",
    "Verify that the loaded model works correctly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Update global variables with loaded vocabulary\n",
    "char_to_idx = loaded_vocab['char_to_idx']\n",
    "idx_to_char = loaded_vocab['idx_to_char']\n",
    "seq_length = loaded_vocab['seq_length']\n",
    "vocab_size = loaded_vocab['vocab_size']\n",
    "\n",
    "# Generate text with the loaded model\n",
    "test_seed = \"COMMANDER ERWIN: Soldiers, dedicate your hearts!\"\n",
    "print(\"\ud83c\udfac Generating text with loaded model...\\n\")\n",
    "print(\"=\"*80)\n",
    "test_generated = generate_text(\n",
    "    loaded_model, \n",
    "    test_seed, \n",
    "    length=350, \n",
    "    temperature=0.5\n",
    ")\n",
    "print(f\"\\n{test_generated}\")\n",
    "print(\"=\"*80)\n",
    "print(\"\\n\u2705 Model loading and generation verified!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 7.0: Summary and Conclusions\n",
    "\n",
    "### Key Achievements\n",
    "\n",
    "\u2705 **Data Processing**: Successfully loaded and processed the Attack on Titan Season 1 script\n",
    "- Character-level tokenization with vocabulary of unique characters\n",
    "- Created training sequences of 100 characters each\n",
    "- Split data into 90% training and 10% validation\n",
    "\n",
    "\u2705 **Model Architecture**: Built an LSTM-based language model\n",
    "- Embedding layer for character representations\n",
    "- Two LSTM layers with dropout for regularization\n",
    "- Softmax output layer for character prediction\n",
    "\n",
    "\u2705 **Training**: Successfully trained the model\n",
    "- Used EarlyStopping and ModelCheckpoint callbacks\n",
    "- Monitored both training and validation metrics\n",
    "- Achieved convergence with reasonable accuracy\n",
    "\n",
    "\u2705 **Text Generation**: Implemented flexible text generation\n",
    "- Temperature-based sampling for controlling creativity\n",
    "- Generated Attack on Titan-style dialogue\n",
    "- Demonstrated different generation strategies\n",
    "\n",
    "\u2705 **Model Persistence**: Saved and loaded the model\n",
    "- Model saved to Google Drive for persistence\n",
    "- Vocabulary and mappings saved for reuse\n",
    "- Verified successful reload and generation\n",
    "\n",
    "### Next Steps and Improvements\n",
    "\n",
    "\ud83d\udd2e **Potential Enhancements**:\n",
    "1. **Word-level tokenization**: Try word-level instead of character-level\n",
    "2. **Transformer architecture**: Implement attention-based models\n",
    "3. **Fine-tuning**: Train on specific characters' dialogue patterns\n",
    "4. **Longer sequences**: Increase sequence length for better context\n",
    "5. **Beam search**: Implement beam search for better generation\n",
    "6. **Interactive demo**: Create a web interface for text generation\n",
    "\n",
    "### Usage Instructions\n",
    "\n",
    "To use this model in the future:\n",
    "\n",
    "```python\n",
    "# 1. Load the model\n",
    "model, vocab = load_aot_model('/content/drive/MyDrive/DLA_Notebooks_Data_PGPM')\n",
    "\n",
    "# 2. Update global variables\n",
    "char_to_idx = vocab['char_to_idx']\n",
    "idx_to_char = vocab['idx_to_char']\n",
    "seq_length = vocab['seq_length']\n",
    "\n",
    "# 3. Generate text\n",
    "seed = \"EREN: The battle begins!\"\n",
    "generated = generate_text(model, seed, length=500, temperature=0.5)\n",
    "print(generated)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "**Author**: DLA Notebooks  \n",
    "**Dataset**: Attack on Titan Season 1 Scripts  \n",
    "**Framework**: TensorFlow/Keras  \n",
    "**Model Type**: Character-level LSTM Language Model"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}