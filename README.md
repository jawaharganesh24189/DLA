# DLA - Deep Learning Academy Dialogue GAN

Advanced adversarial dialogue generation system using GANs with enhanced multi-file data processing capabilities.

## ğŸš€ Features

- **GAN-based Dialogue Generation**: Generate realistic dialogue using SeqGAN architecture
- **Multi-File Data Processing**: Process thousands of dialogue files efficiently
- **Google Drive Integration**: Seamless integration with Google Colab
- **Multiple Format Support**: TXT, JSON, JSONL, CSV
- **Quality Filtering**: Automatic quality control and duplicate removal
- **Data Augmentation**: Expand datasets with smart augmentation techniques
- **Intelligent Caching**: Avoid reprocessing with smart caching
- **Real-time Monitoring**: Progress tracking and detailed statistics
- **Configurable Pipeline**: YAML-based configuration

## ğŸ“‹ Table of Contents

- [Installation](#installation)
- [Quick Start](#quick-start)
- [Components](#components)
- [Documentation](#documentation)
- [Examples](#examples)
- [Configuration](#configuration)
- [Contributing](#contributing)

## ğŸ”§ Installation

### For Google Colab

```python
# Clone the repository
!git clone https://github.com/jawaharganesh24189/DLA.git
%cd DLA

# Install dependencies
!pip install -r requirements.txt
```

### For Local Development

```bash
git clone https://github.com/jawaharganesh24189/DLA.git
cd DLA
pip install -r requirements.txt
```

## ğŸ¯ Quick Start

### In Google Colab

```python
from src.drive_data_loader import DriveDataLoader
from src.enhanced_data_processor import EnhancedDialogueProcessor

# Step 1: Mount Google Drive and load files
loader = DriveDataLoader()
loader.mount_drive()
files = loader.scan_folder('/content/drive/MyDrive/YourDataFolder', recursive=True)

print(f"Found {len(files)} dialogue files")

# Step 2: Process dialogues
processor = EnhancedDialogueProcessor.from_config_file('config/data_config.yaml')
dialogues = processor.process_drive_files(files, batch_size=100)

print(f"Processed {len(dialogues)} dialogues")

# Step 3: Build vocabulary
vocab = processor.build_vocabulary(dialogues)
print(f"Vocabulary size: {len(vocab)}")

# Step 4: Export for GAN training
processor.export_to_format(dialogues, 'processed_dialogues.jsonl', 'jsonl')
```

### Standalone Processing

```bash
python examples/process_drive_folder.py
```

## ğŸ§© Components

### 1. **DriveDataLoader** (`src/drive_data_loader.py`)

Handles Google Drive integration:
- Auto-mount with retry logic
- Recursive folder scanning
- Batch file loading
- Rate limit handling
- File metadata caching

### 2. **EnhancedDialogueProcessor** (`src/enhanced_data_processor.py`)

Core processing engine:
- Multi-format parsing (TXT, JSON, JSONL, CSV)
- Quality filtering and validation
- Duplicate detection and removal
- Vocabulary building
- Data augmentation
- Parallel processing

### 3. **DataCache** (`src/data_cache.py`)

Caching and checkpointing:
- Processed data caching
- Checkpoint management
- Cache validation and expiration
- Resume interrupted processing

### 4. **ProcessingMonitor** (`src/processing_monitor.py`)

Monitoring and logging:
- Real-time progress bars
- Processing statistics
- Memory usage tracking
- Error logging
- Report generation

### 5. **DialogueParser** (`src/data_processor.py`)

Base parser for dialogue files:
- Context-response format parsing
- Speaker dialogue format parsing
- Multiple format detection
- Text cleaning and normalization

## ğŸ“š Documentation

Comprehensive documentation is available in the `docs/` directory:

- **[Data Processing Guide](docs/DATA_PROCESSING_GUIDE.md)**: Complete guide for data processing
- **[Drive Setup Guide](docs/DRIVE_SETUP.md)**: Google Drive integration setup
- **[Configuration Reference](config/data_config.yaml)**: Full configuration options

## ğŸ“ Examples

Example scripts are available in the `examples/` directory:

- **[process_drive_folder.py](examples/process_drive_folder.py)**: Complete processing pipeline
- **[sample_config.yaml](examples/sample_config.yaml)**: Example configuration file

## âš™ï¸ Configuration

Configuration is managed via YAML files. Key settings:

```yaml
# config/data_config.yaml

drive:
  base_path: "/content/drive/MyDrive/DLA_Notebooks_Data_PGPM"
  data_folder: "Dataset"

processing:
  batch_size: 100
  parallel_workers: 4
  show_progress: true

quality:
  min_dialogue_length: 5
  max_dialogue_length: 150
  remove_duplicates: true

augmentation:
  enabled: true
  augment_ratio: 0.3
  techniques: ["synonym_replace", "context_shuffle"]
```

See [sample_config.yaml](examples/sample_config.yaml) for full options.

## ğŸ“ Usage with GAN Model

The processed dialogues integrate seamlessly with the GAN model:

```python
# Process dialogues
processor = EnhancedDialogueProcessor.from_config_file('config/data_config.yaml')
dialogues = processor.process_drive_files(files)

# Prepare for GAN training
contexts = [d.context for d in dialogues]
responses = [d.response for d in dialogues]

# Use with GAN model (see notebook for details)
# ... GAN training code ...
```

## ğŸ“Š Supported File Formats

### TXT Files
- Context-response format: `context: ... response: ...`
- Speaker dialogue format: `Speaker: dialogue text`

### JSON Files
```json
[
  {"context": "Hello", "response": "Hi there!"},
  {"context": "How are you?", "response": "I'm good!"}
]
```

### JSONL Files
```jsonl
{"context": "Hello", "response": "Hi there!"}
{"context": "How are you?", "response": "I'm good!"}
```

### CSV Files
```csv
context,response
"Hello","Hi there!"
"How are you?","I'm good!"
```

## ğŸš€ Performance

The system is optimized for large-scale processing:

- **Speed**: Process 3836+ files in < 10 minutes on Colab
- **Memory**: < 2GB memory usage during processing
- **Scalability**: Support up to 100K dialogues in memory
- **Caching**: > 80% cache hit rate on re-runs

## ğŸ” Features in Detail

### Quality Filtering

Automatically filters dialogues based on:
- Character length (min/max)
- Word count (min/max)
- Character frequency
- Duplicate detection

### Data Augmentation

Expand your dataset with:
- **Synonym replacement**: Replace words with synonyms
- **Context shuffling**: Rearrange context words
- **Configurable ratio**: Control augmentation amount

### Caching

Intelligent caching system:
- Avoid reprocessing on reruns
- Checkpoint support for long jobs
- Automatic cache expiration
- Cache integrity validation

### Monitoring

Comprehensive monitoring:
- Real-time progress bars (via tqdm)
- Processing statistics (files/sec, dialogues/sec)
- Memory usage tracking
- Error logging with details
- JSON statistics export
- CSV error reports

## ğŸ› ï¸ Development

### Project Structure

```
DLA/
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ data_processor.py              # Base dialogue parser
â”‚   â”œâ”€â”€ drive_data_loader.py           # Google Drive integration
â”‚   â”œâ”€â”€ enhanced_data_processor.py     # Main processing engine
â”‚   â”œâ”€â”€ data_cache.py                  # Caching system
â”‚   â””â”€â”€ processing_monitor.py          # Monitoring and logging
â”œâ”€â”€ config/
â”‚   â””â”€â”€ data_config.yaml               # Main configuration
â”œâ”€â”€ docs/
â”‚   â”œâ”€â”€ DATA_PROCESSING_GUIDE.md       # Processing guide
â”‚   â””â”€â”€ DRIVE_SETUP.md                 # Drive setup guide
â”œâ”€â”€ examples/
â”‚   â”œâ”€â”€ process_drive_folder.py        # Example script
â”‚   â””â”€â”€ sample_config.yaml             # Example config
â”œâ”€â”€ tests/
â”‚   â””â”€â”€ test_data_processing.ipynb     # Test notebook
â”œâ”€â”€ Copy_of_8E_Adversarial_Dialogue_GAN.ipynb  # Main GAN notebook
â”œâ”€â”€ requirements.txt                   # Dependencies
â””â”€â”€ README.md                          # This file
```

## ğŸ¤ Contributing

Contributions are welcome! Please feel free to submit a Pull Request.

## ğŸ“„ License

This project is part of the Deep Learning Academy curriculum.

## ğŸ™ Acknowledgments

- Deep Learning Academy for the GAN architecture
- Google Colab for providing free GPU resources
- Open source community for excellent libraries (tqdm, pandas, etc.)

## ğŸ“ Support

For issues and questions:
- Check the [documentation](docs/)
- Review [examples](examples/)
- Open an issue on GitHub

## ğŸ”® Future Enhancements

- [ ] API deployment for inference
- [ ] Additional augmentation techniques
- [ ] Multi-language support
- [ ] Integration with more data sources
- [ ] Advanced quality metrics
- [ ] Real-time inference dashboard