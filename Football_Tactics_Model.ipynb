{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Football Tactics Model using DLA Transformer Architecture\n",
        "\n",
        "**Author**: Deep Learning Academy  \n",
        "**Date**: {}\n",
        "\n",
        "This notebook implements a football tactics prediction model using advanced Transformer architectures from DLA notebooks:\n",
        "- **Notebook 9**: Decoder-only LLM with temperature and top-k sampling\n",
        "- **Notebook 8B**: Encoder-decoder seq2seq architecture\n",
        "\n",
        "We'll build a model that:\n",
        "1. Understands game state (positions, formations, ball location)\n",
        "2. Predicts optimal tactical sequences\n",
        "3. Generates diverse tactics using sampling strategies"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1.0 Background: Football Tactics as Sequence-to-Sequence\n",
        "\n",
        "### Problem Formulation\n",
        "\n",
        "**Input (Game State)**: Current field positions, ball location, formation  \n",
        "**Output (Tactics)**: Sequence of tactical actions (pass, press, position)\n",
        "\n",
        "### Architecture Choice: Encoder-Decoder Transformer\n",
        "\n",
        "```\n",
        "Game State \u2192 Encoder \u2192 Context\n",
        "                         \u2193\n",
        "Start Token \u2192 Decoder \u2192 Cross-Attention \u2192 Next Tactic\n",
        "                \u2193\n",
        "         (autoregressive)\n",
        "```\n",
        "\n",
        "### Key DLA Techniques Used\n",
        "\n",
        "From **Notebook 8B** (Translation Model):\n",
        "- TransformerEncoder with self-attention\n",
        "- TransformerDecoder with cross-attention\n",
        "- PositionalEmbedding for sequences\n",
        "- Causal masking for autoregressive generation\n",
        "\n",
        "From **Notebook 9** (LLM Building):\n",
        "- Temperature sampling for diversity\n",
        "- Top-K sampling for quality control\n",
        "- Custom learning rate schedules\n",
        "- Efficient batch processing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2.0 Setup and Dependencies"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Install if needed\n",
        "# !pip install tensorflow keras numpy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import random\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "\n",
        "print(f\"TensorFlow version: {tf.__version__}\")\n",
        "print(f\"Keras version: {keras.__version__}\")\n",
        "\n",
        "# Set seed for reproducibility\n",
        "random.seed(42)\n",
        "np.random.seed(42)\n",
        "tf.random.set_seed(42)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3.0 Football Tactics Data Preparation\n",
        "\n",
        "We'll create synthetic football tactics data representing:\n",
        "- **Game State**: Player positions, ball location, score\n",
        "- **Tactics**: Sequence of actions (pass, move, press, etc.)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Define football vocabulary\n",
        "\n",
        "# Player positions\n",
        "POSITIONS = ['GK', 'LB', 'CB', 'RB', 'LWB', 'RWB', 'CDM', 'CM', 'CAM', 'LM', 'RM', 'LW', 'RW', 'ST', 'CF']\n",
        "\n",
        "# Tactical actions\n",
        "ACTIONS = ['pass', 'dribble', 'shoot', 'cross', 'tackle', 'intercept', 'press', 'fallback', 'support', 'move_forward']\n",
        "\n",
        "# Directions/targets\n",
        "DIRECTIONS = ['left', 'right', 'center', 'forward', 'back', 'wide']\n",
        "\n",
        "# Formations\n",
        "FORMATIONS = ['4-4-2', '4-3-3', '3-5-2', '4-2-3-1', '5-3-2']\n",
        "\n",
        "print(f\"Positions: {len(POSITIONS)}\")\n",
        "print(f\"Actions: {len(ACTIONS)}\")\n",
        "print(f\"Directions: {len(DIRECTIONS)}\")\n",
        "print(f\"Formations: {len(FORMATIONS)}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Generate synthetic football tactics data\n",
        "def generate_game_state():\n",
        "    \"\"\"Generate a random game state description\"\"\"\n",
        "    formation = random.choice(FORMATIONS)\n",
        "    ball_position = random.choice(['defense', 'midfield', 'attack'])\n",
        "    score_diff = random.choice(['losing', 'drawing', 'winning'])\n",
        "    return f\"formation {formation} ball {ball_position} status {score_diff}\"\n",
        "\n",
        "def generate_tactic_sequence():\n",
        "    \"\"\"Generate a tactical sequence\"\"\"\n",
        "    num_actions = random.randint(3, 6)\n",
        "    tactics = []\n",
        "    \n",
        "    for _ in range(num_actions):\n",
        "        position = random.choice(POSITIONS)\n",
        "        action = random.choice(ACTIONS)\n",
        "        direction = random.choice(DIRECTIONS)\n",
        "        tactics.append(f\"{position} {action} {direction}\")\n",
        "    \n",
        "    return \" , \".join(tactics)\n",
        "\n",
        "# Generate training data\n",
        "num_samples = 500\n",
        "game_states = [generate_game_state() for _ in range(num_samples)]\n",
        "tactic_sequences = [generate_tactic_sequence() for _ in range(num_samples)]\n",
        "\n",
        "# Add start and end tokens to tactics\n",
        "tactic_sequences = [\"[start] \" + seq + \" [end]\" for seq in tactic_sequences]\n",
        "\n",
        "print(f\"Generated {len(game_states)} training samples\")\n",
        "print(f\"\\nExample game state: {game_states[0]}\")\n",
        "print(f\"Example tactic: {tactic_sequences[0]}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4.0 Data Preprocessing and Vectorization\n",
        "\n",
        "Using techniques from **Notebook 8B**: Text vectorization with custom vocabulary"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create text pairs for training\n",
        "text_pairs = list(zip(game_states, tactic_sequences))\n",
        "\n",
        "# Shuffle and split\n",
        "random.shuffle(text_pairs)\n",
        "val_samples = int(0.15 * len(text_pairs))\n",
        "train_samples = len(text_pairs) - val_samples\n",
        "\n",
        "train_pairs = text_pairs[:train_samples]\n",
        "val_pairs = text_pairs[train_samples:]\n",
        "\n",
        "print(f\"Training samples: {len(train_pairs)}\")\n",
        "print(f\"Validation samples: {len(val_pairs)}\")\n",
        "print(f\"\\nSample pair:\")\n",
        "print(f\"Game State: {train_pairs[0][0]}\")\n",
        "print(f\"Tactics: {train_pairs[0][1]}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from tensorflow.keras.layers import TextVectorization\n",
        "\n",
        "# Configuration\n",
        "vocab_size = 500\n",
        "state_sequence_length = 15\n",
        "tactic_sequence_length = 40\n",
        "\n",
        "# Create vectorization layers (technique from Notebook 8B)\n",
        "state_vectorization = TextVectorization(\n",
        "    max_tokens=vocab_size,\n",
        "    output_mode='int',\n",
        "    output_sequence_length=state_sequence_length,\n",
        ")\n",
        "\n",
        "tactic_vectorization = TextVectorization(\n",
        "    max_tokens=vocab_size,\n",
        "    output_mode='int',\n",
        "    output_sequence_length=tactic_sequence_length + 1,\n",
        ")\n",
        "\n",
        "# Adapt to training data\n",
        "train_states = [pair[0] for pair in train_pairs]\n",
        "train_tactics = [pair[1] for pair in train_pairs]\n",
        "\n",
        "state_vectorization.adapt(train_states)\n",
        "tactic_vectorization.adapt(train_tactics)\n",
        "\n",
        "print(f\"State vocabulary size: {state_vectorization.vocabulary_size()}\")\n",
        "print(f\"Tactic vocabulary size: {tactic_vectorization.vocabulary_size()}\")\n",
        "print(f\"\\nSample vocabulary (states): {state_vectorization.get_vocabulary()[:15]}\")\n",
        "print(f\"\\nSample vocabulary (tactics): {tactic_vectorization.get_vocabulary()[:15]}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5.0 Create Training Datasets\n",
        "\n",
        "Following **Notebook 8B** approach for efficient data pipeline"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def format_dataset(states, tactics):\n",
        "    \"\"\"Format data for encoder-decoder training\"\"\"\n",
        "    states = state_vectorization(states)\n",
        "    tactics = tactic_vectorization(tactics)\n",
        "    return ({\n",
        "        \"encoder_inputs\": states,\n",
        "        \"decoder_inputs\": tactics[:, :-1],\n",
        "    }, tactics[:, 1:])\n",
        "\n",
        "def make_dataset(pairs, batch_size=32):\n",
        "    \"\"\"Create tf.data.Dataset with prefetching\"\"\"\n",
        "    states_list = [pair[0] for pair in pairs]\n",
        "    tactics_list = [pair[1] for pair in pairs]\n",
        "    dataset = tf.data.Dataset.from_tensor_slices((states_list, tactics_list))\n",
        "    dataset = dataset.batch(batch_size)\n",
        "    dataset = dataset.map(format_dataset, num_parallel_calls=tf.data.AUTOTUNE)\n",
        "    return dataset.shuffle(2048).prefetch(tf.data.AUTOTUNE).cache()\n",
        "\n",
        "train_ds = make_dataset(train_pairs)\n",
        "val_ds = make_dataset(val_pairs)\n",
        "\n",
        "print(\"Training dataset created successfully!\")\n",
        "print(f\"Dataset structure: {train_ds.element_spec}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6.0 Build Transformer Components (DLA Architecture)\n",
        "\n",
        "### 6.1 PositionalEmbedding Layer\n",
        "\n",
        "From **Notebook 8B**: Combines token embeddings with learned positional embeddings"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class PositionalEmbedding(layers.Layer):\n",
        "    \"\"\"Positional Embedding from DLA Notebook 8B\"\"\"\n",
        "    def __init__(self, sequence_length, input_dim, output_dim, **kwargs):\n",
        "        super().__init__(**kwargs)\n",
        "        self.token_embeddings = layers.Embedding(\n",
        "            input_dim=input_dim, output_dim=output_dim\n",
        "        )\n",
        "        self.position_embeddings = layers.Embedding(\n",
        "            input_dim=sequence_length, output_dim=output_dim\n",
        "        )\n",
        "        self.sequence_length = sequence_length\n",
        "        self.input_dim = input_dim\n",
        "        self.output_dim = output_dim\n",
        "\n",
        "    def call(self, inputs):\n",
        "        length = tf.shape(inputs)[-1]\n",
        "        positions = tf.range(start=0, limit=length, delta=1)\n",
        "        embedded_tokens = self.token_embeddings(inputs)\n",
        "        embedded_positions = self.position_embeddings(positions)\n",
        "        return embedded_tokens + embedded_positions\n",
        "\n",
        "    def compute_mask(self, inputs, mask=None):\n",
        "        return tf.math.not_equal(inputs, 0)\n",
        "\n",
        "    def get_config(self):\n",
        "        config = super().get_config()\n",
        "        config.update({\n",
        "            \"sequence_length\": self.sequence_length,\n",
        "            \"input_dim\": self.input_dim,\n",
        "            \"output_dim\": self.output_dim,\n",
        "        })\n",
        "        return config\n",
        "\n",
        "print(\"\u2713 PositionalEmbedding defined\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 6.2 TransformerEncoder\n",
        "\n",
        "From **Notebook 8B**: Self-attention encoder for understanding game state"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class TransformerEncoder(layers.Layer):\n",
        "    \"\"\"Transformer Encoder from DLA Notebook 8B\"\"\"\n",
        "    def __init__(self, embed_dim, dense_dim, num_heads, **kwargs):\n",
        "        super().__init__(**kwargs)\n",
        "        self.embed_dim = embed_dim\n",
        "        self.dense_dim = dense_dim\n",
        "        self.num_heads = num_heads\n",
        "        self.attention = layers.MultiHeadAttention(\n",
        "            num_heads=num_heads, key_dim=embed_dim\n",
        "        )\n",
        "        self.dense_proj = keras.Sequential([\n",
        "            layers.Dense(dense_dim, activation='relu'),\n",
        "            layers.Dense(embed_dim),\n",
        "        ])\n",
        "        self.layernorm_1 = layers.LayerNormalization()\n",
        "        self.layernorm_2 = layers.LayerNormalization()\n",
        "\n",
        "    def call(self, inputs, mask=None):\n",
        "        if mask is not None:\n",
        "            mask = mask[:, tf.newaxis, :]\n",
        "        attention_output = self.attention(inputs, inputs, attention_mask=mask)\n",
        "        proj_input = self.layernorm_1(inputs + attention_output)\n",
        "        proj_output = self.dense_proj(proj_input)\n",
        "        return self.layernorm_2(proj_input + proj_output)\n",
        "\n",
        "    def get_config(self):\n",
        "        config = super().get_config()\n",
        "        config.update({\n",
        "            \"embed_dim\": self.embed_dim,\n",
        "            \"dense_dim\": self.dense_dim,\n",
        "            \"num_heads\": self.num_heads,\n",
        "        })\n",
        "        return config\n",
        "\n",
        "print(\"\u2713 TransformerEncoder defined\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 6.3 TransformerDecoder\n",
        "\n",
        "From **Notebook 8B**: Decoder with causal self-attention and cross-attention"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class TransformerDecoder(layers.Layer):\n",
        "    \"\"\"Transformer Decoder from DLA Notebook 8B\"\"\"\n",
        "    def __init__(self, embed_dim, dense_dim, num_heads, **kwargs):\n",
        "        super().__init__(**kwargs)\n",
        "        self.embed_dim = embed_dim\n",
        "        self.dense_dim = dense_dim\n",
        "        self.num_heads = num_heads\n",
        "        self.attention_1 = layers.MultiHeadAttention(\n",
        "            num_heads=num_heads, key_dim=embed_dim\n",
        "        )\n",
        "        self.attention_2 = layers.MultiHeadAttention(\n",
        "            num_heads=num_heads, key_dim=embed_dim\n",
        "        )\n",
        "        self.dense_proj = keras.Sequential([\n",
        "            layers.Dense(dense_dim, activation='relu'),\n",
        "            layers.Dense(embed_dim),\n",
        "        ])\n",
        "        self.layernorm_1 = layers.LayerNormalization()\n",
        "        self.layernorm_2 = layers.LayerNormalization()\n",
        "        self.layernorm_3 = layers.LayerNormalization()\n",
        "        self.supports_masking = True\n",
        "\n",
        "    def get_causal_attention_mask(self, inputs):\n",
        "        \"\"\"Create causal mask for autoregressive generation\"\"\"\n",
        "        input_shape = tf.shape(inputs)\n",
        "        batch_size, sequence_length = input_shape[0], input_shape[1]\n",
        "        i = tf.range(sequence_length)[:, tf.newaxis]\n",
        "        j = tf.range(sequence_length)\n",
        "        mask = tf.cast(i >= j, dtype='int32')\n",
        "        mask = tf.reshape(mask, (1, input_shape[1], input_shape[1]))\n",
        "        mult = tf.concat(\n",
        "            [tf.expand_dims(batch_size, -1), tf.constant([1, 1], dtype=tf.int32)],\n",
        "            axis=0,\n",
        "        )\n",
        "        return tf.tile(mask, mult)\n",
        "\n",
        "    def call(self, inputs, encoder_outputs, mask=None):\n",
        "        causal_mask = self.get_causal_attention_mask(inputs)\n",
        "        if mask is not None:\n",
        "            padding_mask = tf.cast(mask[:, tf.newaxis, :], dtype='int32')\n",
        "            padding_mask = tf.minimum(padding_mask, causal_mask)\n",
        "        else:\n",
        "            padding_mask = causal_mask\n",
        "        \n",
        "        # Self-attention on decoder (causal)\n",
        "        attention_output_1 = self.attention_1(\n",
        "            query=inputs,\n",
        "            value=inputs,\n",
        "            key=inputs,\n",
        "            attention_mask=causal_mask,\n",
        "        )\n",
        "        attention_output_1 = self.layernorm_1(inputs + attention_output_1)\n",
        "        \n",
        "        # Cross-attention to encoder\n",
        "        attention_output_2 = self.attention_2(\n",
        "            query=attention_output_1,\n",
        "            value=encoder_outputs,\n",
        "            key=encoder_outputs,\n",
        "            attention_mask=mask[:, tf.newaxis, :] if mask is not None else None,\n",
        "        )\n",
        "        attention_output_2 = self.layernorm_2(attention_output_1 + attention_output_2)\n",
        "        \n",
        "        # Feed-forward\n",
        "        proj_output = self.dense_proj(attention_output_2)\n",
        "        return self.layernorm_3(attention_output_2 + proj_output)\n",
        "\n",
        "    def get_config(self):\n",
        "        config = super().get_config()\n",
        "        config.update({\n",
        "            \"embed_dim\": self.embed_dim,\n",
        "            \"dense_dim\": self.dense_dim,\n",
        "            \"num_heads\": self.num_heads,\n",
        "        })\n",
        "        return config\n",
        "\n",
        "print(\"\u2713 TransformerDecoder defined\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 7.0 Build Complete Football Tactics Model\n",
        "\n",
        "Encoder-decoder architecture combining techniques from both notebooks"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Model hyperparameters\n",
        "embed_dim = 128\n",
        "dense_dim = 512\n",
        "num_heads = 4\n",
        "\n",
        "# Encoder: Process game state\n",
        "encoder_inputs = keras.Input(shape=(None,), dtype='int64', name='encoder_inputs')\n",
        "x = PositionalEmbedding(state_sequence_length, vocab_size, embed_dim)(encoder_inputs)\n",
        "encoder_outputs = TransformerEncoder(embed_dim, dense_dim, num_heads)(x)\n",
        "encoder = keras.Model(encoder_inputs, encoder_outputs, name='encoder')\n",
        "\n",
        "# Decoder: Generate tactics\n",
        "decoder_inputs = keras.Input(shape=(None,), dtype='int64', name='decoder_inputs')\n",
        "encoded_seq_inputs = keras.Input(shape=(None, embed_dim), name='encoder_outputs')\n",
        "x = PositionalEmbedding(tactic_sequence_length, vocab_size, embed_dim)(decoder_inputs)\n",
        "x = TransformerDecoder(embed_dim, dense_dim, num_heads)(x, encoded_seq_inputs)\n",
        "x = layers.Dropout(0.3)(x)\n",
        "decoder_outputs = layers.Dense(vocab_size, activation='softmax')(x)\n",
        "decoder = keras.Model([decoder_inputs, encoded_seq_inputs], decoder_outputs, name='decoder')\n",
        "\n",
        "# Full model\n",
        "decoder_outputs = decoder([decoder_inputs, encoder_outputs])\n",
        "football_model = keras.Model(\n",
        "    [encoder_inputs, decoder_inputs],\n",
        "    decoder_outputs,\n",
        "    name='football_tactics_model'\n",
        ")\n",
        "\n",
        "print(\"\u2713 Football Tactics Model built successfully!\")\n",
        "football_model.summary()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 8.0 Compile and Train\n",
        "\n",
        "Using techniques from **Notebook 8B** for seq2seq training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Compile model\n",
        "football_model.compile(\n",
        "    optimizer='adam',\n",
        "    loss='sparse_categorical_crossentropy',\n",
        "    metrics=['accuracy']\n",
        ")\n",
        "\n",
        "print(\"\u2713 Model compiled successfully!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Train the model\n",
        "history = football_model.fit(\n",
        "    train_ds,\n",
        "    epochs=30,\n",
        "    validation_data=val_ds,\n",
        ")\n",
        "\n",
        "print(\"\u2713 Training complete!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 9.0 Inference with Advanced Sampling (from Notebook 9)\n",
        "\n",
        "### 9.1 Greedy Decoding\n",
        "\n",
        "Simple argmax at each step"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def decode_sequence_greedy(input_state):\n",
        "    \"\"\"Greedy decoding: always pick most likely next token\"\"\"\n",
        "    # Encode game state\n",
        "    tokenized_input = state_vectorization([input_state])\n",
        "    encoder_output = encoder(tokenized_input)\n",
        "    \n",
        "    # Start with [start] token\n",
        "    decoded_tactic = \"[start]\"\n",
        "    \n",
        "    for i in range(tactic_sequence_length):\n",
        "        tokenized_target = tactic_vectorization([decoded_tactic])\n",
        "        predictions = decoder([tokenized_target, encoder_output])\n",
        "        \n",
        "        # Greedy: take argmax\n",
        "        sampled_token_index = np.argmax(predictions[0, i, :])\n",
        "        sampled_token = tactic_vectorization.get_vocabulary()[sampled_token_index]\n",
        "        \n",
        "        decoded_tactic += \" \" + sampled_token\n",
        "        \n",
        "        if sampled_token == \"[end]\":\n",
        "            break\n",
        "    \n",
        "    # Clean up\n",
        "    decoded_tactic = decoded_tactic.replace(\"[start] \", \"\")\n",
        "    decoded_tactic = decoded_tactic.replace(\" [end]\", \"\")\n",
        "    \n",
        "    return decoded_tactic\n",
        "\n",
        "print(\"\u2713 Greedy decoding function defined\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 9.2 Temperature Sampling (from Notebook 9)\n",
        "\n",
        "Control randomness: lower temperature = more deterministic, higher = more diverse"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def decode_sequence_temperature(input_state, temperature=1.0):\n",
        "    \"\"\"Temperature sampling from DLA Notebook 9\"\"\"\n",
        "    # Encode game state\n",
        "    tokenized_input = state_vectorization([input_state])\n",
        "    encoder_output = encoder(tokenized_input)\n",
        "    \n",
        "    decoded_tactic = \"[start]\"\n",
        "    \n",
        "    for i in range(tactic_sequence_length):\n",
        "        tokenized_target = tactic_vectorization([decoded_tactic])\n",
        "        predictions = decoder([tokenized_target, encoder_output])\n",
        "        \n",
        "        # Temperature sampling\n",
        "        logits = predictions[0, i, :]\n",
        "        logits = logits / temperature  # Scale by temperature\n",
        "        probabilities = tf.nn.softmax(logits).numpy()\n",
        "        \n",
        "        # Sample from distribution\n",
        "        sampled_token_index = np.random.choice(len(probabilities), p=probabilities)\n",
        "        sampled_token = tactic_vectorization.get_vocabulary()[sampled_token_index]\n",
        "        \n",
        "        decoded_tactic += \" \" + sampled_token\n",
        "        \n",
        "        if sampled_token == \"[end]\":\n",
        "            break\n",
        "    \n",
        "    decoded_tactic = decoded_tactic.replace(\"[start] \", \"\")\n",
        "    decoded_tactic = decoded_tactic.replace(\" [end]\", \"\")\n",
        "    \n",
        "    return decoded_tactic\n",
        "\n",
        "print(\"\u2713 Temperature sampling function defined\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 9.3 Top-K Sampling (from Notebook 9)\n",
        "\n",
        "Only sample from top K most likely tokens for better quality"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def decode_sequence_topk(input_state, k=5, temperature=1.0):\n",
        "    \"\"\"Top-K sampling from DLA Notebook 9\"\"\"\n",
        "    # Encode game state\n",
        "    tokenized_input = state_vectorization([input_state])\n",
        "    encoder_output = encoder(tokenized_input)\n",
        "    \n",
        "    decoded_tactic = \"[start]\"\n",
        "    \n",
        "    for i in range(tactic_sequence_length):\n",
        "        tokenized_target = tactic_vectorization([decoded_tactic])\n",
        "        predictions = decoder([tokenized_target, encoder_output])\n",
        "        \n",
        "        # Get logits and apply temperature\n",
        "        logits = predictions[0, i, :]\n",
        "        logits = logits / temperature\n",
        "        \n",
        "        # Top-K filtering\n",
        "        top_k_indices = tf.argsort(logits, direction='DESCENDING')[:k]\n",
        "        top_k_logits = tf.gather(logits, top_k_indices)\n",
        "        top_k_probs = tf.nn.softmax(top_k_logits).numpy()\n",
        "        \n",
        "        # Sample from top K\n",
        "        sampled_index = np.random.choice(k, p=top_k_probs)\n",
        "        sampled_token_index = top_k_indices[sampled_index].numpy()\n",
        "        sampled_token = tactic_vectorization.get_vocabulary()[sampled_token_index]\n",
        "        \n",
        "        decoded_tactic += \" \" + sampled_token\n",
        "        \n",
        "        if sampled_token == \"[end]\":\n",
        "            break\n",
        "    \n",
        "    decoded_tactic = decoded_tactic.replace(\"[start] \", \"\")\n",
        "    decoded_tactic = decoded_tactic.replace(\" [end]\", \"\")\n",
        "    \n",
        "    return decoded_tactic\n",
        "\n",
        "print(\"\u2713 Top-K sampling function defined\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 10.0 Test Tactics Generation\n",
        "\n",
        "Generate tactics for various game situations using all three sampling methods"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Test scenarios\n",
        "test_scenarios = [\n",
        "    \"formation 4-4-2 ball midfield status drawing\",\n",
        "    \"formation 4-3-3 ball attack status winning\",\n",
        "    \"formation 3-5-2 ball defense status losing\",\n",
        "    \"formation 4-2-3-1 ball attack status drawing\",\n",
        "]\n",
        "\n",
        "print(\"=\" * 100)\n",
        "print(\"FOOTBALL TACTICS GENERATION - DLA TRANSFORMER MODEL\")\n",
        "print(\"=\" * 100)\n",
        "\n",
        "for scenario in test_scenarios:\n",
        "    print(f\"\\n{'='*100}\")\n",
        "    print(f\"GAME STATE: {scenario}\")\n",
        "    print(f\"{'='*100}\")\n",
        "    \n",
        "    # Greedy\n",
        "    greedy_tactic = decode_sequence_greedy(scenario)\n",
        "    print(f\"\\n[GREEDY] Predicted Tactics:\")\n",
        "    print(f\"  {greedy_tactic}\")\n",
        "    \n",
        "    # Temperature sampling\n",
        "    print(f\"\\n[TEMPERATURE SAMPLING (T=0.7)] Predicted Tactics:\")\n",
        "    for j in range(2):\n",
        "        temp_tactic = decode_sequence_temperature(scenario, temperature=0.7)\n",
        "        print(f\"  Variant {j+1}: {temp_tactic}\")\n",
        "    \n",
        "    # Top-K sampling\n",
        "    print(f\"\\n[TOP-K SAMPLING (K=5, T=0.8)] Predicted Tactics:\")\n",
        "    for j in range(2):\n",
        "        topk_tactic = decode_sequence_topk(scenario, k=5, temperature=0.8)\n",
        "        print(f\"  Variant {j+1}: {topk_tactic}\")\n",
        "    \n",
        "    print()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 11.0 Summary and Key DLA Techniques Used\n",
        "\n",
        "### Architecture Summary\n",
        "\n",
        "```\n",
        "Game State (formation, ball position, score)\n",
        "    \u2193\n",
        "Encoder (TransformerEncoder)\n",
        "    - Self-attention on game state\n",
        "    - Positional embeddings\n",
        "    - Feed-forward networks\n",
        "    \u2193\n",
        "Context Representation\n",
        "    \u2193\n",
        "Decoder (TransformerDecoder)\n",
        "    - Causal self-attention (autoregressive)\n",
        "    - Cross-attention to encoder\n",
        "    - Feed-forward networks\n",
        "    \u2193\n",
        "Tactical Sequence (player actions)\n",
        "```\n",
        "\n",
        "### DLA Techniques Implemented\n",
        "\n",
        "#### From Notebook 8B (Translation Model):\n",
        "\u2705 **TransformerEncoder**: Self-attention for game state understanding  \n",
        "\u2705 **TransformerDecoder**: Causal attention + cross-attention  \n",
        "\u2705 **PositionalEmbedding**: Token + position embeddings  \n",
        "\u2705 **Causal Masking**: Prevents future information leakage  \n",
        "\u2705 **TextVectorization**: Custom vocabulary adaptation  \n",
        "\u2705 **Efficient Data Pipeline**: tf.data with prefetching  \n",
        "\n",
        "#### From Notebook 9 (LLM Building):\n",
        "\u2705 **Temperature Sampling**: Control generation diversity  \n",
        "\u2705 **Top-K Sampling**: Quality control by filtering low-probability tokens  \n",
        "\u2705 **Greedy Decoding**: Deterministic generation  \n",
        "\u2705 **Batch Processing**: Efficient training with shuffling/caching  \n",
        "\n",
        "### Model Specifications\n",
        "- **Embedding Dimension**: 128\n",
        "- **Feed-forward Dimension**: 512\n",
        "- **Attention Heads**: 4\n",
        "- **Vocabulary Size**: 500\n",
        "- **State Sequence Length**: 15\n",
        "- **Tactic Sequence Length**: 40\n",
        "\n",
        "### Next Steps\n",
        "\n",
        "1. **Enhanced Data**: Use real match data from football APIs\n",
        "2. **Reward Signal**: Integrate match outcome as reward for RL fine-tuning\n",
        "3. **Beam Search**: Implement beam search for multiple candidate tactics\n",
        "4. **Attention Visualization**: Visualize what game aspects the model focuses on\n",
        "5. **Multi-task Learning**: Predict both tactics and expected outcomes\n",
        "\n",
        "### Key Innovation\n",
        "\n",
        "This model combines:\n",
        "- **Seq2seq understanding** (game state \u2192 tactics) from Notebook 8B\n",
        "- **Advanced sampling strategies** for diverse tactical generation from Notebook 9\n",
        "- **Autoregressive generation** for sequential tactical planning\n",
        "\n",
        "The result is a football tactics model that can:\n",
        "1. Understand complex game states\n",
        "2. Generate contextually appropriate tactical sequences\n",
        "3. Provide multiple tactical options with controlled diversity"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}